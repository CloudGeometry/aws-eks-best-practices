<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<title>EKS Scalability best practices</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
</head>
<body class="article">
<div id="header">
<h1>EKS Scalability best practices</h1>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This guide provides advice for scaling EKS clusters. The goal of scaling an EKS cluster is to maximize the amount of work a single cluster can perform. Using a single, large EKS cluster can reduce operational load compared to using multiple clusters, but it has trade-offs for things like multi-region deployments, tenant isolation, and cluster upgrades. In this document we will focus on how to achieve maximum scalability with a single cluster.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_how_to_use_this_guide">How to use this guide</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This guide is meant for developers and administrators responsible for creating and managing EKS clusters in AWS. It focuses on some generic Kubernetes scaling practices, but it does not have specifics for self-managed Kubernetes clusters or clusters that run outside of an AWS region with <a href="https://anywhere.eks.amazonaws.com/">EKS Anywhere</a>.</p>
</div>
<div class="paragraph">
<p>Each topic has a brief overview, followed by recommendations and best practices for operating EKS clusters at scale. Topics do not need to be read in a particular order and recommendations should not be applied without testing and verifying they work in your clusters.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_understanding_scaling_dimensions">Understanding scaling dimensions</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Scalability is different from performance and <a href="https://aws.github.io/aws-eks-best-practices/reliability/docs/">reliability</a>, and all three should be considered when planning your cluster and workload needs. As clusters scale, they need to be monitored, but this guide will not cover monitoring best practices. EKS can scale to large sizes, but you will need to plan how you are going to scale a cluster beyond 300 nodes or 5000 pods. These are not absolute numbers, but they come from collaborating this guide with multiple users, engineers, and support professionals.</p>
</div>
<div class="paragraph">
<p>Scaling in Kubernetes is multi-dimensional and there are no specific settings or recommendations that work in every situation. The main areas areas where we can provide guidance for scaling include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="control-plane">Kubernetes Control Plane</a></p>
</li>
<li>
<p><a href="data-plane">Kubernetes Data Plane</a></p>
</li>
<li>
<p><a href="cluster-services">Cluster Services</a></p>
</li>
<li>
<p><a href="workloads">Workloads</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Kubernetes Control Plane</strong> in an EKS cluster includes all of the services AWS runs and scales for you automatically (e.g. Kubernetes API server). Scaling the Control Plane is AWS&#8217;s responsibility, but using the Control Plane responsibly is your responsibility.</p>
</div>
<div class="paragraph">
<p><strong>Kubernetes Data Plane</strong> scaling deals with AWS resources that are required for your cluster and workloads, but they are outside of the EKS Control Plane. Resources including EC2 instances, kubelet, and storage all need to be scaled as your cluster scales.</p>
</div>
<div class="paragraph">
<p><strong>Cluster services</strong> are Kubernetes controllers and applications that run inside the cluster and provide functionality for your cluster and workloads. These can be <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html">EKS Add-ons</a> and also other services or Helm charts you install for compliance and integrations. These services are often depended on by workloads and as your workloads scale your cluster services will need to scale with them.</p>
</div>
<div class="paragraph">
<p><strong>Workloads</strong> are the reason you have a cluster and should scale horizontally with the cluster. There are integrations and settings that workloads have in Kubernetes that can help the cluster scale. There are also architectural considerations with Kubernetes abstractions such as namespaces and services.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_extra_large_scaling">Extra large scaling</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If you are scaling a single cluster beyond 1000 nodes or 50,000 pods we would love to talk to you. We recommend reaching out to your support team or technical account manager to get in touch with specialists who can help you plan and scale beyond the information provided in this guide.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_kubernetes_control_plane">Kubernetes Control Plane</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Kubernetes control plane consists of the Kubernetes API Server, Kubernetes Controller Manager, Scheduler and other components that are required for Kubernetes to function. Scalability limits of these components are different depending on what you&#8217;re running in the cluster, but the areas with the biggest impact to scaling include the Kubernetes version, utilization, and individual Node scaling.</p>
</div>
<div class="sect2">
<h3 id="_use_eks_1_24_or_above">Use EKS 1.24 or above</h3>
<div class="paragraph">
<p>EKS 1.24 introduced a number of changes and switches the container runtime to <a href="https://containerd.io/">containerd</a> instead of docker. Containerd helps clusters scale by increasing individual node performance by limiting container runtime features to closely align with Kubernetes&#8217; needs. Containerd is available in every supported version of EKS and if you would like to switch to containerd in versions prior to 1.24 please use the <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html#containerd-bootstrap"><code>--container-runtime</code> bootstrap flag</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_limit_workload_and_node_bursting">Limit workload and node bursting</h3>
<div class="paragraph">
<p>!!! Attention
    To avoid reaching API limits on the control plane you should limit scaling spikes that increase cluster size by double digit percentages at a time (e.g. 1000 nodes to 1100 nodes or 4000 to 4500 pods at once).</p>
</div>
<div class="paragraph">
<p>The EKS control plane will automatically scale as your cluster grows, but there are limits on how fast it will scale. When you first create an EKS cluster the Control Plane will not immediately be able to scale to hundreds of nodes or thousands of pods. To read more about how EKS has made scaling improvements see <a href="https://aws.amazon.com/blogs/containers/amazon-eks-control-plane-auto-scaling-enhancements-improve-speed-by-4x/">this blog post</a>.</p>
</div>
<div class="paragraph">
<p>Scaling large applications requires infrastructure to adapt to become fully ready (e.g. warming load balancers). To control the speed of scaling make sure you are scaling based on the right metrics for your application. CPU and memory scaling may not accurately predict your application constraints and using custom metrics (e.g. requests per second) in Kubernetes Horizontal Pod Autoscaler (HPA) may be a better scaling option.</p>
</div>
<div class="paragraph">
<p>To use a custom metric see the examples in the <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics">Kubernetes documentation</a>. If you have more advanced scaling needs or need to scale based on external sources (e.g. AWS SQS queue) then use <a href="https://keda.sh">KEDA</a> for event based workload scaling.</p>
</div>
</div>
<div class="sect2">
<h3 id="_scale_nodes_and_pods_down_safely">Scale nodes and pods down safely</h3>
<div class="sect3">
<h4 id="_replace_long_running_instances">Replace long running instances</h4>
<div class="paragraph">
<p>Replacing nodes regularly keeps your cluster healthy by avoiding configuration drift and issues that only happen after extended uptime (e.g. slow memory leaks). Automated replacement will give you good process and practices for node upgrades and security patching. If every node in your cluster is replaced regularly then there is less toil required to maintain separate processes for ongoing maintenance.</p>
</div>
<div class="paragraph">
<p>Use Karpenter&#8217;s <a href="https://aws.github.io/aws-eks-best-practices/karpenter/#use-timers-ttl-to-automatically-delete-nodes-from-the-cluster">time to live (TTL)</a> settings to replace instances after they&#8217;ve been running for a specified amount of time. Self managed node groups can use the <code>max-instance-lifetime</code> setting to cycle nodes automatically. Managed node groups do not currently have this feature but you can track the request <a href="https://github.com/aws/containers-roadmap/issues/1190">here on GitHub</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_remove_underutilized_nodes">Remove underutilized nodes</h4>
<div class="paragraph">
<p>You can remove nodes when they have no running workloads using the scale down threshold in the Kubernetes Cluster Autoscaler with the <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-down-work"><code>--scale-down-utilization-threshold</code></a> or in Karpenter you can use the <code>ttlSecondsAfterEmpty</code> provisioner setting.</p>
</div>
</div>
<div class="sect3">
<h4 id="_use_pod_disruption_budgets_and_safe_node_shutdown">Use pod disruption budgets and safe node shutdown</h4>
<div class="paragraph">
<p>Removing pods and nodes from a Kubernetes cluster requires controllers to make updates to multiple resources (e.g. EndpointSlices). Doing this frequently or too quickly can cause API server throttling and application outages as changes propogate to controllers. <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">Pod Disruption Budgets</a> are a best practice to slow down churn to protect workload availability as nodes are removed or rescheduled in a cluster.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_use_client_side_cache_when_running_kubectl">Use Client-Side Cache when running Kubectl</h3>
<div class="paragraph">
<p>Using the kubectl command inefficiently can add additional load to the Kubernetes API Server. You should avoid running scripts or automation that uses kubectl repeatedly (e.g. in a for loop) or running commands without a local cache.</p>
</div>
<div class="paragraph">
<p><code>kubectl</code> has a client-side cache that caches discovery information from the cluster to reduce the amount of API calls required. The cache is enabled by default and is refreshed every 10 minutes.</p>
</div>
<div class="paragraph">
<p>If you run kubectl from a container or without a client-side cache you may run into API throttling issues. It is recommended to retain your cluster cache by mounting the <code>--cache-dir</code> to avoid making uncessesary API calls.</p>
</div>
</div>
<div class="sect2">
<h3 id="_disable_kubectl_compression">Disable kubectl Compression</h3>
<div class="paragraph">
<p>Disabling kubectl compression in your kubeconfig file can reduce API and client CPU usage. By default the server will compress data sent to the client to optimize network bandwidth. This adds CPU load on the client and server for every request and disabling compression can reduce the overhead and latency if you have adequate bandwidth. To disable compression you can use the <code>--disable-compression=true</code> flag or set <code>disable-compression: true</code> in your kubeconfig file.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>apiVersion: v1
clusters:
- cluster:
    server: serverURL
    disable-compression: true
  name: cluster</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_shard_cluster_autoscaler">Shard Cluster Autoscaler</h3>
<div class="paragraph">
<p>The <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/proposals/scalability_tests.md">Kubernetes Cluster Autoscaler has been tested</a> to scale up to 1000 nodes. On a large cluster with more than 1000 nodes, it is recommended to run multiple instances of the Cluster Autoscaler in shard mode. Each Cluster Autoscaler instance is configured to scale a set of node groups. The following example shows 2 cluster autoscaling configurations that are configured to each scale 4 node groups.</p>
</div>
<div class="paragraph">
<p>ClusterAutoscaler-1</p>
</div>
<div class="listingblock">
<div class="content">
<pre>autoscalingGroups:
- name: eks-core-node-grp-20220823190924690000000011-80c1660e-030d-476d-cb0d-d04d585a8fcb
  maxSize: 50
  minSize: 2
- name: eks-data_m1-20220824130553925600000011-5ec167fa-ca93-8ca4-53a5-003e1ed8d306
  maxSize: 450
  minSize: 2
- name: eks-data_m2-20220824130733258600000015-aac167fb-8bf7-429d-d032-e195af4e25f5
  maxSize: 450
  minSize: 2
- name: eks-data_m3-20220824130553914900000003-18c167fa-ca7f-23c9-0fea-f9edefbda002
  maxSize: 450
  minSize: 2</pre>
</div>
</div>
<div class="paragraph">
<p>ClusterAutoscaler-2</p>
</div>
<div class="listingblock">
<div class="content">
<pre>autoscalingGroups:
- name: eks-data_m4-2022082413055392550000000f-5ec167fa-ca86-6b83-ae9d-1e07ade3e7c4
  maxSize: 450
  minSize: 2
- name: eks-data_m5-20220824130744542100000017-02c167fb-a1f7-3d9e-a583-43b4975c050c
  maxSize: 450
  minSize: 2
- name: eks-data_m6-2022082413055392430000000d-9cc167fa-ca94-132a-04ad-e43166cef41f
  maxSize: 450
  minSize: 2
- name: eks-data_m7-20220824130553921000000009-96c167fa-ca91-d767-0427-91c879ddf5af
  maxSize: 450
  minSize: 2</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_api_priority_and_fairness">API Priority and Fairness</h3>
<div class="imageblock">
<div class="content">
<img src="../images/APF.jpg" alt="APF">
</div>
</div>
<div class="sect3">
<h4 id="_overview">Overview<iframe width="560" height="315" src="https://www.youtube.com/embed/YnPPHBawhE0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></h4>
<div class="paragraph">
<p>To protect itself from being overloaded during periods of increased requests, the API Server limits the number of inflight requests it can have outstanding at a given time. Once this limit is exceeded, the API Server will start rejecting requests and return a 429 HTTP response code for "Too Many Requests" back to clients. The server dropping requests and having clients try again later is preferable to having no server-side limits on the number of requests and overloading the control plane, which could result in degraded performance or unavailability.</p>
</div>
<div class="paragraph">
<p>The mechanism used by Kubernetes to configure how these inflights requests are divided among different request types is called <a href="https://kubernetes.io/docs/concepts/cluster-administration/flow-control/">API Priority and Fairness</a>. The API Server configures the total number of inflight requests it can accept by summing together the values specified by the <code>--max-requests-inflight</code> and <code>--max-mutating-requests-inflight</code> flags. EKS uses the default values of 400 and 200 requests for these flags, allowing a total of 600 requests to be dispatched at a given time. However, as it scales the control-plane to larger sizes in response to increased utilization and workload churn, it correspondingly increases the inflight request quota all the way till 2000 (subject to change). APF specifies how these inflight request quota is further sub-divided among different request types. Note that EKS control planes are highly available with at least 2 API Servers registered to each cluster. This means the total number of inflight requests your cluster can handle is twice (or higher if horizontally scaled out further) the inflight quota set per kube-apiserver. This amounts to several thousands of requests/second on the largest EKS clusters.</p>
</div>
<div class="paragraph">
<p>Two kinds of Kubernetes objects, called PriorityLevelConfigurations and FlowSchemas, configure how the total number of requests is divided between different request types. These objects are maintained by the API Server automatically and EKS uses the default configuration of these objects for the given Kubernetes minor version. PriorityLevelConfigurations represent a fraction of the total number of allowed requests. For example, the workload-high PriorityLevelConfiguration is allocated 98 out of the total of 600 requests. The sum of requests allocated to all PriorityLevelConfigurations will equal 600 (or slightly above 600 because the API Server will round up if a given level is granted a fraction of a request). To check the PriorityLevelConfigurations in your cluster and the number of requests allocated to each, you can run the following command. These are the defaults on EKS 1.24:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ kubectl get --raw /metrics | grep apiserver_flowcontrol_request_concurrency_limit
apiserver_flowcontrol_request_concurrency_limit{priority_level="catch-all"} 13
apiserver_flowcontrol_request_concurrency_limit{priority_level="global-default"} 49
apiserver_flowcontrol_request_concurrency_limit{priority_level="leader-election"} 25
apiserver_flowcontrol_request_concurrency_limit{priority_level="node-high"} 98
apiserver_flowcontrol_request_concurrency_limit{priority_level="system"} 74
apiserver_flowcontrol_request_concurrency_limit{priority_level="workload-high"} 98
apiserver_flowcontrol_request_concurrency_limit{priority_level="workload-low"} 245</pre>
</div>
</div>
<div class="paragraph">
<p>The second type of object are FlowSchemas. API Server requests with a given set of properties are classified under the same FlowSchema. These properties include either the authenticated user or attributes of the request, such as the API group, namespace, or resource. A FlowSchema also specifies which PriorityLevelConfiguration this type of request should map to. The two objects together say, "I want this type of request to count towards this share of inflight requests." When a request hits the API Server, it will check each of its FlowSchemas until it finds one that matches all the required properties. If multiple FlowSchemas match a request, the API Server will choose the FlowSchema with the smallest matching precedence which is specified as a property in the object.</p>
</div>
<div class="paragraph">
<p>The mapping of FlowSchemas to PriorityLevelConfigurations can be viewed using this command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ kubectl get flowschemas
NAME                           PRIORITYLEVEL     MATCHINGPRECEDENCE   DISTINGUISHERMETHOD   AGE     MISSINGPL
exempt                         exempt            1                    &lt;none&gt;                7h19m   False
eks-exempt                     exempt            2                    &lt;none&gt;                7h19m   False
probes                         exempt            2                    &lt;none&gt;                7h19m   False
system-leader-election         leader-election   100                  ByUser                7h19m   False
endpoint-controller            workload-high     150                  ByUser                7h19m   False
workload-leader-election       leader-election   200                  ByUser                7h19m   False
system-node-high               node-high         400                  ByUser                7h19m   False
system-nodes                   system            500                  ByUser                7h19m   False
kube-controller-manager        workload-high     800                  ByNamespace           7h19m   False
kube-scheduler                 workload-high     800                  ByNamespace           7h19m   False
kube-system-service-accounts   workload-high     900                  ByNamespace           7h19m   False
eks-workload-high              workload-high     1000                 ByUser                7h14m   False
service-accounts               workload-low      9000                 ByUser                7h19m   False
global-default                 global-default    9900                 ByUser                7h19m   False
catch-all                      catch-all         10000                ByUser                7h19m   False</pre>
</div>
</div>
<div class="paragraph">
<p>PriorityLevelConfigurations can have a type of Queue, Reject, or Exempt. For types Queue and Reject, a limit is enforced on the maximum number of inflight requests for that priority level, however, the behavior differs when that limit is reached. For example, the workload-high PriorityLevelConfiguration uses type Queue and has 98 requests available for use by the controller-manager, endpoint-controller, scheduler,eks related controllers and from pods running in the kube-system namespace. Since type Queue is used, the API Server will attempt to keep requests in memory and hope that the number of inflight requests drops below 98 before these requests time out. If a given request times out in the queue or if too many requests are already queued, the API Server has no choice but to drop the request and return the client a 429. Note that queuing may prevent a request from receiving a 429, but it comes with the tradeoff of increased end-to-end latency on the request.</p>
</div>
<div class="paragraph">
<p>Now consider the catch-all FlowSchema that maps to the catch-all PriorityLevelConfiguration with type Reject. If clients reach the limit of 13 inflight requests, the API Server will not exercise queuing and will drop the requests instantly with a 429 response code. Finally, requests mapping to a PriorityLevelConfiguration with type Exempt will never receive a 429 and always be dispatched immediately. This is used for high-priority requests such as healthz requests or requests coming from the system:masters group.</p>
</div>
</div>
<div class="sect3">
<h4 id="_monitoring_apf_and_dropped_requests">Monitoring APF and Dropped Requests</h4>
<div class="paragraph">
<p>To confirm if any requests are being dropped due to APF, the API Server metrics for <code>apiserver_flowcontrol_rejected_requests_total</code> can be monitored to check the impacted FlowSchemas and PriorityLevelConfigurations. For example, this metric shows that 100 requests from the service-accounts FlowSchema were dropped due to requests timing out in workload-low queues:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>% kubectl get --raw /metrics | grep apiserver_flowcontrol_rejected_requests_total
apiserver_flowcontrol_rejected_requests_total{flow_schema="service-accounts",priority_level="workload-low",reason="time-out"} 100</pre>
</div>
</div>
<div class="paragraph">
<p>To check how close a given PriorityLevelConfiguration is to receiving 429s or experiencing increased latency due to queuing, you can compare the difference between the concurrency limit and the concurrency in use. In this example, we have a buffer of 100 requests.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>% kubectl get --raw /metrics | grep 'apiserver_flowcontrol_request_concurrency_limit.*workload-low'
apiserver_flowcontrol_request_concurrency_limit{priority_level="workload-low"} 245

% kubectl get --raw /metrics | grep 'apiserver_flowcontrol_request_concurrency_in_use.*workload-low'
apiserver_flowcontrol_request_concurrency_in_use{flow_schema="service-accounts",priority_level="workload-low"} 145</pre>
</div>
</div>
<div class="paragraph">
<p>To check if a given PriorityLevelConfiguration is experiencing queuing but not necessarily dropped requests, the metric for <code>apiserver_flowcontrol_current_inqueue_requests</code> can be referenced:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>% kubectl get --raw /metrics | grep 'apiserver_flowcontrol_current_inqueue_requests.*workload-low'
apiserver_flowcontrol_current_inqueue_requests{flow_schema="service-accounts",priority_level="workload-low"} 10</pre>
</div>
</div>
<div class="paragraph">
<p>Other useful Prometheus metrics include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>apiserver_flowcontrol_dispatched_requests_total</p>
</li>
<li>
<p>apiserver_flowcontrol_request_execution_seconds</p>
</li>
<li>
<p>apiserver_flowcontrol_request_wait_duration_seconds</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>See the upstream documentation for a complete list of <a href="https://kubernetes.io/docs/concepts/cluster-administration/flow-control/#observability">APF metrics</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_preventing_dropped_requests">Preventing Dropped Requests</h4>
<div class="sect4">
<h5 id="_prevent_429s_by_changing_your_workload">Prevent 429s by changing your workload</h5>
<div class="paragraph">
<p>When APF is dropping requests due to a given PriorityLevelConfiguration exceeding its maximum number of allowed inflight requests, clients in the affected FlowSchemas can decrease the number of requests executing at a given time. This can be accomplished by reducing the total number of requests made over the period where 429s are occurring. Note that long-running requests such as expensive list calls are especially problematic because they count as an inflight request for the entire duration they are executing. Reducing the number of these expensive requests or optimizing the latency of these list calls (for example, by reducing the number of objects fetched per request or switching to using a watch request) can help reduce the total concurrency required by the given workload.</p>
</div>
</div>
<div class="sect4">
<h5 id="_prevent_429s_by_changing_your_apf_settings">Prevent 429s by changing your APF settings</h5>
<div class="paragraph">
<p>!!! Warning
    Only change default APF settings if you know what you are doing. Misconfigured APF settings can result in dropped API Server requests and significant workload disruptions.</p>
</div>
<div class="paragraph">
<p>One other approach for preventing dropped requests is changing the default FlowSchemas or PriorityLevelConfigurations installed on EKS clusters. EKS installs the upstream default settings for FlowSchemas and PriorityLevelConfigurations for the given Kubernetes minor version. The API Server will automatically reconcile these objects back to their defaults if modified unless the following annotation on the objects is set to false:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>  metadata:
    annotations:
      apf.kubernetes.io/autoupdate-spec: "false"</pre>
</div>
</div>
<div class="paragraph">
<p>At a high-level, APF settings can be modified to either:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Allocate more inflight capacity to requests you care about.</p>
</li>
<li>
<p>Isolate non-essential or expensive requests that can starve capacity for other request types.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This can be accomplished by either changing the default FlowSchemas and PriorityLevelConfigurations or by creating new objects of these types. Operators can increase the values for assuredConcurrencyShares for the relevant PriorityLevelConfigurations objects to increase the fraction of inflight requests they are allocated. Additionally, the number of requests that can be queued at a given time can also be increased if the application can handle the additional latency caused by requests being queued before they are dispatched.</p>
</div>
<div class="paragraph">
<p>Alternatively, new FlowSchema and PriorityLevelConfigurations objects can be created that are specific to the customer&#8217;s workload. Be aware that allocating more assuredConcurrencyShares to either existing PriorityLevelConfigurations or to new PriorityLevelConfigurations will cause the number of requests that can be handled by other buckets to be reduced as the overall limit will stay as 600 inflight per API Server.</p>
</div>
<div class="paragraph">
<p>When making changes to APF defaults, these metrics should be monitored on a non-production cluster to ensure changing the settings do not cause unintended 429s:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>The metric for <code>apiserver_flowcontrol_rejected_requests_total</code> should be monitored for all FlowSchemas to ensure that no buckets start to drop requests.</p>
</li>
<li>
<p>The values for <code>apiserver_flowcontrol_request_concurrency_limit</code> and <code>apiserver_flowcontrol_request_concurrency_in_use</code> should be compared to ensure that the concurrency in use is not at risk for breaching the limit for that priority level.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>One common use-case for defining a new FlowSchema and PriorityLevelConfiguration is for isolation. Suppose we want to isolate long-running list event calls from pods to their own share of requests. This will prevent important requests from pods using the existing service-accounts FlowSchema from receiving 429s and being starved of request capacity. Recall that the total number of inflight requests is finite, however, this example shows APF settings can be modified to better divide request capacity for the given workload:</p>
</div>
<div class="paragraph">
<p>Example FlowSchema object to isolate list event requests:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>apiVersion: flowcontrol.apiserver.k8s.io/v1beta1
kind: FlowSchema
metadata:
  name: list-events-default-service-accounts
spec:
  distinguisherMethod:
    type: ByUser
  matchingPrecedence: 8000
  priorityLevelConfiguration:
    name: catch-all
  rules:
  - resourceRules:
    - apiGroups:
      - '*'
      namespaces:
      - default
      resources:
      - events
      verbs:
      - list
    subjects:
    - kind: ServiceAccount
      serviceAccount:
        name: default
        namespace: default</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>This FlowSchema captures all list event calls made by service accounts in the default namespace.</p>
</li>
<li>
<p>The matching precedence 8000 is lower than the value of 9000 used by the existing service-accounts FlowSchema so these list event calls will match list-events-default-service-accounts rather than service-accounts.</p>
</li>
<li>
<p>We&#8217;re using the catch-all PriorityLevelConfiguration to isolate these requests. This bucket only allows 13 inflight requests to be used by these long-running list event calls. Pods will start to receive 429s as soon they try to issue more than 13 of these requests concurrently.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_retrieving_resources_in_the_api_server">Retrieving resources in the API server</h3>
<div class="paragraph">
<p>Getting information from the API server is an expected behavior for clusters of any size. As you scale the number of resources in the cluster the frequency of requests and volume of data can quickly become a bottleneck for the control plane and will lead to API latency and slowness. Depending on the severity of the latency it cause unexpected downtime if you are not careful.</p>
</div>
<div class="paragraph">
<p>Being aware of what you are requesting and how often are the first steps to avoiding these types of problems. Here is guidance to limit the volume of queries based on the scaling best practices. Suggestions in this section are provided in order starting with the options that are known to scale the best.</p>
</div>
<div class="sect3">
<h4 id="_use_shared_informers">Use Shared Informers</h4>
<div class="paragraph">
<p>When building controllers and automation that integrate with the Kubernetes API you will often need to get information from Kubernetes resources. If you poll for these resources regularly it can cause a significant load on the API server.</p>
</div>
<div class="paragraph">
<p>Using an <a href="https://pkg.go.dev/k8s.io/client-go/informers">informer</a> from the client-go library will give you benefits of watching for changes to the resources based on events instead of polling for changes. Informers further reduce the load by using shared cache for the events and changes so multiple controllers watching the same resources do not add additional load.</p>
</div>
<div class="paragraph">
<p>Controllers should avoid polling cluster wide resources without labels and field selectors especially in large clusters. Each un-filtered poll requires a lot of unnecessary data to be sent from etcd through the API server to be filtered by the client. By filtering based on labels and namespaces you can reduce the amount of work the API server needs to perform to fullfil the request and data sent to the client.</p>
</div>
</div>
<div class="sect3">
<h4 id="_optimize_kubernetes_api_usage">Optimize Kubernetes API usage</h4>
<div class="paragraph">
<p>When calling the Kubernetes API with custom controllers or automation it&#8217;s important that you limit the calls to only the resources you need. Without limits you can cause unneeded load on the API server and etcd.</p>
</div>
<div class="paragraph">
<p>It is recommended that you use the watch argument whenever possible. With no arguments the default behavior is to list objects. To use watch instead of list you can append <code>?watch=true</code> to the end of your API request. For example, to get all pods in the default namespace with a watch use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>/api/v1/namespaces/default/pods?watch=true</pre>
</div>
</div>
<div class="paragraph">
<p>If you are listing objects you should limit the scope of what you are listing and the amount of data returned. You can limit the returned data by adding <code>limit=500</code> argument to requests. The <code>fieldSelector</code> argument and <code>/namespace/</code> path can be useful to make sure your lists are as narrowly scoped as needed. For example, to list only running pods in the default namespace use the following API path and arguments.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>/api/v1/namespaces/default/pods?fieldSelector=status.phase=Running&amp;limit=500</pre>
</div>
</div>
<div class="paragraph">
<p>Or list all pods that are running with:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>/api/v1/pods?fieldSelector=status.phase=Running&amp;limit=500</pre>
</div>
</div>
<div class="paragraph">
<p>Another option to limit watch calls or listed objects is to use <a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions"><code>resourceVersions</code> which you can read about in the Kubernetes documentation</a>. Without a <code>resourceVersion</code> argument you will receive the most recent version available which requires an etcd quorum read which is the most expensive and slowest read for the database. The resourceVersion depends on what resources you are trying to query and can be found in the <code>metadata.resourseVersion</code> field. This is also recommended in case of using watch calls and not just list calls</p>
</div>
<div class="paragraph">
<p>There is a special <code>resourceVersion=0</code> available that will return results from the API server cache. This can reduce etcd load but it does not support pagination.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>/api/v1/namespaces/default/pods?resourceVersion=0</pre>
</div>
</div>
<div class="paragraph">
<p>It&#8217;s recommended to use watch with a resourceVersion set to be the most recent known value received from its preceding list or watch. This is handled automatically in client-go. But it&#8217;s suggested to double check it if you are using a k8s client in other languages.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>/api/v1/namespaces/default/pods?watch=true&amp;resourceVersion=362812295</pre>
</div>
</div>
<div class="paragraph">
<p>If you call the API without any arguments it will be the most resource intensive for the API server and etcd. This call will get all pods in all namespaces without pagination or limiting the scope and require a quorum read from etcd.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>/api/v1/pods</pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_kubernetes_data_plane">Kubernetes Data Plane</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Kubernetes Data Plane includes EC2 instances, load balancers, storage, and other APIs used by the Kubernetes Control Plane. For organization purposes we grouped <a href="./cluster-services.html">cluster services</a> in a separate page and load balancer scaling can be found in the <a href="./workloads.html">workloads section</a>. This section will focus on scaling compute resources.</p>
</div>
<div class="paragraph">
<p>Selecting EC2 instance types is possibly one of the hardest decisions customers face because in clusters with multiple workloads. There is no one-size-fits all solution. Here are some tips to help you avoid common pitfalls with scaling compute.</p>
</div>
<div class="sect2">
<h3 id="_automatic_node_autoscaling">Automatic node autoscaling</h3>
<div class="paragraph">
<p>We recommend you use node autoscaling that reduces toil and integrates deeply with Kubernetes. <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html">Managed node groups</a> and <a href="https://karpenter.sh/">Karpenter</a> are recommended for large scale clusters.</p>
</div>
<div class="paragraph">
<p>Managed node groups will give you the flexibility of Amazon EC2 Auto Scaling groups with added benefits for managed upgrades and configuration. It can be scaled with the <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">Kubernetes Cluster Autoscaler</a> and is a common option for clusters that have a variety of compute needs.</p>
</div>
<div class="paragraph">
<p>Karpenter is an open source, workload-native node autoscaler created by AWS. It scales nodes in a cluster based on the workload requirements for resources (e.g. GPU) and taints and tolerations (e.g. zone spread) without managing node groups. Nodes are created directly from EC2 which avoids default node group quotas&#8212;&#8203;450 nodes per group&#8212;&#8203;and provides greater instance selection flexibility with less operational overhead. We recommend customers use Karpenter when possible.</p>
</div>
</div>
<div class="sect2">
<h3 id="_use_many_different_ec2_instance_types">Use many different EC2 instance types</h3>
<div class="paragraph">
<p>Each AWS region has a limited number of available instances per instance type. If you create a cluster that uses only one instance type and scale the number of nodes beyond the capacity of the region you will receive an error that no instances are available. To avoid this issue you should not arbitrarily limit the type of instances that can be use in your cluster.</p>
</div>
<div class="paragraph">
<p>Karpenter will use a broad set of compatible instance types by default and will pick an instance at provisioning time based on pending workload requirements, availability, and cost. You can broaden the list of instance types used in the <code>karpenter.k8s.aws/instance-category</code> key of <a href="https://karpenter.sh/docs/concepts/nodepools/#instance-types">NodePools</a>.</p>
</div>
<div class="paragraph">
<p>The Kubernetes Cluster Autoscaler requires node groups to be similarly sized so they can be consistently scaled. You should create multiple groups based on CPU and memory size and scale them independently. Use the <a href="https://github.com/aws/amazon-ec2-instance-selector">ec2-instance-selector</a> to identify instances that are similarly sized for your node groups.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>ec2-instance-selector --service eks --vcpus-min 8 --memory-min 16
a1.2xlarge
a1.4xlarge
a1.metal
c4.4xlarge
c4.8xlarge
c5.12xlarge
c5.18xlarge
c5.24xlarge
c5.2xlarge
c5.4xlarge
c5.9xlarge
c5.metal</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_prefer_larger_nodes_to_reduce_api_server_load">Prefer larger nodes to reduce API server load</h3>
<div class="paragraph">
<p>When deciding what instance types to use, fewer, large nodes will put less load on the Kubernetes Control Plane because there will be fewer kubelets and DaemonSets running. However, large nodes may not be utilized fully like smaller nodes. Node sizes should be evaluated based on your workload availability and scale requirements.</p>
</div>
<div class="paragraph">
<p>A cluster with three u-24tb1.metal instances (24 TB memory and 448 cores) has 3 kubelets, and would be limited to 110 pods per node by default. If your pods use 4 cores each then this might be expected (4 cores x 110 = 440 cores/node). With a 3 node cluster your ability to handle an instance incident would be low because 1 instance outage could impact 1/3 of the cluster. You should specify node requirements and pod spread in your workloads so the Kubernetes scheduler can place workloads properly.</p>
</div>
<div class="paragraph">
<p>Workloads should define the resources they need and the availability required via taints, tolerations, and <a href="https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/">PodTopologySpread</a>. They should prefer the largest nodes that can be fully utilized and meet availability goals to reduce control plane load, lower operations, and reduce cost.</p>
</div>
<div class="paragraph">
<p>The Kubernetes Scheduler will automatically try to spread workloads across availability zones and hosts if resources are available. If no capacity is available the Kubernetes Cluster Autoscaler will attempt to add nodes in each Availability Zone evenly. Karpenter will attempt to add nodes as quickly and cheaply as possible unless the workload specifies other requirements.</p>
</div>
<div class="paragraph">
<p>To force workloads to spread with the scheduler and new nodes to be created across availability zones you should use topologySpreadConstraints:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>spec:
  topologySpreadConstraints:
    - maxSkew: 3
      topologyKey: "topology.kubernetes.io/zone"
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          dev: my-deployment
    - maxSkew: 2
      topologyKey: "kubernetes.io/hostname"
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          dev: my-deployment</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_use_similar_node_sizes_for_consistent_workload_performance">Use similar node sizes for consistent workload performance</h3>
<div class="paragraph">
<p>Workloads should define what size nodes they need to be run on to allow consistent performance and predictable scaling. A workload requesting 500m CPU will perform differently on an instance with 4 cores vs one with 16 cores. Avoid instance types that use burstable CPUs like T series instances.</p>
</div>
<div class="paragraph">
<p>To make sure your workloads get consistent performance a workload can use the <a href="https://karpenter.sh/docs/concepts/scheduling/#labels">supported Karpenter labels</a> to target specific instances sizes.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>kind: deployment
...
spec:
  template:
    spec:
    containers:
    nodeSelector:
      karpenter.k8s.aws/instance-size: 8xlarge</pre>
</div>
</div>
<div class="paragraph">
<p>Workloads being scheduled in a cluster with the Kubernetes Cluster Autoscaler should match a node selector to node groups based on label matching.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: eks.amazonaws.com/nodegroup
            operator: In
            values:
            - 8-core-node-group    # match your node group name</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_use_compute_resources_efficiently">Use compute resources efficiently</h3>
<div class="paragraph">
<p>Compute resources include EC2 instances and availability zones. Using compute resources effectively will increase your scalability, availability, performance, and reduce your total cost. Efficient resource usage is extremely difficult to predict in an autoscaling environment with multiple applications. <a href="https://karpenter.sh/">Karpenter</a> was created to provision instances on-demand based on the workload needs to maximize utilization and flexibility.</p>
</div>
<div class="paragraph">
<p>Karpenter allows workloads to declare the type of compute resources it needs without first creating node groups or configuring label taints for specific nodes. See the <a href="https://aws.github.io/aws-eks-best-practices/karpenter/">Karpenter best practices</a> for more information. Consider enabling <a href="https://aws.github.io/aws-eks-best-practices/karpenter/#configure-requestslimits-for-all-non-cpu-resources-when-using-consolidation">consolidation</a> in your Karpenter provisioner to replace nodes that are under utilized.</p>
</div>
</div>
<div class="sect2">
<h3 id="_automate_amazon_machine_image_ami_updates">Automate Amazon Machine Image (AMI) updates</h3>
<div class="paragraph">
<p>Keeping worker node components up to date will make sure you have the latest security patches and compatible features with the Kubernetes API. Updating the kubelet is the most important component for Kubernetes functionality, but automating OS, kernel, and locally installed application patches will reduce maintenance as you scale.</p>
</div>
<div class="paragraph">
<p>It is recommended that you use the latest <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html">Amazon EKS optimized Amazon Linux 2</a> or <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami-bottlerocket.html">Amazon EKS optimized Bottlerocket AMI</a> for your node image. Karpenter will automatically use the <a href="https://karpenter.sh/docs/concepts/nodepools/#instance-types">latest available AMI</a> to provision new nodes in the cluster. Managed node groups will update the AMI during a <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html">node group update</a> but will not update the AMI ID at node provisioning time.</p>
</div>
<div class="paragraph">
<p>For Managed Node Groups you need to update the Auto Scaling Group (ASG) launch template with new AMI IDs when they are available for patch releases. AMI minor versions (e.g. 1.23.5 to 1.24.3) will be available in the EKS console and API as <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html">upgrades for the node group</a>. Patch release versions (e.g. 1.23.5 to 1.23.6) will not be presented as upgrades for the node groups. If you want to keep your node group up to date with AMI patch releases you need to create new launch template version and let the node group replace instances with the new AMI release.</p>
</div>
<div class="paragraph">
<p>You can find the latest available AMI from <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html">this page</a> or use the AWS CLI.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>aws ssm get-parameter \
  --name /aws/service/eks/optimized-ami/1.24/amazon-linux-2/recommended/image_id \
  --query "Parameter.Value" \
  --output text</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_use_multiple_ebs_volumes_for_containers">Use multiple EBS volumes for containers</h3>
<div class="paragraph">
<p>EBS volumes have input/output (I/O) quota based on the type of volume (e.g. gp3) and the size of the disk. If your applications share a single EBS root volume with the host this can exhaust the disk quota for the entire host and cause other applications to wait for available capacity. Applications write to disk if they write files to their overlay partition, mount a local volume from the host, and also when they log to standard out (STDOUT) depending on the logging agent used.</p>
</div>
<div class="paragraph">
<p>To avoid disk I/O exhaustion you should mount a second volume to the container state folder (e.g. /run/containerd), use separate EBS volumes for workload storage, and disable unnecessary local logging.</p>
</div>
<div class="paragraph">
<p>To mount a second volume to your EC2 instances using <a href="https://eksctl.io/">eksctl</a> you can use a node group with this configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>managedNodeGroups:
  - name: al2-workers
    amiFamily: AmazonLinux2
    desiredCapacity: 2
    volumeSize: 80
    additionalVolumes:
      - volumeName: '/dev/sdz'
        volumeSize: 100
    preBootstrapCommands:
    - |
      "systemctl stop containerd"
      "mkfs -t ext4 /dev/nvme1n1"
      "rm -rf /var/lib/containerd/*"
      "mount /dev/nvme1n1 /var/lib/containerd/"
      "systemctl start containerd"</pre>
</div>
</div>
<div class="paragraph">
<p>If you are using terraform to provision your node groups please see examples in <a href="https://aws-ia.github.io/terraform-aws-eks-blueprints/patterns/stateful/#eks-managed-nodegroup-w-multiple-volumes">EKS Blueprints for terraform</a>. If you are using Karpenter to provision nodes you can use <a href="https://karpenter.sh/docs/concepts/nodeclasses/#specblockdevicemappings"><code>blockDeviceMappings</code></a> with node user-data to add additional volumes.</p>
</div>
<div class="paragraph">
<p>To mount an EBS volume directly to your pod you should use the <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">AWS EBS CSI driver</a> and consume a volume with a storage class.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-sc
provisioner: ebs.csi.aws.com
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-claim
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ebs-sc
  resources:
    requests:
      storage: 4Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: public.ecr.aws/docker/library/nginx
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: ebs-claim</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_avoid_instances_with_low_ebs_attach_limits_if_workloads_use_ebs_volumes">Avoid instances with low EBS attach limits if workloads use EBS volumes</h3>
<div class="paragraph">
<p>EBS is one of the easiest ways for workloads to have persistent storage, but it also comes with scalability limitations. Each instance type has a maximum number of <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/volume_limits.html">EBS volumes that can be attached</a>. Workloads need to declare what instance types they should run on and limit the number of replicas on a single instance with Kubernetes taints.</p>
</div>
</div>
<div class="sect2">
<h3 id="_disable_unnecessary_logging_to_disk">Disable unnecessary logging to disk</h3>
<div class="paragraph">
<p>Avoid unnecessary local logging by not running your applications with debug logging in production and disabling logging that reads and writes to disk frequently. Journald is the local logging service that keeps a log buffer in memory and flushes to disk periodically. Journald is preferred over syslog which logs every line immediately to disk. Disabling syslog also lowers the total amount of storage you need and avoids needing complicated log rotation rules. To disable syslog you can add the following snippet to your cloud-init configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>runcmd:
  - [ systemctl, disable, --now, syslog.service ]</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_patch_instances_in_place_when_os_update_speed_is_a_necessity">Patch instances in place when OS update speed is a necessity</h3>
<div class="paragraph">
<p>!!! Attention
    Patching instances in place should only be done when required. Amazon recommends treating infrastructure as immutable and thoroughly testing updates that are promoted through lower environments the same way applications are. This section applies when that is not possible.</p>
</div>
<div class="paragraph">
<p>It takes seconds to install a package on an existing Linux host without disrupting containerized workloads. The package can be installed and validated without cordoning, draining, or replacing the instance.</p>
</div>
<div class="paragraph">
<p>To replace an instance you first need to create, validate, and distribute new AMIs. The instance needs to have a replacement created, and the old instance needs to be cordoned and drained. Then workloads need to be created on the new instance, verified, and repeated for all instances that need to be patched. It takes hours, days, or weeks to replace instances safely without disrupting workloads.</p>
</div>
<div class="paragraph">
<p>Amazon recommends using immutable infrastructure that is built, tested, and promoted from an automated, declarative system, but if you have a requirement to patch systems quickly then you will need to patch systems in place and replace them as new AMIs are made available. Because of the large time differential between patching and replacing systems we recommend using <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html">AWS Systems Manager Patch Manager</a> to automate patching nodes when required to do so.</p>
</div>
<div class="paragraph">
<p>Patching nodes will allow you to quickly roll out security updates and replace the instances on a regular schedule after your AMI has been updated. If you are using an operating system with a read-only root file system like <a href="https://flatcar-linux.org/">Flatcar Container Linux</a> or <a href="https://github.com/bottlerocket-os/bottlerocket">Bottlerocket OS</a> we recommend using the update operators that work with those operating systems. The <a href="https://github.com/flatcar/flatcar-linux-update-operator">Flatcar Linux update operator</a> and <a href="https://github.com/bottlerocket-os/bottlerocket-update-operator">Bottlerocket update operator</a> will reboot instances to keep nodes up to date automatically.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cluster_services">Cluster Services</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Cluster services run inside an EKS cluster, but they are not user workloads. If you have a Linux server you often need to run services like NTP, syslog, and a container runtime to support your workloads. Cluster services are similar, supporting services that help you automate and operate your cluster. In Kubernetes these are usually run in the kube-system namespace and some are run as <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSets</a>.</p>
</div>
<div class="paragraph">
<p>Cluster services are expected to have a high up-time and are often critical during outages and for troubleshooting. If a core cluster service is not available you may lose access to data that can help recover or prevent an outage (e.g. high disk utilization). They should run on dedicated compute instances such as a separate node group or AWS Fargate. This will ensure that the cluster services are not impacted on shared instances by workloads that may be scaling up or using more resources.</p>
</div>
<div class="sect2">
<h3 id="_scale_coredns">Scale CoreDNS</h3>
<div class="paragraph">
<p>Scaling CoreDNS has two primary mechanisms. Reducing the number of calls to the CoreDNS service and increasing the number of replicas.</p>
</div>
<div class="sect3">
<h4 id="_reduce_external_queries_by_lowering_ndots">Reduce external queries by lowering ndots</h4>
<div class="paragraph">
<p>The ndots setting specifies how many periods (a.k.a. "dots") in a domain name are considered enough to avoid querying DNS. If your application has an ndots setting of 5 (default) and you request resources from an external domain such as api.example.com (2 dots) then CoreDNS will be queried for each search domain defined in /etc/resolv.conf for a more specific domain. By default the following domains will be searched before making an external request.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>api.example.&lt;namespace&gt;.svc.cluster.local
api.example.svc.cluster.local
api.example.cluster.local
api.example.&lt;region&gt;.compute.internal</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>namespace</code> and <code>region</code> values will be replaced with your workloads namespace and your compute region. You may have additional search domains based on your cluster settings.</p>
</div>
<div class="paragraph">
<p>You can reduce the number of requests to CoreDNS by <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config">lowering the ndots option</a> of your workload or fully qualifying your domain requests by including a trailing . (e.g. <code>api.example.com.</code> ). If your workload connects to external services via DNS we recommend setting ndots to 2 so workloads do not make unnecessary, cluster DNS queries inside the cluster. You can set a different DNS server and search domain if the workload doesn&#8217;t require access to services inside the cluster.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>spec:
  dnsPolicy: "None"
  dnsConfig:
    options:
      - name: ndots
        value: "2"
      - name: edns0</pre>
</div>
</div>
<div class="paragraph">
<p>If you lower ndots to a value that is too low or the domains you are connecting to do not include enough specificity (including trailing .) then it is possible DNS lookups will fail. Make sure you test how this setting will impact your workloads.</p>
</div>
</div>
<div class="sect3">
<h4 id="_scale_coredns_horizontally">Scale CoreDNS Horizontally</h4>
<div class="paragraph">
<p>CoreDNS instances can scale by adding additional replicas to the deployment. It&#8217;s recommended you use <a href="https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/">NodeLocal DNS</a> or the <a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster proportional autoscaler</a> to scale CoreDNS.</p>
</div>
<div class="paragraph">
<p>NodeLocal DNS will require run one instance per node&#8212;&#8203;as a DaemonSet&#8212;&#8203;which requires more compute resources in the cluster, but it will avoid failed DNS requests and decrease the response time for DNS queries in the cluster. The cluster proportional autoscaler will scale CoreDNS based on the number of nodes or cores in the cluster. This isn&#8217;t a direct correlation to request queries, but can be useful depending on your workloads and cluster size. The default proportional scale is to add an additional replica for every 256 cores or 16 nodes in the cluster&#8212;&#8203;whichever happens first.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_scale_kubernetes_metrics_server_vertically">Scale Kubernetes Metrics Server Vertically</h3>
<div class="paragraph">
<p>The Kubernetes Metrics Server supports horizontal and vertical scaling. By horizontally scaling the Metrics Server it will be highly available, but it will not scale horizontally to handle more cluster metrics. You will need to vertically scale the Metrics Server based on <a href="https://kubernetes-sigs.github.io/metrics-server/#scaling">their recommendations</a> as nodes and collected metrics are added to the cluster.</p>
</div>
<div class="paragraph">
<p>The Metrics Server keeps the data it collects, aggregates, and serves in memory. As a cluster grows, the amount of data the Metrics Server stores increases. In large clusters the Metrics Server will require more compute resources than the memory and CPU reservation specified in the default installation. You can use the <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">Vertical Pod Autoscaler</a> (VPA) or <a href="https://github.com/kubernetes/autoscaler/tree/master/addon-resizer">Addon Resizer</a> to scale the Metrics Server. The Addon Resizer scales vertically in proportion to worker nodes and VPA scales based on CPU and memory usage.</p>
</div>
</div>
<div class="sect2">
<h3 id="_coredns_lameduck_duration">CoreDNS lameduck duration</h3>
<div class="paragraph">
<p>Pods use the <code>kube-dns</code> Service for name resolution. Kubernetes uses destination NAT (DNAT) to redirect <code>kube-dns</code> traffic from nodes to CoreDNS backend pods. As you scale the CoreDNS Deployment, <code>kube-proxy</code> updates iptables rules and chains on nodes to redirect DNS traffic to CoreDNS pods. Propagating new endpoints when you scale up and deleting rules when you scale down CoreDNS can take between 1 to 10 seconds depending on the size of the cluster.</p>
</div>
<div class="paragraph">
<p>This propagation delay can cause DNS lookup failures when a CoreDNS pod gets terminated yet the node&#8217;s iptables rules haven&#8217;t been updated. In this scenario, the node may continue to send DNS queries to a terminated CoreDNS Pod.</p>
</div>
<div class="paragraph">
<p>You can reduce DNS lookup failures by setting a <a href="https://coredns.io/plugins/health/">lameduck</a> duration in your CoreDNS pods. While in lameduck mode, CoreDNS will continue to respond to in-flight requests. Setting a lameduck duration will delay the CoreDNS shutdown process, allowing nodes the time they need to update their iptables rules and chains.</p>
</div>
<div class="paragraph">
<p>We recommend setting CoreDNS lameduck duration to 30 seconds.</p>
</div>
</div>
<div class="sect2">
<h3 id="_coredns_readiness_probe">CoreDNS readiness probe</h3>
<div class="paragraph">
<p>We recommend using <code>/ready</code> instead of <code>/health</code> for CoreDNS&#8217;s readiness probe.</p>
</div>
<div class="paragraph">
<p>In alignment with the earlier recommendation to set the lameduck duration to 30 seconds, providing ample time for the node&#8217;s iptables rules to be updated before pod termination, employing <code>/ready</code> instead of <code>/health</code> for the CoreDNS readiness probe ensures that the CoreDNS pod is fully prepared at startup to promptly respond to DNS requests.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">readinessProbe:
  httpGet:
    path: /ready
    port: 8181
    scheme: HTTP</code></pre>
</div>
</div>
<div class="paragraph">
<p>For more information about the CoreDNS Ready plugin please refer to <a href="https://coredns.io/plugins/ready/" class="bare">https://coredns.io/plugins/ready/</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_logging_and_monitoring_agents">Logging and monitoring agents</h3>
<div class="paragraph">
<p>Logging and monitoring agents can add significant load to your cluster control plane because the agents query the API server to enrich logs and metrics with workload metadata. The agent on a node only has access to the local node resources to see things like container and process name. Querying the API server it can add more details such as Kubernetes deployment name and labels. This can be extremely helpful for troubleshooting but detrimental to scaling.</p>
</div>
<div class="paragraph">
<p>Because there are so many different options for logging and monitoring we cannot show examples for every provider. With <a href="https://docs.fluentbit.io/manual/pipeline/filters/kubernetes">fluentbit</a> we recommend enabling Use_Kubelet to fetch metadata from the local kubelet instead of the Kubernetes API Server and set <code>Kube_Meta_Cache_TTL</code> to a number that reduces repeated calls when data can be cached (e.g. 60).</p>
</div>
<div class="paragraph">
<p>Scaling monitoring and logging has two general options:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Disable integrations</p>
</li>
<li>
<p>Sampling and filtering</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Disabling integrations is often not an option because you lose log metadata. This eliminates the API scaling problem, but it will introduce other issues by not having the required metadata when needed.</p>
</div>
<div class="paragraph">
<p>Sampling and filtering reduces the number of metrics and logs that are collected. This will lower the amount of requests to the Kubernetes API, and it will reduce the amount of storage needed for the metrics and logs that are collected. Reducing the storage costs will lower the cost for the overall system.</p>
</div>
<div class="paragraph">
<p>The ability to configure sampling depends on the agent software and can be implemented at different points of ingestion. It&#8217;s important to add sampling as close to the agent as possible because that is likely where the API server calls happen. Contact your provider to find out more about sampling support.</p>
</div>
<div class="paragraph">
<p>If you are using CloudWatch and CloudWatch Logs you can add agent filtering using patterns <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html">described in the documentation</a>.</p>
</div>
<div class="paragraph">
<p>To avoid losing logs and metrics you should send your data to a system that can buffer data in case of an outage on the receiving endpoint. With fluentbit you can use <a href="https://docs.fluentbit.io/manual/pipeline/outputs/firehose">Amazon Kinesis Data Firehose</a> to temporarily keep data which can reduce the chance of overloading your final data storage location.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_workloads">Workloads</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Workloads have an impact on how large your cluster can scale. Workloads that use the Kubernetes APIs heavily will limit the total amount of workloads you can have in a single cluster, but there are some defaults you can change to help reduce the load.</p>
</div>
<div class="paragraph">
<p>Workloads in a Kubernetes cluster have access to features that integrate with the Kubernetes API (e.g. Secrets and ServiceAccounts), but these features are not always required and should be disabled if they&#8217;re not being used. Limiting workload access and dependence on the Kubernetes control plane will increase the number of workloads you can run in the cluster and improve the security of your clusters by removing unnecessary access to workloads and implementing least privilege practices. Please read the <a href="https://aws.github.io/aws-eks-best-practices/security/docs/">security best practices</a> for more information.</p>
</div>
<div class="sect2">
<h3 id="_use_ipv6_for_pod_networking">Use IPv6 for pod networking</h3>
<div class="paragraph">
<p>You cannot transition a VPC from IPv4 to IPv6 so enabling IPv6 before provisioning a cluster is important. If you enable IPv6 in a VPC it does not mean you have to use it and if your pods and services use IPv6 you can still route traffic to and from IPv4 addresses. Please see the <a href="https://aws.github.io/aws-eks-best-practices/networking/index/">EKS networking best practices</a> for more information.</p>
</div>
<div class="paragraph">
<p>Using <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-ipv6.html">IPv6 in your cluster</a> avoids some of the most common cluster and workload scaling limits. IPv6 avoids IP address exhaustion where pods and nodes cannot be created because no IP address is available. It also has per node performance improvements because pods receive IP addresses faster by reducing the number of ENI attachments per node. You can achieve similar node performance by using <a href="https://aws.github.io/aws-eks-best-practices/networking/prefix-mode/">IPv4 prefix mode in the VPC CNI</a>, but you still need to make sure you have enough IP addresses available in the VPC.</p>
</div>
</div>
<div class="sect2">
<h3 id="_limit_number_of_services_per_namespace">Limit number of services per namespace</h3>
<div class="paragraph">
<p>The maximum number of <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md">services in a namespaces is 5,000 and the maximum number of services in a cluster is 10,000</a>. To help organize workloads and services, increase performance, and to avoid cascading impact for namespace scoped resources we recommend limiting the number of services per namespace to 500.</p>
</div>
<div class="paragraph">
<p>The number of IP tables rules that are created per node with kube-proxy grows with the total number of services in the cluster. Generating thousands of IP tables rules and routing packets through those rules have a performance impact on the nodes and add network latency.</p>
</div>
<div class="paragraph">
<p>Create Kubernetes namespaces that encompass a single application environment so long as the number of services per namespace is under 500. This will keep service discovery small enough to avoid service discovery limits and can also help you avoid service naming collisions. Applications environments (e.g. dev, test, prod) should use separate EKS clusters instead of namespaces.</p>
</div>
</div>
<div class="sect2">
<h3 id="_understand_elastic_load_balancer_quotas">Understand Elastic Load Balancer Quotas</h3>
<div class="paragraph">
<p>When creating your services consider what type of load balancing you will use (e.g. Network Load Balancer (NLB) or Application Load Balancer (ALB)). Each load balancer type provides different functionality and have <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-limits.html">different quotas</a>. Some of the default quotas can be adjusted, but there are some quota maximums which cannot be changed. To view your account quotas and usage view the <a href="http://console.aws.amazon.com/servicequotas">Service Quotas dashboard</a> in the AWS console.</p>
</div>
<div class="paragraph">
<p>For example, the default ALB targets is 1000. If you have a service with more than 1000 endpoints you will need to increase the quota or split the service across multiple ALBs or use Kubernetes Ingress. The default NLB targets is 3000, but is limited to 500 targets per AZ. If your cluster runs more than 500 pods for an NLB service you will need to use multiple AZs or request a quota limit increase.</p>
</div>
<div class="paragraph">
<p>An alternative to using a load balancer coupled to a service is to use an <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">ingress controller</a>. The AWS Load Balancer controller can create ALBs for ingress resources, but you may consider running a dedicated controller in your cluster. An in-cluster ingress controller allows you to expose multiple Kubernetes services from a single load balancer by running a reverse proxy inside your cluster. Controllers have different features such as support for the <a href="https://gateway-api.sigs.k8s.io/">Gateway API</a> which may have benefits depending on how many and how large your workloads are.</p>
</div>
</div>
<div class="sect2">
<h3 id="_use_route_53_global_accelerator_or_cloudfront">Use Route 53, Global Accelerator, or CloudFront</h3>
<div class="paragraph">
<p>To make a service using multiple load balancers available as a single endpoint you need to use <a href="https://aws.amazon.com/cloudfront/">Amazon CloudFront</a>, <a href="https://aws.amazon.com/global-accelerator/">AWS Global Accelerator</a>, or <a href="https://aws.amazon.com/route53/">Amazon Route 53</a> to expose all of the load balancers as a single, customer facing endpoint. Each options has different benefits and can be used separately or together depending on your needs.</p>
</div>
<div class="paragraph">
<p>Route 53 can expose multiple load balancers under a common name and can send traffic to each of them based on the weight assigned. You can read more about <a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html#rrsets-values-weighted-weight">DNS weights in the documentation</a> and you can read how to implement them with the <a href="https://github.com/kubernetes-sigs/external-dns">Kubernetes external DNS controller</a> in the <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/integrations/external_dns/#usage">AWS Load Balancer Controller documentation</a>.</p>
</div>
<div class="paragraph">
<p>Global Accelerator can route workloads to the nearest region based on request IP address. This may be useful for workloads that are deployed to multiple regions, but it does not improve routing to a single cluster in a single region. Using Route 53 in combination with the Global Accelerator has additional benefits such as health checking and automatic failover if an AZ is not available. You can see an example of using Global Accelerator with Route 53 in <a href="https://aws.amazon.com/blogs/containers/operating-a-multi-regional-stateless-application-using-amazon-eks/">this blog post</a>.</p>
</div>
<div class="paragraph">
<p>CloudFront can be use with Route 53 and Global Accelerator or by itself to route traffic to multiple destinations. CloudFront caches assets being served from the origin sources which may reduce bandwidth requirements depending on what you are serving.</p>
</div>
</div>
<div class="sect2">
<h3 id="_use_endpointslices_instead_of_endpoints">Use EndpointSlices instead of Endpoints</h3>
<div class="paragraph">
<p>When discovering pods that match a service label you should use <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/">EndpointSlices</a> instead of Endpoints. Endpoints were a simple way to expose services at small scales but large services that automatically scale or have updates causes a lot of traffic on the Kubernetes control plane. EndpointSlices have automatic grouping which enable things like topology aware hints.</p>
</div>
<div class="paragraph">
<p>Not all controllers use EndpointSlices by default. You should verify your controller settings and enable it if needed. For the <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/deploy/configurations/#controller-command-line-flags">AWS Load Balancer Controller</a> you should enable the <code>--enable-endpoint-slices</code> optional flag to use EndpointSlices.</p>
</div>
</div>
<div class="sect2">
<h3 id="_use_immutable_and_external_secrets_if_possible">Use immutable and external secrets if possible</h3>
<div class="paragraph">
<p>The kubelet keeps a cache of the current keys and values for the Secrets that are used in volumes for pods on that node. The kubelet sets a watch on the Secrets to detect changes. As the cluster scales, the growing number of watches can negatively impact the API server performance.</p>
</div>
<div class="paragraph">
<p>There are two strategies to reduce the number of watches on Secrets:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>For applications that don&#8217;t need access to Kubernetes resources, you can disable auto-mounting service account secrets by setting automountServiceAccountToken: false</p>
</li>
<li>
<p>If your application&#8217;s secrets are static and will not be modified in the future, mark the <a href="https://kubernetes.io/docs/concepts/configuration/secret/#secret-immutable">secret as immutable</a>. The kubelet does not maintain an API watch for immutable secrets.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To disable automatically mounting a service account to pods you can use the following setting in your workload. You can override these settings if specific workloads need a service account.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>apiVersion: v1
kind: ServiceAccount
metadata:
  name: app
automountServiceAccountToken: true</pre>
</div>
</div>
<div class="paragraph">
<p>Monitor the number of secrets in the cluster before it exceeds the limit of 10,000. You can see a total count of secrets in a cluster with the following command. You should monitor this limit through your cluster monitoring tooling.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>kubectl get secrets -A | wc -l</pre>
</div>
</div>
<div class="paragraph">
<p>You should set up monitoring to alert a cluster admin before this limit is reached. Consider using external secrets management options such as <a href="https://aws.amazon.com/kms/">AWS Key Management Service (AWS KMS)</a> or <a href="https://www.vaultproject.io/">Hashicorp Vault</a> with the <a href="https://secrets-store-csi-driver.sigs.k8s.io/">Secrets Store CSI driver</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_limit_deployment_history">Limit Deployment history</h3>
<div class="paragraph">
<p>Pods can be slow when creating, updating, or deleting because old objects are still tracked in the cluster. You can reduce the <code>revisionHistoryLimit</code> of <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#clean-up-policy">deployments</a> to cleanup older ReplicaSets which will lower to total amount of objects tracked by the Kubernetes Controller Manager. The default history limit for Deployments in 10.</p>
</div>
<div class="paragraph">
<p>If your cluster creates a lot of job objects through CronJobs or other mechanisms you should use the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/"><code>ttlSecondsAfterFinished</code> setting</a> to automatically clean up old pods in the cluster. This will remove successfully executed jobs from the job history after a specified amount of time.</p>
</div>
</div>
<div class="sect2">
<h3 id="_disable_enableservicelinks_by_default">Disable enableServiceLinks by default</h3>
<div class="paragraph">
<p>When a Pod runs on a Node, the kubelet adds a set of environment variables for each active Service. Linux processes have a maximum size for their environment which can be reached if you have too many services in your namespace. The number of services per namespace should not exceed 5,000. After this, the number of service environment variables outgrows shell limits, causing Pods to crash on startup.</p>
</div>
<div class="paragraph">
<p>There are other reasons pods should not use service environment variables for service discovery. Environment variable name clashes, leaking service names, and total environment size are a few. You should use CoreDNS for discovering service endpoints.</p>
</div>
</div>
<div class="sect2">
<h3 id="_limit_dynamic_admission_webhooks_per_resource">Limit dynamic admission webhooks per resource</h3>
<div class="paragraph">
<p><a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">Dynamic Admission Webhooks</a> include admission webhooks and mutating webhooks. They are API endpoints not part of the Kubernetes Control Plane that are called in sequence when a resource is sent to the Kubernetes API. Each webhook has a default timeout of 10 seconds and can increase the amount of time an API request takes if you have multiple webhooks or any of them timeout.</p>
</div>
<div class="paragraph">
<p>Make sure your webhooks are highly available&#8212;&#8203;especially during an AZ incident&#8212;&#8203;and the <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#failure-policy">failurePolicy</a> is set properly to reject the resource or ignore the failure. Do not call webhooks when not needed by allowing --dry-run kubectl commands to bypass the webhook.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>apiVersion: admission.k8s.io/v1
kind: AdmissionReview
request:
  dryRun: False</pre>
</div>
</div>
<div class="paragraph">
<p>Mutating webhooks can modify resources in frequent succession. If you have 5 mutating webhooks and deploy 50 resources etcd will store all versions of each resource until compaction runs&#8212;&#8203;every 5 minutes&#8212;&#8203;to remove old versions of modified resources. In this scenario when etcd removes superseded resources there will be 200 resource version removed from etcd and depending on the size of the resources may use considerable space on the etcd host until defragmentation runs every 15 minutes.</p>
</div>
<div class="paragraph">
<p>This defragmentation may cause pauses in etcd which could have other affects on the Kubernetes API and controllers. You should avoid frequent modification of large resources or modifying hundreds of resources in quick succession.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_kubernetes_scaling_theory">Kubernetes Scaling Theory</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_nodes_vs_churn_rate">Nodes vs. Churn Rate</h3>
<div class="paragraph">
<p>Often when we discuss the scalability of Kubernetes, we do so in terms of how many nodes there are in a single cluster. Interestingly, this is seldom the most useful metric for understanding scalability. For example, a 5,000 node cluster with a large but fixed number of pods would not put a great deal of stress on the control plane after the initial setup. However, if we took a 1,000 node cluster and tried creating 10,000 short lived jobs in less than a minute, it would put a great deal of sustained pressure on the control plane.</p>
</div>
<div class="paragraph">
<p>Simply using the number of nodes to understand scaling can be misleading. It&#8217;s better to think in terms of the rate of change that occurs within a specific period of time (let&#8217;s use a 5 minute interval for this discussion, as this is what Prometheus queries typically use by default). Let&#8217;s explore why framing the problem in terms of the rate of change can give us a better idea of what to tune to achieve our desired scale.</p>
</div>
</div>
<div class="sect2">
<h3 id="_thinking_in_queries_per_second">Thinking in Queries Per Second</h3>
<div class="paragraph">
<p>Kubernetes has a number of protection mechanisms for each component - the Kubelet, Scheduler, Kube Controller Manager, and API server - to prevent overwhelming the next link in the Kubernetes chain. For example, the Kubelet has a flag to throttle calls to the API server at a certain rate. These protection mechanisms are generally, but not always, expressed in terms of queries allowed on a per second basis or QPS.</p>
</div>
<div class="paragraph">
<p>Great care must be taken when changing these QPS settings. Removing one bottleneck, such as the queries per second on a Kubelet will have an impact on other down stream components. This can and will overwhelm the system above a certain rate, so understanding and monitoring each part of the service chain is key to successfully scaling workloads on Kubernetes.</p>
</div>
<div class="paragraph">
<p>!!! note
    The API server has a more complex system with introduction of API Priority and Fairness which we will discuss separately.</p>
</div>
<div class="paragraph">
<p>!!! note
    Caution, some metrics seem like the right fit but are in fact measuring something else. As an example, <code>kubelet_http_inflight_requests</code> relates to just the metrics server in Kubelet, not the number of requests from Kubelet to apiserver requests. This could cause us to misconfigure the QPS flag on the Kubelet. A query on audit logs for a particular Kubelet would be a more reliable way to check metrics.</p>
</div>
</div>
<div class="sect2">
<h3 id="_scaling_distributed_components">Scaling Distributed Components</h3>
<div class="paragraph">
<p>Since EKS is a managed service, let&#8217;s split the Kubernetes components into two categories: AWS managed components which include etcd, Kube Controller Manager, and the Scheduler (on the left part of diagram), and customer configurable components such as the Kubelet, Container Runtime, and the various operators that call AWS APIs such as the Networking and Storage drivers (on the right part of diagram). We leave the API server in the middle even though it is AWS managed, as the settings for API Priority and Fairness can be configured by customers.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/k8s-components.png" alt="Kubernetes components">
</div>
</div>
</div>
<div class="sect2">
<h3 id="_upstream_and_downstream_bottlenecks">Upstream and Downstream Bottlenecks</h3>
<div class="paragraph">
<p>As we monitor each service, it&#8217;s important to look at metrics in both directions to look for bottlenecks. Let&#8217;s learn how to do this by using Kubelet as an example. Kubelet talks both to the API server and the container runtime; <strong>how</strong> and <strong>what</strong> do we need to monitor to detect whether either component is experiencing an issue?</p>
</div>
<div class="sect3">
<h4 id="_how_many_pods_per_node">How many Pods per Node</h4>
<div class="paragraph">
<p>When we look at scaling numbers, such as how many pods can run on a node, we could take the 110 pods per node that upstream supports at face value.</p>
</div>
<div class="paragraph">
<p>!!! note
    <a href="https://kubernetes.io/docs/setup/best-practices/cluster-large/" class="bare">https://kubernetes.io/docs/setup/best-practices/cluster-large/</a></p>
</div>
<div class="paragraph">
<p>However, your workload is likely more complex than what was tested in a scalability test in Upstream. To ensure we can service the number of pods we want to run in production, let&#8217;s make sure that the Kubelet is &#8220;keeping up&#8221; with the Containerd runtime.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/keeping-up.png" alt="Keeping up">
</div>
</div>
<div class="paragraph">
<p>To oversimplify, the Kubelet is getting the status of the pods from the container runtime (in our case Containerd). What if we had too many pods changing status too quickly? If the rate of change is too high, requests [to the container runtime] can timeout.</p>
</div>
<div class="paragraph">
<p>!!! note
    Kubernetes is constantly evolving, this subsystem is currently undergoing changes. <a href="https://github.com/kubernetes/enhancements/issues/3386" class="bare">https://github.com/kubernetes/enhancements/issues/3386</a></p>
</div>
<div class="paragraph">
<p><span class="image"><img src="../images/flow.png" alt="Flow"></span>
<span class="image"><img src="../images/PLEG-duration.png" alt="PLEG duration"></span></p>
</div>
<div class="paragraph">
<p>In the graph above, we see a flat line indicating we have just hit the timeout value for the pod lifecycle event generation duration metric. If you would like to see this in your own cluster you could use the following PromQL syntax.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>increase(kubelet_pleg_relist_duration_seconds_bucket{instance="$instance"}[$__rate_interval])</pre>
</div>
</div>
<div class="paragraph">
<p>If we witness this timeout behavior, we know we pushed the node over the limit it was capable of. We need to fix the cause of the timeout before proceeding further. This could be achieved by reducing the number of pods per node, or looking for errors that might be causing a high volume of retries (thus effecting the churn rate). The important take-away is that metrics are the best way to understand if a node is able to handle the churn rate of the pods assigned vs. using a fixed number.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_scale_by_metrics">Scale by Metrics</h3>
<div class="paragraph">
<p>While the concept of using metrics to optimize systems is an old one, it&#8217;s often overlooked as people begin their Kubernetes journey. Instead of focusing on specific numbers (i.e. 110 pods per node), we focus our efforts on finding the metrics that help us find bottlenecks in our system. Understanding the right thresholds for these metrics can give us a high degree of confidence our system is optimally configured.</p>
</div>
<div class="sect3">
<h4 id="_the_impact_of_changes">The Impact of Changes</h4>
<div class="paragraph">
<p>A common pattern that could get us into trouble is focusing on the first metric or log error that looks suspect. When we saw that the Kubelet was timing out earlier, we could try random things, such as increasing the per second rate that the Kubelet is allowed to send, etc. However, it is wise to look at the whole picture of everything downstream of the error we find first. <em>Make each change with purpose and backed by data</em>.</p>
</div>
<div class="paragraph">
<p>Downstream of the Kubelet would be the Containerd runtime (pod errors), DaemonSets such as the storage driver (CSI) and the network driver (CNI) that talk to the EC2 API, etc.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/flow-addons.png" alt="Flow add-ons">
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s continue our earlier example of the Kubelet not keeping up with the runtime. There are a number of points where we could bin pack a node so densely that it triggers errors.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/bottlenecks.png" alt="Bottlenecks">
</div>
</div>
<div class="paragraph">
<p>When designing the right node size for our workloads these are easy-to-overlook signals that might be putting unnecessary pressure on the system thus limiting both our scale and performance.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_cost_of_unnecessary_errors">The Cost of Unnecessary Errors</h4>
<div class="paragraph">
<p>Kubernetes controllers excel at retrying when error conditions arise, however this comes at a cost. These retries can increase the pressure on components such as the Kube Controller Manager. It is an important tenant of scale testing to monitor for such errors.</p>
</div>
<div class="paragraph">
<p>When fewer errors are occurring, it is easier spot issues in the system. By periodically ensuring that our clusters are error free before major operations (such as upgrades) we can simplify troubleshooting logs when unforeseen events happen.</p>
</div>
<div class="sect4">
<h5 id="_expanding_our_view">Expanding Our View</h5>
<div class="paragraph">
<p>In large scale clusters with 1,000&#8217;s of nodes we don&#8217;t want to look for bottlenecks individually. In PromQL we can find the highest values in a data set using a function called topk; K being a variable we place the number of items we want. Here we use three nodes to get an idea whether all of the the Kubelets in the cluster are saturated. We have been looking at latency up to this point, now let&#8217;s see if the Kubelet is discarding events.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>topk(3, increase(kubelet_pleg_discard_events{}[$__rate_interval]))</pre>
</div>
</div>
<div class="paragraph">
<p>Breaking this statement down.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>We use the Grafana variable <code>$__rate_interval</code> to ensure it gets the four samples it needs. This bypasses a complex topic in monitoring with a simple variable.</p>
</li>
<li>
<p><code>topk</code> give us just the top results and the number 3 limits those results to three. This is a useful function for cluster wide metrics.</p>
</li>
<li>
<p><code>{}</code> tell us there are no filters, normally you would put the job name of whatever the scraping rule, however since these names vary we will leave it blank.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_splitting_the_problem_in_half">Splitting the Problem in Half</h5>
<div class="paragraph">
<p>To address a bottleneck in the system, we will take the approach of finding a metric that shows us there is a problem upstream or downstream as this allows us to split the problem in half. It will also be a core tenet of how we display our metrics data.</p>
</div>
<div class="paragraph">
<p>A good place to start with this process is the API server, as it allow us to see if there&#8217;s a problem with a client application or the Control Plane.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_control_plane_monitoring">Control Plane Monitoring</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_api_server">API Server</h3>
<div class="paragraph">
<p>When looking at our API server it&#8217;s important to remember that one of its functions is to throttle inbound requests to prevent overloading the control plane. What can seem like a bottleneck at the API server level might actually be protecting it from more serious issues. We need to factor in the pros and cons of increasing the volume of requests moving through the system. To make a determination if the API server values should be increased, here is small sampling of the things we need to be mindful of:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>What is the latency of requests moving through the system?</p>
</li>
<li>
<p>Is that latency the API server itself, or something &#8220;downstream&#8221; like etcd?</p>
</li>
<li>
<p>Is the API server queue depth a factor in this latency?</p>
</li>
<li>
<p>Are the API Priority and Fairness (APF) queues setup correctly for the API call patterns we want?</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_where_is_the_issue">Where is the issue?</h3>
<div class="paragraph">
<p>To start, we can use the metric for API latency to give us insight into how long it&#8217;s taking the API server to service requests. Let&#8217;s use the below PromQL and Grafana heatmap to display this data.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>max(increase(apiserver_request_duration_seconds_bucket{subresource!="status",subresource!="token",subresource!="scale",subresource!="/healthz",subresource!="binding",subresource!="proxy",verb!="WATCH"}[$__rate_interval])) by (le)</pre>
</div>
</div>
<div class="paragraph">
<p>!!! tip
    For an in depth write up on how to monitor the API server with the API dashboard used in this article, please see the following <a href="https://aws.amazon.com/blogs/containers/troubleshooting-amazon-eks-api-servers-with-prometheus/">blog</a></p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/api-request-duration.png" alt="API request duration heatmap">
</div>
</div>
<div class="paragraph">
<p>These requests are all under the one second mark, which is a good indication that the control plane is handling requests in a timely fashion.  But what if that was not the case?</p>
</div>
<div class="paragraph">
<p>The format we are using in the above API Request Duration is a heatmap. What&#8217;s nice about the heatmap format, is that it tells us the timeout value for the API by default (60 sec). However, what we really need to know is at what threshold should this value be of concern before we reach the timeout threshold. For a rough guideline of what acceptable thresholds are we can use the upstream Kubernetes SLO, which can be found <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md#steady-state-slisslos">here</a></p>
</div>
<div class="paragraph">
<p>!!! tip
    Notice the max function on this statement? When using metrics that are aggregating multiple servers (by default two API servers on EKS) it&#8217;s important not to average those servers together.</p>
</div>
<div class="sect3">
<h4 id="_asymmetrical_traffic_patterns">Asymmetrical traffic patterns</h4>
<div class="paragraph">
<p>What if one API server [pod] was lightly loaded, and the other heavily loaded? If we averaged those two numbers together we might misinterpret what was happening. For example, here we have three API servers but all of the load is on one of these API servers. As a rule anything that has multiple servers such as etcd and API servers should be broken out when investing scale and performance issues.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/inflight-requests.png" alt="Total inflight requests">
</div>
</div>
<div class="paragraph">
<p>With the move to API Priority and Fairness the total number of requests on the system is only one factor to check to see if the API server is oversubscribed. Since the system now works off a series of queues, we must look to see if any of these queues are full and if the traffic for that queue is getting dropped.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s look at these queues with the following query:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>max without(instance)(apiserver_flowcontrol_request_concurrency_limit{})</pre>
</div>
</div>
<div class="paragraph">
<p>!!! note
    For more information on how API A&amp;F works please see the following <a href="https://aws.github.io/aws-eks-best-practices/scalability/docs/control-plane/#api-priority-and-fairness">best practices guide</a></p>
</div>
<div class="paragraph">
<p>Here we see the seven different priority groups that come by default on the cluster</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/shared-concurrency.png" alt="Shared concurrency">
</div>
</div>
<div class="paragraph">
<p>Next we want to see what percentage of that priority group is being used, so that we can understand if a certain priority level is being saturated. Throttling requests in the workload-low level might be desirable, however drops in a leader election level would not be.</p>
</div>
<div class="paragraph">
<p>The API Priority and Fairness (APF) system has a number of complex options, some of those options can have unintended consequences. A common issue we see in the field is increasing the queue depth to the point it starts adding unnecessary latency. We can monitor this problem by using the <code>apiserver_flowcontrol_current_inqueue_request</code> metric. We can check for drops using the <code>apiserver_flowcontrol_rejected_requests_total</code>. These metrics will be a non-zero value if any bucket exceeds its concurrency.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/requests-in-use.png" alt="Requests in use">
</div>
</div>
<div class="paragraph">
<p>Increasing the queue depth can make the API Server a significant source of latency and should be done with care. We recommend being judicious with the number of queues created. For example, the number of shares on a EKS system is 600, if we create too many queues, this can reduce the shares in important queues that need the throughput such as the leader-election queue or system queue. Creating too many extra queues can make it more difficult to size theses queues correctly.</p>
</div>
<div class="paragraph">
<p>To focus on a simple impactful change you can make in APF we simply take shares from underutilized buckets and increase the size of buckets that are at their max usage. By intelligently redistributing the shares among these buckets, you can make drops less likely.</p>
</div>
<div class="paragraph">
<p>For more information, visit <a href="https://aws.github.io/aws-eks-best-practices/scalability/docs/control-plane/#api-priority-and-fairness">API Priority and Fairness settings</a> in the EKS Best Practices Guide.</p>
</div>
</div>
<div class="sect3">
<h4 id="_api_vs_etcd_latency">API vs. etcd latency</h4>
<div class="paragraph">
<p>How can we use the metrics/logs of the API server to determine whether there&#8217;s a problem with API server, or a problem that&#8217;s upstream/downstream of the API server, or a combination of both. To understand this better, lets look at how API Server and etcd can be related, and how easy it can be to troubleshoot the wrong system.</p>
</div>
<div class="paragraph">
<p>In the below chart we see API server latency, but we also see much of this latency is correlated to the etcd server due to the bars in the graph showing most of the latency at the etcd level. If there is 15 secs of etcd latency at the same time there is 20 seconds of API server latency, then the majority of the latency is actually at the etcd level.</p>
</div>
<div class="paragraph">
<p>By looking at the whole flow, we see that it&#8217;s wise to not focus solely on the API Server, but also look for signals that indicate that etcd is under duress (i.e. slow apply counters increasing). Being able to quickly move to the right problem area with just a glance is what makes a dashboard powerful.</p>
</div>
<div class="paragraph">
<p>!!! tip
    The dashboard in section can be found at <a href="https://github.com/RiskyAdventure/Troubleshooting-Dashboards/blob/main/api-troubleshooter.json" class="bare">https://github.com/RiskyAdventure/Troubleshooting-Dashboards/blob/main/api-troubleshooter.json</a></p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/etcd-duress.png" alt="ETCD duress">
</div>
</div>
</div>
<div class="sect3">
<h4 id="_control_plane_vs_client_side_issues">Control plane vs. Client side issues</h4>
<div class="paragraph">
<p>In this chart we are looking for the API calls that took the most time to complete for that period. In this case we see a custom resource (CRD) is calling a APPLY function that is the most latent call during the 05:40 time frame.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/slowest-requests.png" alt="Slowest requests">
</div>
</div>
<div class="paragraph">
<p>Armed with this data we can use an Ad-Hoc PromQL or a CloudWatch Insights query to pull LIST requests from the audit log during that time frame to see which application this might be.</p>
</div>
</div>
<div class="sect3">
<h4 id="_finding_the_source_with_cloudwatch">Finding the Source with CloudWatch</h4>
<div class="paragraph">
<p>Metrics are best used to find the problem area we want to look at and narrow both the timeframe and the search parameters of the problem. Once we have this data we want to transition to logs for more detailed times and errors. To do this we will turn our logs into metrics using <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html">CloudWatch Logs Insights</a>.</p>
</div>
<div class="paragraph">
<p>For example, to investigate the issue above, we will use the following CloudWatch Logs Insights query to pull the userAgent and requestURI so that we can pin down which application is causing this latency.</p>
</div>
<div class="paragraph">
<p>!!! tip
    An appropriate Count needs to be used as to not pull normal List/Resync behavior on a Watch.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>fields *@timestamp*, *@message*
| filter *@logStream* like "kube-apiserver-audit"
| filter ispresent(requestURI)
| filter verb = "list"
| parse requestReceivedTimestamp /\d+-\d+-(?&lt;StartDay&gt;\d+)T(?&lt;StartHour&gt;\d+):(?&lt;StartMinute&gt;\d+):(?&lt;StartSec&gt;\d+).(?&lt;StartMsec&gt;\d+)Z/
| parse stageTimestamp /\d+-\d+-(?&lt;EndDay&gt;\d+)T(?&lt;EndHour&gt;\d+):(?&lt;EndMinute&gt;\d+):(?&lt;EndSec&gt;\d+).(?&lt;EndMsec&gt;\d+)Z/
| fields (StartHour * 3600 + StartMinute * 60 + StartSec + StartMsec / 1000000) as StartTime, (EndHour * 3600 + EndMinute * 60 + EndSec + EndMsec / 1000000) as EndTime, (EndTime - StartTime) as DeltaTime
| stats avg(DeltaTime) as AverageDeltaTime, count(*) as CountTime by requestURI, userAgent
| filter CountTime &gt;=50
| sort AverageDeltaTime desc</pre>
</div>
</div>
<div class="paragraph">
<p>Using this query we found two different agents running a large number of high latency list operations. Splunk and CloudWatch agent. Armed with the data, we can make a decision to remove, update, or replace this controller with another project.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/query-results.png" alt="Query results">
</div>
</div>
<div class="paragraph">
<p>!!! tip
    For more details on this subject please see the following <a href="https://aws.amazon.com/blogs/containers/troubleshooting-amazon-eks-api-servers-with-prometheus/">blog</a></p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_scheduler">Scheduler</h3>
<div class="paragraph">
<p>Since the EKS control plane instances are run in separate AWS account we will not be able to scrape those components for metrics (The API server being the exception). However, since we have access to the audit logs for these components, we can turn those logs into metrics to see if any of the sub-systems are causing a scaling bottleneck. Let&#8217;s use CloudWatch Logs Insights to see how many unscheduled pods are in the scheduler queue.</p>
</div>
<div class="sect3">
<h4 id="_unscheduled_pods_in_the_scheduler_log">Unscheduled pods in the scheduler log</h4>
<div class="paragraph">
<p>If we had access to scrape the scheduler metrics directly on a self managed Kubernetes (such as Kops) we would use the following PromQL to understand the scheduler backlog.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>max without(instance)(scheduler_pending_pods)</pre>
</div>
</div>
<div class="paragraph">
<p>Since we do not have access to the above metric in EKS, we will use the below CloudWatch Logs Insights query to see the backlog by checking for how many pods were unable to unscheduled during a particular time frame. Then we could dive further into into the messages at the peak time frame to understand the nature of the bottleneck. For example, nodes not spinning up fast enough, or the rate limiter in the scheduler itself.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>fields timestamp, pod, err, *@message*
| filter *@logStream* like "scheduler"
| filter *@message* like "Unable to schedule pod"
| parse *@message*  /^.(?&lt;date&gt;\d{4})\s+(?&lt;timestamp&gt;\d+:\d+:\d+\.\d+)\s+\S*\s+\S+\]\s\"(.*?)\"\s+pod=(?&lt;pod&gt;\"(.*?)\")\s+err=(?&lt;err&gt;\"(.*?)\")/
| count(*) as count by pod, err
| sort count desc</pre>
</div>
</div>
<div class="paragraph">
<p>Here we see the errors from the scheduler saying the pod did not deploy because the storage PVC was unavailable.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/cwl-query.png" alt="CloudWatch Logs query">
</div>
</div>
<div class="paragraph">
<p>!!! note
    Audit logging must be turned on the control plane to enable this function. It is also a best practice to limit the log retention as to not drive up cost over time unnecessarily. An example for turning on all logging functions using the EKSCTL tool below.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">cloudWatch:
  clusterLogging:
    enableTypes: ["*"]
    logRetentionInDays: 10</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_kube_controller_manager">Kube Controller Manager</h3>
<div class="paragraph">
<p>Kube Controller Manager, like all other controllers, has limits on how many operations it can do at once. Let&#8217;s review what some of those flags are by looking at a KOPS configuration where we can set these parameters.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">  kubeControllerManager:
    concurrentEndpointSyncs: 5
    concurrentReplicasetSyncs: 5
    concurrentNamespaceSyncs: 10
    concurrentServiceaccountTokenSyncs: 5
    concurrentServiceSyncs: 5
    concurrentResourceQuotaSyncs: 5
    concurrentGcSyncs: 20
    kubeAPIBurst: 20
    kubeAPIQPS: "30"</code></pre>
</div>
</div>
<div class="paragraph">
<p>These controllers have queues that fill up during times of high churn on a cluster. In this case we see the replicaset set controller has a large backlog in its queue.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/queues.png" alt="Queues">
</div>
</div>
<div class="paragraph">
<p>We have two different ways of addressing such a situation. If running self managed we could simply increase the concurrent goroutines, however this would have an impact on etcd by processing more data in the KCM. The other option would be to reduce the number of replicaset objects using <code>.spec.revisionHistoryLimit</code> on the deployment to reduce the number of replicaset objects we can rollback, thus reducing the pressure on this controller.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">spec:
  revisionHistoryLimit: 2</code></pre>
</div>
</div>
<div class="paragraph">
<p>Other Kubernetes features can be tuned or turned off to reduce pressure in high churn rate systems. For example, if the application in our pods doesn&#8217;t need to speak to the k8s API directly then turning off the projected secret into those pods would decrease the load on ServiceaccountTokenSyncs. This is the more desirable way to address such issues if possible.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">kind: Pod
spec:
  automountServiceAccountToken: false</code></pre>
</div>
</div>
<div class="paragraph">
<p>In systems where we can&#8217;t get access to the metrics, we can again look at the logs to detect contention. If we wanted to see the number of requests being being processed on a per controller or an aggregate level we would use the following CloudWatch Logs Insights Query.</p>
</div>
<div class="sect3">
<h4 id="_total_volume_processed_by_the_kcm">Total Volume Processed by the KCM</h4>
<div class="listingblock">
<div class="content">
<pre># Query to count API qps coming from kube-controller-manager, split by controller type.
# If you're seeing values close to 20/sec for any particular controller, it's most likely seeing client-side API throttling.
fields @timestamp, @logStream, @message
| filter @logStream like /kube-apiserver-audit/
| filter userAgent like /kube-controller-manager/
# Exclude lease-related calls (not counted under kcm qps)
| filter requestURI not like "apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager"
# Exclude API discovery calls (not counted under kcm qps)
| filter requestURI not like "?timeout=32s"
# Exclude watch calls (not counted under kcm qps)
| filter verb != "watch"
# If you want to get counts of API calls coming from a specific controller, uncomment the appropriate line below:
# | filter user.username like "system:serviceaccount:kube-system:job-controller"
# | filter user.username like "system:serviceaccount:kube-system:cronjob-controller"
# | filter user.username like "system:serviceaccount:kube-system:deployment-controller"
# | filter user.username like "system:serviceaccount:kube-system:replicaset-controller"
# | filter user.username like "system:serviceaccount:kube-system:horizontal-pod-autoscaler"
# | filter user.username like "system:serviceaccount:kube-system:persistent-volume-binder"
# | filter user.username like "system:serviceaccount:kube-system:endpointslice-controller"
# | filter user.username like "system:serviceaccount:kube-system:endpoint-controller"
# | filter user.username like "system:serviceaccount:kube-system:generic-garbage-controller"
| stats count(*) as count by user.username
| sort count desc</pre>
</div>
</div>
<div class="paragraph">
<p>The key takeaway here is when looking into scalability issues, to look at every step in the path (API, scheduler, KCM, etcd) before moving to the detailed troubleshooting phase. Often in production you will find that it takes adjustments to more than one part of Kubernetes to allow the system to work at its most performant. It&#8217;s easy to inadvertently troubleshoot what is just a symptom (such as a node timeout) of a much larger bottle neck.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_etcd">ETCD</h3>
<div class="paragraph">
<p>etcd uses a memory mapped file to store key value pairs efficiently. There is a protection mechanism to set the size of this memory space available set commonly at the 2, 4, and 8GB limits. Fewer objects in the database means less clean up etcd needs to do when objects are updated and older versions needs to be cleaned out. This process of cleaning old versions of an object out is referred to as compaction. After a number of compaction operations, there is a subsequent process that recovers usable space space called defragging that happens above a certain threshold or on a fixed schedule of time.</p>
</div>
<div class="paragraph">
<p>There are a couple user related items we can do to limit the number of objects in Kubernetes and thus reduce the impact of both the compaction and de-fragmentation process. For example, Helm keeps a high <code>revisionHistoryLimit</code>. This keeps older objects such as ReplicaSets on the system to be able to do rollbacks. By setting the history limits down to 2 we can reduce the the number of objects (like ReplicaSets) from ten to two which in turn would put less load on the system.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: apps/v1
kind: Deployment
spec:
  revisionHistoryLimit: 2</code></pre>
</div>
</div>
<div class="paragraph">
<p>From a monitoring standpoint, if system latency spikes occur in a set pattern separated by hours, checking to see if this defragmentation process is the source can be helpful. We can see this by using CloudWatch Logs.</p>
</div>
<div class="paragraph">
<p>If you want to see start/end times of defrag use the following query:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>fields *@timestamp*, *@message*
| filter *@logStream* like /etcd-manager/
| filter *@message* like /defraging|defraged/
| sort *@timestamp* asc</pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/defrag.png" alt="Defrag query">
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_node_and_workload_efficiency">Node and Workload Efficiency</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Being efficient with our workloads and nodes reduces complexity/cost while increasing performance and scale. There are many factors to consider when planning this efficiency, and it&#8217;s easiest to think in terms of trade offs vs. one best practice setting for each feature. Let&#8217;s explore these tradeoffs in depth in the following section.</p>
</div>
<div class="sect2">
<h3 id="_node_selection">Node Selection</h3>
<div class="paragraph">
<p>Using node sizes that are slightly larger (4-12xlarge) increases the available space that we have for running pods due to the fact it reduces the percentage of the node used for &#8220;overhead&#8221; such as <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSets</a> and <a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">Reserves</a> for system components. In the diagram below we see the difference between the usable space on a 2xlarge vs. a 8xlarge system with just a moderate number of DaemonSets.</p>
</div>
<div class="paragraph">
<p>!!! note
    Since k8s scales horizontally as a general rule, for most applications it does not make sense to take the performance impact of NUMA sizes nodes, thus the recommendation of a range below that node size.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/node-size.png" alt="Node size">
</div>
</div>
<div class="paragraph">
<p>Large nodes sizes allow us to have a higher percentage of usable space per node. However, this model can be taken to to the extreme by packing the node with so many pods that it causes errors or saturates the node. Monitoring node saturation is key to successfully using larger node sizes.</p>
</div>
<div class="paragraph">
<p>Node selection is rarely a one-size-fits-all proposition. Often it is best to split workloads with dramatically different churn rates into different node groups. Small batch workloads with a high churn rate would be best served by the the 4xlarge family of instances, while a large scale application such as Kafka which takes 8 vCPU and has a low churn rate would be better served by the 12xlarge family.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/churn-rate.png" alt="Churn rate">
</div>
</div>
<div class="paragraph">
<p>!!! tip
    Another factor to consider with very large node sizes is since CGROUPS do not hide the total number of vCPU from the containerized application. Dynamic runtimes can often spawn an unintentional number of OS threads, creating latency that is difficult to troubleshoot. For these application <a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy">CPU pinning</a> is recommend. For a deeper exploration of topic please see the following video <a href="https://www.youtube.com/watch?v=NqtfDy_KAqg" class="bare">https://www.youtube.com/watch?v=NqtfDy_KAqg</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_node_bin_packing">Node Bin-packing</h3>
<div class="sect3">
<h4 id="_kubernetes_vs_linux_rules">Kubernetes vs. Linux Rules</h4>
<div class="paragraph">
<p>There are two sets of rules we need to be mindful of when dealing with workloads on Kubernetes. The rules of the Kubernetes Scheduler, which uses the request value to schedule pods on a node, and then what happens after the pod is scheduled, which is the realm of Linux, not Kubernetes.</p>
</div>
<div class="paragraph">
<p>After Kubernetes scheduler is finished, a new set of rules takes over, the Linux Completely Fair Scheduler (CFS). The key take away is that Linux CFS doesn&#8217;t have a the concept of a core. We will discuss why thinking in cores can lead to major problems with optimizing workloads for scale.</p>
</div>
</div>
<div class="sect3">
<h4 id="_thinking_in_cores">Thinking in Cores</h4>
<div class="paragraph">
<p>The confusion starts because the Kubernetes scheduler does have the concept of cores. From a Kubernetes scheduler perspective if we looked at a node with 4 NGINX pods, each with a request of one core set, the node would look like this.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/cores-1.png" alt="cores 1">
</div>
</div>
<div class="paragraph">
<p>However, let&#8217;s do a thought experiment on how different this looks from a Linux CFS perspective. The most important thing to remember when using the Linux CFS system is: busy containers (CGROUPS) are the only containers that count toward the share system. In this case, only the first container is busy so it is allowed to use all 4 cores on the node.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/cores-2.png" alt="cores 2">
</div>
</div>
<div class="paragraph">
<p>Why does this matter? Let&#8217;s say we ran our performance testing in a development cluster where an NGINX application was the only busy container on that node. When we move the app to production, the following would happen: the NGINX application wants 4 vCPU of resources however, because all the other pods on the node are busy, our app&#8217;s performance is constrained.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/cores-3.png" alt="cores 3">
</div>
</div>
<div class="paragraph">
<p>This situation would lead us to add more containers unnecessarily because we were not allowing our applications scale to their "`sweet spot"`. Let&#8217;s explore this important concept of a <code>"sweet spot"</code> in a bit more detail.</p>
</div>
</div>
<div class="sect3">
<h4 id="_application_right_sizing">Application right sizing</h4>
<div class="paragraph">
<p>Each application has a certain point where it can not take anymore traffic. Going above this point can increase processing times and even drop traffic when pushed well beyond this point. This is known as the application&#8217;s saturation point. To avoid scaling issues, we should attempt to scale the application <strong>before</strong> it reaches its saturation point. Let&#8217;s call this point the sweet spot.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/sweet-spot.png" alt="The sweet spot">
</div>
</div>
<div class="paragraph">
<p>We need to test each of our applications to understand its sweet spot. There will be no universal guidance here as each application is different. During this testing we are trying to understand the best metric that shows our applications saturation point. Oftentimes, utilization metrics are used to indicate an application is saturated but this can quickly lead to scaling issues (We will explore this topic in detail in a later section). Once we have this "`sweet spot"` we can use it to efficiently scale our workloads.</p>
</div>
<div class="paragraph">
<p>Conversely, what would happen if we scale up well before the sweet spot and created unnecessary pods? Let&#8217;s explore that in the next section.</p>
</div>
</div>
<div class="sect3">
<h4 id="_pod_sprawl">Pod sprawl</h4>
<div class="paragraph">
<p>To see how creating unnecessary pods could quickly get out of hand, let&#8217;s look at the first example on the left. The correct vertical scale of this container takes up about two vCPUs worth of utilization when handling 100 requests a second. However, If we were to under-provision the requests value by setting requests to half a core, we would now need 4 pods for each one pods we actually needed. Exacerbating this problem further, if our <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">HPA</a> was set at the default of 50% CPU, those pods would scale half empty, creating an 8:1 ratio.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/scaling-ratio.png" alt="scaling ratio">
</div>
</div>
<div class="paragraph">
<p>Scaling this problem up we can quickly see how this can get out of hand. A deployment of ten pods whose sweet spot was set incorrectly could quickly spiral to 80 pods and the additional infrastructure needed to run them.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/bad-sweetspot.png" alt="bad sweetspot">
</div>
</div>
<div class="paragraph">
<p>Now that we understand the impact of not allowing applications to operate in their sweet spot, let&#8217;s return to the node level and ask why this difference between the Kubernetes scheduler and Linux CFS so important?</p>
</div>
<div class="paragraph">
<p>When scaling up and down with HPA, we can have a scenario where we have a lot of space to allocate more pods. This would be a bad decision because the node depicted on the left is already at 100% CPU utilization. In a unrealistic but theoretically possible scenario, we could have the other extreme where our node is completely full, yet our CPU utilization is zero.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/hpa-utilization.png" alt="hpa utilization">
</div>
</div>
</div>
<div class="sect3">
<h4 id="_setting_requests">Setting Requests</h4>
<div class="paragraph">
<p>It would tempting to set the request at the &#8220;sweet spot&#8221; value for that application, however this would cause inefficiencies as pictured in the diagram below.  Here we have set the request value to 2 vCPU, however the average utilization of these pods runs only 1 CPU most of the time. This setting would cause us to waste 50% of our CPU cycles, which would be unacceptable.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/requests-1.png" alt="requests 1">
</div>
</div>
<div class="paragraph">
<p>This bring us to the complex answer to problem. Container utilization cannot be thought of in a vacuum; one must take into account the other applications running on the node. In the following example containers that are bursty in nature are mixed in with two low CPU utilization containers that might be memory constrained. In this way we allow the containers to hit their sweet spot without taxing the node.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/requests-2.png" alt="requests 2">
</div>
</div>
<div class="paragraph">
<p>The important concept to take away from all this is that using Kubernetes scheduler concept of cores to understand Linux container performance can lead to poor decision making as they are not related.</p>
</div>
<div class="paragraph">
<p>!!! tip
    Linux CFS has its strong points. This is especially true for I/O based workloads. However, if your application uses full cores without sidecars, and has no I/O requirements, CPU pinning can remove a great deal of complexity from this process and is encouraged with those caveats.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_utilization_vs_saturation">Utilization vs. Saturation</h3>
<div class="paragraph">
<p>A common mistake in application scaling is only using CPU utilization for your scaling metric. In complex applications this is almost always a poor indicator that an application is actually saturated with requests. In the example on the left, we see all of our requests are actually hitting the web server, so CPU utilization is tracking well with saturation.</p>
</div>
<div class="paragraph">
<p>In real world applications, it&#8217;s likely that some of those requests will be getting serviced by a database layer or an authentication layer, etc. In this more common case, notice CPU is not tracking with saturation as the request is being serviced by other entities. In this case CPU is a very poor indicator for saturation.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/util-vs-saturation-1.png" alt="util vs saturation 1">
</div>
</div>
<div class="paragraph">
<p>Using the wrong metric in application performance is the number one reason for unnecessary and unpredictable scaling in Kubernetes. Great care must be taken in picking the correct saturation metric for the type of application that you&#8217;re using. It is important to note that there is not a one size fits all recommendation that can be given. Depending on the language used and the type of application in question, there is a diverse set of metrics for saturation.</p>
</div>
<div class="paragraph">
<p>We might think this problem is only with CPU Utilization, however other common metrics such as request per second can also fall into the exact same problem as discussed above.  Notice the request can also go to DB layers, auth layers, not being directly serviced by our web server, thus it&#8217;s a poor metric for true saturation of the web server itself.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/util-vs-saturation-2.png" alt="util vs saturation 2">
</div>
</div>
<div class="paragraph">
<p>Unfortunately there are no easy answers when it comes to picking the right saturation metric. Here are some guidelines to take into consideration:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Understand your language runtime - languages with multiple OS threads will react differently than single threaded applications, thus impacting the node differently.</p>
</li>
<li>
<p>Understand the correct vertical scale - how much buffer do you want in your applications vertical scale before scaling a new pod?</p>
</li>
<li>
<p>What metrics truly reflect the saturation of your application - The saturation metric for a Kafka Producer would be quite different than a complex web application.</p>
</li>
<li>
<p>How do all the other applications on the node effect each other - Application performance is not done in a vacuum the other workloads on the node have a major impact.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To close out this section, it would be easy to dismiss the above as overly complex and unnecessary. It can often be the case that we are experiencing an issue but we are unaware of the true nature of the problem because we are looking at the wrong metrics. In the next section we will look at how that could happen.</p>
</div>
<div class="sect3">
<h4 id="_node_saturation">Node Saturation</h4>
<div class="paragraph">
<p>Now that we have explored application saturation, let&#8217;s look at this same concept from a node point of view. Let&#8217;s take two CPUs that are 100% utilized to see the difference between utilization vs. saturation.</p>
</div>
<div class="paragraph">
<p>The vCPU on the left is 100% utilized, however no other tasks are waiting to run on this vCPU, so in a purely theoretical sense, this is quite efficient. Meanwhile, we have 20 single threaded applications waiting to get processed by a vCPU in the second example. All 20 applications now will experience some type of latency while they&#8217;re waiting their turn to be processed by the vCPU. In other words, the vCPU on the right is saturated.</p>
</div>
<div class="paragraph">
<p>Not only would we not see this problem if we where just looking at utilization, but we might attribute this latency to something unrelated such as networking which would lead us down the wrong path.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/node-saturation.png" alt="node saturation">
</div>
</div>
<div class="paragraph">
<p>It is important to view saturation metrics, not just utilization metrics when increasing the total number of pods running on a node at any given time as we can easily miss the fact we have over-saturated a node. For this task we can use pressure stall information metrics as seen in the below chart.</p>
</div>
<div class="paragraph">
<p>PromQL - Stalled I/O</p>
</div>
<div class="listingblock">
<div class="content">
<pre>topk(3, ((irate(node_pressure_io_stalled_seconds_total[1m])) * 100))</pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/stalled-io.png" alt="stalled io">
</div>
</div>
<div class="paragraph">
<p>!!! note
    For more on Pressure stall metrics, see <a href="https://facebookmicrosites.github.io/psi/docs/overview*" class="bare">https://facebookmicrosites.github.io/psi/docs/overview*</a></p>
</div>
<div class="paragraph">
<p>With these metrics we can tell if threads are waiting on CPU, or even if every thread on the box is stalled waiting on resource like memory or I/O. For example, we could see what percentage every thread on the instance was stalled waiting on I/O over the period of 1 min.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>topk(3, ((irate(node_pressure_io_stalled_seconds_total[1m])) * 100))</pre>
</div>
</div>
<div class="paragraph">
<p>Using this metric, we can see in the above chart every thread on the box was stalled 45% of the time waiting on I/O at the high water mark, meaning we were throwing away all of those CPU cycles in that minute. Understanding that this is happening can help us reclaim a significant amount of vCPU time, thus making scaling more efficient.</p>
</div>
</div>
<div class="sect3">
<h4 id="_hpa_v2">HPA V2</h4>
<div class="paragraph">
<p>It is recommended to use the autoscaling/v2 version of the HPA API. The older versions of the HPA API could get stuck scaling in certain edge cases. It was also limited to pods only doubling during each scaling step, which created issues for small deployments that needed to scale rapidly.</p>
</div>
<div class="paragraph">
<p>Autoscaling/v2 allows us more flexibility to include multiple criteria to scale on and allows us a great deal of flexibility when using custom and external metrics (non K8s metrics).</p>
</div>
<div class="paragraph">
<p>As an example, we can scaling on the highest of three values (see below). We scale if the average utilization of all the pods are over 50%, if custom metrics the packets per second of the ingress exceed an average of 1,000, or ingress object exceeds 10K request per second.</p>
</div>
<div class="paragraph">
<p>!!! note
    This is just to show the flexibility of the auto-scaling API, we recommend against overly complex rules that can be difficult to troubleshoot in production.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Pods
    pods:
      metric:
        name: packets-per-second
      target:
        type: AverageValue
        averageValue: 1k
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        name: main-route
      target:
        type: Value
        value: 10k</code></pre>
</div>
</div>
<div class="paragraph">
<p>However, we learned the danger of using such metrics for complex web applications. In this case we would be better served by using custom or external metric that accurately reflects the saturation of our application vs. the utilization. HPAv2 allows for this by having the ability to scale according to any metric, however we still need to find and export that metric to Kubernetes for use.</p>
</div>
<div class="paragraph">
<p>For example, we can look at the active thread queue count in Apache. This often creates a &#8220;smoother&#8221; scaling profile (more on that term soon). If a thread is active, it doesn&#8217;t matter if that thread is waiting on a database layer or servicing a request locally, if all of the applications threads are being used, it&#8217;s a great indication that application is saturated.</p>
</div>
<div class="paragraph">
<p>We can use this thread exhaustion as a signal to create a new pod with a fully available thread pool. This also gives us control over how big a buffer we want in the application to absorb during times of heavy traffic. For example, if we had a total thread pool of 10, scaling at 4 threads used vs. 8 threads used would have a major impact on the buffer we have available when scaling the application. A setting of 4 would make sense for an application that needs to rapidly scale under heavy load, where a setting of 8 would be more efficient with our resources if we had plenty of time to scale due to the number of requests increasing slowly vs. sharply over time.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/thread-pool.png" alt="thread pool">
</div>
</div>
<div class="paragraph">
<p>What do we mean by the term &#8220;smooth&#8221; when it comes to scaling? Notice the below chart where we are using CPU as a metric. The pods in this deployment are spiking in a short period for from 50 pods, all the way up to 250 pods only to immediately scale down again. This is highly inefficient scaling is the leading cause on churn on clusters.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/spiky-scaling.png" alt="spiky scaling">
</div>
</div>
<div class="paragraph">
<p>Notice how after we change to a metric that reflects the correct sweet spot of our application (mid-part of chart), we are able to scale smoothly. Our scaling is now efficient, and our pods are allowed to fully scale with the headroom we provided by adjusting requests settings. Now a smaller group of pods are doing the work the hundreds of pods were doing before.  Real world data shows that this is the number one factor in scalability of Kubernetes clusters.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/smooth-scaling.png" alt="smooth scaling">
</div>
</div>
<div class="paragraph">
<p>The key takeaway is CPU utilization is only one dimension of both application and node performance. Using CPU utilization as a sole health indicator for our nodes and applications creates problems in scaling, performance and cost which are all tightly linked concepts. The more performant the application and nodes are, the less that you need to scale, which in turn lowers your costs.</p>
</div>
<div class="paragraph">
<p>Finding and using the correct saturation metrics for scaling your particular application also allows you to monitor and alarm on the true bottlenecks for that application. If this critical step is skipped, reports of performance problems will be difficult, if not impossible, to understand.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_setting_cpu_limits">Setting CPU Limits</h3>
<div class="paragraph">
<p>To round out this section on misunderstood topics, we will cover CPU limits. In short, limits are metadata associated with the container that has a counter that resets every 100ms. This helps Linux keep track of how many CPU resources are used node-wide by a specific container in a 100ms period of time.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/cpu-limits.png" alt="CPU limits">
</div>
</div>
<div class="paragraph">
<p>A common error with setting limits is assuming that the application is single threaded and only running on it&#8217;s "`assigned"` vCPU. In the above section we learned that CFS doesn&#8217;t assign cores, and in reality a container running large thread pools will schedule on all available vCPU&#8217;s on the box.</p>
</div>
<div class="paragraph">
<p>If 64 OS threads are running across 64 available cores (from a Linux node perspective) we will make the total bill of used CPU time in a 100ms period quite large after the time running on all of those 64 cores are added up. Since this might only occur during a garbage collection process it can be quite easy to miss something like this. This is why it is necessary to use metrics to ensure we have the correct usage over time before attempting to set a limit.</p>
</div>
<div class="paragraph">
<p>Fortunately, we have a way to see exactly how much vCPU is being used by all the threads in a application. We will use the metric <code>container_cpu_usage_seconds_total</code> for this purpose.</p>
</div>
<div class="paragraph">
<p>Since throttling logic happens every 100ms and this metric is a per second metric, we will PromQL to match this 100ms period. If you would like to dive deep into this PromQL statement work please see the following <a href="https://aws.amazon.com/blogs/containers/using-prometheus-to-avoid-disasters-with-kubernetes-cpu-limits/">blog</a>.</p>
</div>
<div class="paragraph">
<p>PromQL query:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>topk(3, max by (pod, container)(rate(container_cpu_usage_seconds_total{image!="", instance="$instance"}[$__rate_interval]))) / 10</pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/cpu-1.png" alt="cpu 1">
</div>
</div>
<div class="paragraph">
<p>Once we feel we have the right value, we can put the limit in production. It then becomes necessary to see if our application is being throttled due to something unexpected. We can do this by looking at  <code>container_cpu_throttled_seconds_total</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre>topk(3, max by (pod, container)(rate(container_cpu_cfs_throttled_seconds_total{image!=``""``, instance=``"$instance"``}[$__rate_interval]))) / 10</pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="../images/cpu-2.png" alt="cpu 2">
</div>
</div>
<div class="sect3">
<h4 id="_memory">Memory</h4>
<div class="paragraph">
<p>The memory allocation is another example where it is easy to confuse Kubernetes scheduling behavior for Linux CGroup behavior. This is a more nuanced topic as there have been major changes in the way that CGroup v2 handles memory in Linux and Kubernetes has changed its syntax to reflect this; read this <a href="https://kubernetes.io/blog/2021/11/26/qos-memory-resources/">blog</a> for further details.</p>
</div>
<div class="paragraph">
<p>Unlike CPU requests, memory requests go unused after the scheduling process completes. This is because we can not compress memory in CGroup v1 the same way we can with CPU. That leaves us with just memory limits, which are designed to act as a fail safe for memory leaks by terminating the pod completely. This is an all or nothing style proposition, however we have now been given new ways to address this problem.</p>
</div>
<div class="paragraph">
<p>First, it is important to understand that setting the right amount of memory for containers is not a straightforward as it appears. The file system in Linux will use memory as a cache to improve performance. This cache will grow over time, and it can be hard to know how much memory is just nice to have for the cache but can be reclaimed without a significant impact to application performance. This often results in misinterpreting memory usage.</p>
</div>
<div class="paragraph">
<p>Having the ability to &#8220;compress&#8221; memory was one of the primary drivers behind CGroup v2. For more history on why CGroup V2 was necessary, please see Chris Down&#8217;s <a href="https://www.youtube.com/watch?v=kPMZYoRxtmg">presentation</a> at LISA21 where he covers why being unable to set the minimum memory correctly was one of the reasons that drove him to create CGroup v2  and pressure stall metrics.</p>
</div>
<div class="paragraph">
<p>Fortunately, Kubernetes now has the concept of <code>memory.min</code> and <code>memory.high</code> under <code>requests.memory</code>. This gives us the option of aggressive releasing this cached memory for other containers to use. Once the container hits the memory high limit, the kernel can aggressively reclaim that container&#8217;s memory up to the value set at <code>memory.min</code>. Thus giving us more flexibility when a node comes under memory pressure.</p>
</div>
<div class="paragraph">
<p>The key question becomes, what value to set <code>memory.min</code> to? This is where memory pressure stall metrics come into play. We can use these metrics to detect memory &#8220;thrashing&#8221; at a container level. Then we can use controllers such as <a href="https://facebookmicrosites.github.io/cgroup2/docs/fbtax-results.html">fbtax</a> to detect the correct values for <code>memory.min</code> by looking for this memory thrashing, and dynamically set the <code>memory.min</code> value to this setting.</p>
</div>
</div>
<div class="sect3">
<h4 id="_summary">Summary</h4>
<div class="paragraph">
<p>To sum up the section, it is easy to conflate the following concepts:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Utilization and Saturation</p>
</li>
<li>
<p>Linux performance rules with Kubernetes Scheduler logic</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Great care must be taken to keep these concepts separated. Performance and scale are linked on a deep level. Unnecessary scaling creates performance problems, which in turn creates scaling problems.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_kubernetes_upstream_slos">Kubernetes Upstream SLOs</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Amazon EKS runs the same code as the upstream Kubernetes releases and ensures that EKS clusters operate within the SLOs defined by the Kubernetes community. The Kuberneteshttps://github.com/kubernetes/community/tree/master/sig-scalability[Scalability Special Interest Group (SIG)] defines the scalability goals and investigates bottlenecks in performance through SLIs and SLOs.</p>
</div>
<div class="paragraph">
<p>SLIs are how we measure a system like metrics or measures that can be used to determine how &#8220;well&#8221; the system is running, e.g. request latency or count. SLOs define the values that are expected for when the system is running &#8220;well&#8221;, e.g. request latency remains less than 3 seconds. The Kubernetes SLOs and SLIs focus on the performance of the Kubernetes components and are completely independent from the Amazon EKS Service SLAs which focus on availability of the EKS cluster endpoint.</p>
</div>
<div class="paragraph">
<p>Kubernetes has a number of features that allow users to extend the system with custom add-ons or drivers, like CSI drivers, admission webhooks, and auto-scalers. These extensions can drastically impact the performance of a Kubernetes cluster in different ways, i.e. an admission webhook with <code>failurePolicy=Ignore</code> could add latency to K8s API requests if the webhook target is unavailable. The Kubernetes Scalability SIG defines scalability using a <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md#how-we-define-scalability">"you promise, we promise" framework</a>:</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>If you promise to:<br>
    - correctly configure your cluster<br>
    - use extensibility features "reasonably"<br>
    - keep the load in the cluster within <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md">recommended limits</a></p>
</div>
<div class="paragraph">
<p>then we promise that your cluster scales, i.e.:<br>
    - all the SLOs are satisfied.</p>
</div>
</blockquote>
</div>
<div class="sect2">
<h3 id="_kubernetes_slos">Kubernetes SLOs</h3>
<div class="paragraph">
<p>The Kubernetes SLOs don&#8217;t account for all of the plugins and external limitations that could impact a cluster, such as worker node scaling or admission webhooks. These SLOs focus on <a href="https://kubernetes.io/docs/concepts/overview/components/">Kubernetes components</a> and ensure that Kubernetes actions and resources are operating within expectations. The SLOs help Kubernetes developers ensure that changes to Kubernetes code do not degrade performance for the entire system.</p>
</div>
<div class="paragraph">
<p>The <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md">Kuberntes Scalability SIG defines the following official SLO/SLIs</a>. The Amazon EKS team regularly runs scalability tests on EKS clusters for these SLOs/SLIs to monitor for performance degradation as changes are made and new versions are released.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Objective</th>
<th class="tableblock halign-left valign-top">Definition</th>
<th class="tableblock halign-left valign-top">SLO</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">API request latency (mutating)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Latency of processing mutating  API calls for single objects for every (resource, verb) pair, measured as 99th percentile over last 5 minutes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">In default Kubernetes installation, for every (resource, verb) pair, excluding virtual and aggregated resources and Custom Resource Definitions, 99th percentile per cluster-day &lt;= 1s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">API request latency (read-only)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Latency of processing non-streaming read-only API calls for every (resource, scope) pair, measured as 99th percentile over last 5 minutes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">In default Kubernetes installation, for every (resource, scope) pair, excluding virtual and aggregated resources and Custom Resource Definitions, 99th percentile per cluster-day: (a) &lt;= 1s if <code>scope=resource</code> (b) &lt;= 30s otherwise (if <code>scope=namespace</code> or <code>scope=cluster</code>)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pod startup latency</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Startup latency of schedulable stateless pods, excluding time to pull images and run init containers, measured from pod creation timestamp to when all its containers are reported as started and observed via watch, measured as 99th percentile over last 5 minutes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">In default Kubernetes installation, 99th percentile per cluster-day &lt;= 5s</p></td>
</tr>
</tbody>
</table>
<div class="sect3">
<h4 id="_api_request_latency">API Request Latency</h4>
<div class="paragraph">
<p>The <code>kube-apiserver</code> has <code>--request-timeout</code> defined as <code>1m0s</code> by default, which means a request can run for up to one minute (60 seconds) before being timed out and cancelled. The SLOs defined for Latency are broken out by the type of request that is being made, which can be mutating or read-only:</p>
</div>
<div class="sect4">
<h5 id="_mutating">Mutating</h5>
<div class="paragraph">
<p>Mutating requests in Kubernetes make changes to a resource, such as creations, deletions, or updates. These requests are expensive because those changes must be written to <a href="https://kubernetes.io/docs/concepts/overview/components/#etcd">the etcd backend</a> before the updated object is returned. <a href="https://etcd.io/">Etcd</a> is a distributed key-value store that is used for all Kubernetes cluster data.</p>
</div>
<div class="paragraph">
<p>This latency is measured as the 99th percentile over 5min for (resource, verb) pairs of Kubernetes resources, for example this would measure the latency for Create Pod requests and Update Node requests. The request latency must be &lt;= 1 second to satisfy the SLO.</p>
</div>
</div>
<div class="sect4">
<h5 id="_read_only">Read-only</h5>
<div class="paragraph">
<p>Read-only requests retrieve a single resource (such as Get Pod X) or a collection (such as &#8220;Get all Pods from Namespace X&#8221;). The <code>kube-apiserver</code> maintains a cache of objects, so the requested resources may be returned from cache or they may need to be retrieved from etcd first.
These latencies are also measured by the 99th percentile over 5 minutes, however read-only requests can have separate scopes. The SLO defines two different objectives:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>For requests made for a <em>single</em> resource (i.e. <code>kubectl get pod -n mynamespace my-controller-xxx</code> ), the request latency should remain &lt;= 1 second.</p>
</li>
<li>
<p>For requests that are made for multiple resources in a namespace or a cluster (for example, <code>kubectl get pods -A</code>) the latency should remain &lt;= 30 seconds</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The SLO has different target values for different request scopes because requests made for a list of Kubernetes resources expect the details of all objects in the request to be returned within the SLO. On large clusters, or large collections of resources, this can result in large response sizes which can take some time to return. For example, in a cluster running tens of thousands of Pods with each Pod being roughly 1 KiB when encoded in JSON, returning all Pods in the cluster would consist of 10MB or more. Kubernetes clients can help reduce this response size <a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#retrieving-large-results-sets-in-chunks">using APIListChunking to retrieve large collections of resources</a>.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_pod_startup_latency">Pod Startup Latency</h4>
<div class="paragraph">
<p>This SLO is primarily concerned with the time it takes from Pod creation to when the containers in that Pod actually begin execution. To measure this the difference from the creation timestamp recorded on the Pod, and when <a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes">a WATCH on that Pod</a> reports the containers have started is calculated (excluding time for container image pulls and init container execution). To satisfy the SLO the 99th percentile per cluster-day of this Pod Startup Latency must remain &lt;=5 seconds.</p>
</div>
<div class="paragraph">
<p>Note that this SLO assumes that the worker nodes already exist in this cluster in a ready state for the Pod to be scheduled on. This SLO does not account for image pulls or init container executions, and also limits the test to &#8220;stateless pods&#8221; which don&#8217;t leverage persistent storage plugins.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_kubernetes_sli_metrics">Kubernetes SLI Metrics</h3>
<div class="paragraph">
<p>Kubernetes is also improving the Observability around the SLIs by adding <a href="https://prometheus.io/docs/concepts/data_model/">Prometheus metrics</a> to Kubernetes components that track these SLIs over time. Using <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/">Prometheus Query Language (PromQL)</a> we can build queries that display the SLI performance over time in tools like Prometheus or Grafana dashboards, below are some examples for the SLOs above.</p>
</div>
<div class="sect3">
<h4 id="_api_server_request_latency">API Server Request Latency</h4>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Metric</th>
<th class="tableblock halign-left valign-top">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">apiserver_request_sli_duration_seconds</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Response latency distribution (not counting webhook duration and priority &amp; fairness queue wait times) in seconds for each verb, group, version, resource, subresource, scope and component.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">apiserver_request_duration_seconds</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Response latency distribution in seconds for each verb, dry run value, group, version, resource, subresource, scope and component.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><em>Note: The <code>apiserver_request_sli_duration_seconds</code> metric is available starting in Kubernetes 1.27.</em></p>
</div>
<div class="paragraph">
<p>You can use these metrics to investigate the API Server response times and if there are bottlenecks in the Kubernetes components or other plugins/components. The queries below are based on <a href="https://github.com/kubernetes/perf-tests/tree/master/clusterloader2/pkg/prometheus/manifests/dashboards">the community SLO dashboard</a>.</p>
</div>
<div class="paragraph">
<p><strong>API Request latency SLI (mutating)</strong> - this time does <em>not</em> include webhook execution or time waiting in queue.<br>
<code>histogram_quantile(0.99, sum(rate(apiserver_request_sli_duration_seconds_bucket{verb=~"CREATE|DELETE|PATCH|POST|PUT", subresource!~"proxy|attach|log|exec|portforward"}[5m])) by (resource, subresource, verb, scope, le)) &gt; 0</code></p>
</div>
<div class="paragraph">
<p><strong>API Request latency Total (mutating)</strong> - this is the total time the request took on the API server, this time may be longer than the SLI time because it includes webhook execution and API Priority and Fairness wait times.<br>
<code>histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb=~"CREATE|DELETE|PATCH|POST|PUT", subresource!~"proxy|attach|log|exec|portforward"}[5m])) by (resource, subresource, verb, scope, le)) &gt; 0</code></p>
</div>
<div class="paragraph">
<p>In these queries we are excluding the streaming API requests which do not return immediately, such as <code>kubectl port-forward</code> or <code>kubectl exec</code> requests (<code>subresource!~"proxy|attach|log|exec|portforward"</code>), and we are filtering for only the Kubernetes verbs that modify objects (<code>verb=~"CREATE|DELETE|PATCH|POST|PUT"</code>). We are then calculating the 99th percentile of that latency over the last 5 minutes.</p>
</div>
<div class="paragraph">
<p>We can use a similar query for the read only API requests, we simply modify the verbs we&#8217;re filtering for to include the Read only actions <code>LIST</code> and <code>GET</code>. There are also different SLO thresholds depending on the scope of the request, i.e. getting a single resource or listing a number of resources.</p>
</div>
<div class="paragraph">
<p><strong>API Request latency SLI  (read-only)</strong> - this time does <em>not</em> include webhook execution or time waiting in queue.
For a single resource (scope=resource, threshold=1s)<br>
<code>histogram_quantile(0.99, sum(rate(apiserver_request_sli_duration_seconds_bucket{verb=~"GET", scope=~"resource"}[5m])) by (resource, subresource, verb, scope, le))</code></p>
</div>
<div class="paragraph">
<p>For a collection of resources in the same namespace (scope=namespace, threshold=5s)<br>
<code>histogram_quantile(0.99, sum(rate(apiserver_request_sli_duration_seconds_bucket{verb=~"LIST", scope=~"namespace"}[5m])) by (resource, subresource, verb, scope, le))</code></p>
</div>
<div class="paragraph">
<p>For a collection of resources across the entire cluster (scope=cluster, threshold=30s)<br>
<code>histogram_quantile(0.99, sum(rate(apiserver_request_sli_duration_seconds_bucket{verb=~"LIST", scope=~"cluster"}[5m])) by (resource, subresource, verb, scope, le))</code></p>
</div>
<div class="paragraph">
<p><strong>API Request latency Total (read-only)</strong> - this is the total time the request took on the API server, this time may be longer than the SLI time because it includes webhook execution and wait times.
For a single resource (scope=resource, threshold=1s)<br>
<code>histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb=~"GET", scope=~"resource"}[5m])) by (resource, subresource, verb, scope, le))</code></p>
</div>
<div class="paragraph">
<p>For a collection of resources in the same namespace (scope=namespace, threshold=5s)<br>
<code>histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb=~"LIST", scope=~"namespace"}[5m])) by (resource, subresource, verb, scope, le))</code></p>
</div>
<div class="paragraph">
<p>For a collection of resources across the entire cluster (scope=cluster, threshold=30s)<br>
<code>histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb=~"LIST", scope=~"cluster"}[5m])) by (resource, subresource, verb, scope, le))</code></p>
</div>
<div class="paragraph">
<p>The SLI metrics provide insight into how Kubernetes components are performing by excluding the time that requests spend waiting in API Priority and Fairness queues, working through admission webhooks, or other Kubernetes extensions. The total metrics provide a more holistic view as it reflects the time your applications would be waiting for a response from the API server. Comparing these metrics can provide insight into where the delays in request processing are being introduced.</p>
</div>
</div>
<div class="sect3">
<h4 id="_pod_startup_latency_2">Pod Startup Latency</h4>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Metric</th>
<th class="tableblock halign-left valign-top">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">kubelet_pod_start_sli_duration_seconds</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Duration in seconds to start a pod, excluding time to pull images and run init containers, measured from pod creation timestamp to when all its containers are reported as started and observed via watch</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">kubelet_pod_start_duration_seconds</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Duration in seconds from kubelet seeing a pod for the first time to the pod starting to run. This does not include the time to schedule the pod or scale out worker node capacity.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><em>Note:  <code>kubelet_pod_start_sli_duration_seconds</code> is available starting in Kubernetes 1.27.</em></p>
</div>
<div class="paragraph">
<p>Similar to the queries above you can use these metrics to gain insight into how long node scaling, image pulls and init containers are delaying the pod launch compared to Kubelet actions.</p>
</div>
<div class="paragraph">
<p><strong>Pod startup latency SLI -</strong> this is the time from the pod being created to when the application containers reported as running. This includes the time it takes for the worker node capacity to be available and the pod to be scheduled, but this does not include the time it takes to pull images or for the init containers to run.<br>
<code>histogram_quantile(0.99, sum(rate(kubelet_pod_start_sli_duration_seconds_bucket[5m])) by (le))</code></p>
</div>
<div class="paragraph">
<p><strong>Pod startup latency Total -</strong> this is the time it takes the kubelet to start the pod for the first time. This is measured from when the kubelet recieves the pod via WATCH, which does not include the time for worker node scaling or scheduling. This includes the time to pull images and init containers to run.<br>
<code>histogram_quantile(0.99, sum(rate(kubelet_pod_start_duration_seconds_bucket[5m])) by (le))</code></p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_slos_on_your_cluster">SLOs on Your Cluster</h3>
<div class="paragraph">
<p>If you are collecting the Prometheus metrics from the Kubernetes resources in your EKS cluster you can gain deeper insights into the performance of the Kubernetes control plane components.</p>
</div>
<div class="paragraph">
<p>The <a href="https://github.com/kubernetes/perf-tests/">perf-tests repo</a> includes Grafana dashboards that display the latencies and critical performance metrics for the cluster during tests. The perf-tests configuration leverages the <a href="https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack">kube-prometheus-stack</a>, an open source project that comes configured to collect Kubernetes metrics, but you can also <a href="https://aws-observability.github.io/terraform-aws-observability-accelerator/eks/">use Amazon Managed Prometheus and Amazon Managed Grafana.</a></p>
</div>
<div class="paragraph">
<p>If you are using the <code>kube-prometheus-stack</code> or similar Prometheus solution you can install the same dashboard to observe the SLOs on your cluster in real time.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>You will first need to install the Prometheus Rules that are used in the dashboards with <code>kubectl apply -f prometheus-rules.yaml</code>. You can download a copy of the rules here: <a href="https://github.com/kubernetes/perf-tests/blob/master/clusterloader2/pkg/prometheus/manifests/prometheus-rules.yaml" class="bare">https://github.com/kubernetes/perf-tests/blob/master/clusterloader2/pkg/prometheus/manifests/prometheus-rules.yaml</a></p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Be sure to check the namespace in the file matches your environment</p>
</li>
<li>
<p>Verify that the labels match the <code>prometheus.prometheusSpec.ruleSelector</code> helm value if you are using <code>kube-prometheus-stack</code></p>
</li>
</ol>
</div>
</li>
<li>
<p>You can then install the dashboards in Grafana. The json dashboards and python scripts to generate them are available here: <a href="https://github.com/kubernetes/perf-tests/tree/master/clusterloader2/pkg/prometheus/manifests/dashboards" class="bare">https://github.com/kubernetes/perf-tests/tree/master/clusterloader2/pkg/prometheus/manifests/dashboards</a></p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p><a href="https://github.com/kubernetes/perf-tests/blob/master/clusterloader2/pkg/prometheus/manifests/dashboards/slo.json">the <code>slo.json</code> dashboard</a> displays the performance of the cluster in relation to the Kubernetes SLOs</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Consider that the SLOs are focused on the performance of the Kubernetes components in your clusters, but there are additional metrics you can review which provide different perspectives or insights in to your cluster. Kubernetes community projects like <a href="https://github.com/kubernetes/kube-state-metrics/tree/main">Kube-state-metrics</a> can help you quickly analyze trends in your cluster. Most common plugins and drivers from the Kubernetes community also emit Prometheus metrics, allowing you to investigate things like autoscalers or custom schedulers.</p>
</div>
<div class="paragraph">
<p>The <a href="https://aws-observability.github.io/observability-best-practices/guides/containers/oss/eks/best-practices-metrics-collection/#control-plane-metrics">Observability Best Practices Guide</a> has examples of other Kubernetes metrics you can use to gain further insight.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_known_limits_and_service_quotas">Known Limits and Service Quotas</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Amazon EKS can be used for a variety of workloads and can interact with a wide range of AWS services, and we have seen customer workloads encounter a similar range of AWS service quotas and other issues that hamper scalability.</p>
</div>
<div class="paragraph">
<p>Your AWS account has default quotas (an upper limit on the number of each AWS resource your team can request). Each AWS service defines their own quota, and quotas are generally region-specific. You can request increases for some quotas (soft limits), and other quotas cannot be increased (hard limits). You should consider these values when architecting your applications. Consider reviewing these service limits periodically and incorporate them during in your application design.</p>
</div>
<div class="paragraph">
<p>You can review the usage in your account and open a quota increase request at the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html#request-increase">AWS Service Quotas console</a>, or using <a href="https://repost.aws/knowledge-center/request-service-quota-increase-cli">the AWS CLI</a>. Refer to the AWS documentation from the respective AWS Service for more details on the Service Quotas and any further restrictions or notices on their increase.</p>
</div>
<div class="paragraph">
<p>!!! note
    <a href="https://docs.aws.amazon.com/eks/latest/userguide/service-quotas.html">Amazon EKS Service Quotas</a> lists the service quotas and has links to request increases where available.</p>
</div>
<div class="sect2">
<h3 id="_other_aws_service_quotas">Other AWS Service Quotas</h3>
<div class="paragraph">
<p>We have seen EKS customers impacted by the quotas listed below for other AWS services. Some of these may only apply to specific use cases or configurations, however you may consider if your solution will encounter any of these as it scales. The Quotas are organized by Service and each Quota has an ID in the format of L-XXXXXXXX you can use to look it up in the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html#request-increase">AWS Service Quotas console</a></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Service</th>
<th class="tableblock halign-left valign-top">Quota (L-xxxxx)</th>
<th class="tableblock halign-left valign-top"><strong>Impact</strong></th>
<th class="tableblock halign-left valign-top"><strong>ID (L-xxxxx)</strong></th>
<th class="tableblock halign-left valign-top">default</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">IAM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Roles per account</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of clusters or IRSA roles in an account.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-FE177D64</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">IAM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenId connect providers per account</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of Clusters per account, OpenID Connect is used by IRSA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-858F3967</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">100</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">IAM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Role trust policy length</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of of clusters an IAM role is associated with for IRSA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-C07B4B0D</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2,048</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">VPC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Security groups per network interface</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control or connectivity of the networking for your cluster</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-2AFB9258</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">VPC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">IPv4 CIDR blocks per VPC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of EKS Worker Nodes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-83CA0A9D</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">VPC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Routes per route table</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control or connectivity of the networking for your cluster</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-93826ACB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">50</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">VPC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Active VPC peering connections per VPC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control or connectivity of the networking for your cluster</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-7E9ECCDB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">50</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">VPC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Inbound or outbound rules per security group.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control or connectivity of the networking for your cluster, some controllers in EKS create new rules</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-0EA8095F</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">50</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">VPC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">VPCs per Region</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of Clusters per account or the control or connectivity of the networking for your cluster</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-F678F1CE</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">VPC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Internet gateways per Region</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of Clusters per account or the control or connectivity of the networking for your cluster</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-A4707A72</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">VPC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Network interfaces per Region</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of EKS Worker nodes, or Impact EKS control plane scaling/update activities.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-DF5E4CA3</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">VPC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Network Address Usage</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of Clusters per account or the control or connectivity of the networking for your cluster</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-BB24F6E5</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">64,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">VPC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Peered Network Address Usage</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of Clusters per account or the control or connectivity of the networking for your cluster</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-CD17FD4B</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">128,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ELB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Listeners per Network Load Balancer</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control of traffic ingress to the cluster.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-57A373D6</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">50</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ELB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Target Groups per Region</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control of traffic ingress to the cluster.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-B22855CB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">3,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ELB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Targets per Application Load Balancer</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control of traffic ingress to the cluster.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-7E6692B2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ELB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Targets per Network Load Balancer</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control of traffic ingress to the cluster.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-EEF1AD04</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">3,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ELB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Targets per Availability Zone per Network Load Balancer</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control of traffic ingress to the cluster.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-B211E961</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">500</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ELB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Targets per Target Group per Region</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control of traffic ingress to the cluster.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-A0D0B863</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ELB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Application Load Balancers per Region</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control of traffic ingress to the cluster.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-53DA6B97</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">50</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ELB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Classic Load Balancers per Region</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control of traffic ingress to the cluster.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-E9E9831D</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">20</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ELB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Network Load Balancers per Region</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the control of traffic ingress to the cluster.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-69A177A2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">50</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">EC2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Running On-Demand Standard (A, C, D, H, I, M, R, T, Z) instances (as a maximum vCPU count)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of EKS Worker Nodes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-1216C47A</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">EC2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">All Standard (A, C, D, H, I, M, R, T, Z) Spot Instance Requests (as a maximum vCPU count)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of EKS Worker Nodes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-34B43A08</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">EC2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">EC2-VPC Elastic IPs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of NAT GWs (and thus VPCs), which may limit the number of clusters in a region</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-0263D0A3</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">EBS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Snapshots per Region</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the backup strategy for stateful workloads</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-309BACF6</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">100,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">EBS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Storage for General Purpose SSD (gp3) volumes, in TiB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of EKS Worker Nodes, or PersistentVolume storage</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-7A658B76</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">50</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">EBS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Storage for General Purpose SSD (gp2) volumes, in TiB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of EKS Worker Nodes,  or PersistentVolume storage</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-D18FCD1D</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">50</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ECR</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Registered repositories</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of workloads in your clusters</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-CFEB8E8D</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">10,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ECR</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Images per repository</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of workloads in your clusters</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-03A36CE1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">10,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SecretsManager</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Secrets per Region</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can limit the number of workloads in your clusters</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L-2F66C23C</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">500,000</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_aws_request_throttling">AWS Request Throttling</h3>
<div class="paragraph">
<p>AWS services also implement request throttling to ensure that they remain performant and available for all customers. Simliar to Service Quotas, each AWS service maintains their own request throttling thresholds. Consider reviewing the respective AWS Service documentation if your workloads will need to quickly issue a large number of API calls or if you notice request throttling errors in your application.</p>
</div>
<div class="paragraph">
<p>EC2 API requests around provisioning EC2 network interfaces or IP addresses can encounter request throttling in large clusters or when clusters scale drastically. The table below shows some of the API actions that we have seen customers encounter request throttling from.
You can review the EC2 rate limit defaults and the steps to request a rate limit increase in the <a href="https://docs.aws.amazon.com/AWSEC2/latest/APIReference/throttling.html">EC2 documentation on Rate Throttling</a>.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Mutating Actions</th>
<th class="tableblock halign-left valign-top">Read-only Actions</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">AssignPrivateIpAddresses</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DescribeDhcpOptions</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">AttachNetworkInterface</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DescribeInstances</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CreateNetworkInterface</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DescribeNetworkInterfaces</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DeleteNetworkInterface</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DescribeSecurityGroups</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DeleteTags</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DescribeTags</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DetachNetworkInterface</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DescribeVpcs</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ModifyNetworkInterfaceAttribute</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">DescribeVolumes</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">UnassignPrivateIpAddresses</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_other_known_limits">Other Known Limits</h3>
<div class="ulist">
<ul>
<li>
<p>Route 53 DNS resolvers are limited to <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#vpc-dns-limits">1024 Packets per second</a>. This limit can be encountered when DNS traffic from a large cluster is funneled through a small number of CoreDNS Pod replicas. <a href="../cluster-services/#scale-coredns">Scaling CoreDNS and optimizing DNS behavior</a> can avoid timeouts on DNS lookups.</p>
<div class="ulist">
<ul>
<li>
<p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/DNSLimitations.html#limits-api-requests">Route 53 also has a fairly low rate limit of 5 requests per second to the Route 53 API</a>. If you have a large number of domains to update with a project like External DNS you may see rate throttling and delays in updating domains.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Some <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/volume_limits.html#instance-type-volume-limits">Nitro instance types have a volume attachment limit of 28</a> that is shared between Amazon EBS volumes, network interfaces, and NVMe instance store volumes. If your workloads are mounting numerous EBS volumes you may encounter limits to the pod density you can achieve with these instance types</p>
</li>
<li>
<p>There is a maximum number of connections that can be tracked per Ec2 instance. <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-connection-tracking.html#connection-tracking-throttling">If your workloads are handling a large number of connections you may see communication failures or errors because this maximum has been hit.</a> You can use the <code>conntrack_allowance_available</code> and <code>conntrack_allowance_exceeded</code> <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-network-performance-ena.html">network performance metrics to monitor the number of tracked connections on your EKS worker nodes</a>.</p>
</li>
<li>
<p>In EKS environment, etcd storage limit is <strong>8 GiB</strong> as per <a href="https://etcd.io/docs/v3.5/dev-guide/limit/#storage-size-limit">upstream guidance</a>. Please monitor metric <code>etcd_db_total_size_in_bytes</code> to track etcd db size. You can refer to <a href="https://github.com/etcd-io/etcd/blob/main/contrib/mixin/mixin.libsonnet#L213-L240">alert rules</a> <code>etcdBackendQuotaLowSpace</code> and <code>etcdExcessiveDatabaseGrowth</code> to setup this monitoring.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2024-08-27 01:00:33 -0500
</div>
</div>
</body>
</html>