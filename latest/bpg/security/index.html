<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<title>Amazon EKS Best Practices Guide for Security</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
</head>
<body class="article">
<div id="header">
<h1>Amazon EKS Best Practices Guide for Security</h1>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This guide provides advice about protecting information, systems, and
assets that are reliant on EKS while delivering business value through
risk assessments and mitigation strategies. The guidance herein is part
of a series of best practices guides that AWS is publishing to help
customers implement EKS in accordance with best practices. Guides for
Performance, Operational Excellence, Cost Optimization, and Reliability
will be available in the coming months.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_how_to_use_this_guide">How to use this guide</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This guide is meant for security practitioners who are responsible for
implementing and monitoring the effectiveness of security controls for
EKS clusters and the workloads they support. The guide is organized into
different topic areas for easier consumption. Each topic starts with a
brief overview, followed by a list of recommendations and best practices
for securing your EKS clusters. The topics do not need to be read in a
particular order.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_understanding_the_shared_responsibility_model">Understanding the Shared Responsibility Model</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Security and compliance are considered shared responsibilities when
using a managed service like EKS. Generally speaking, AWS is responsible
for security &#8220;of&#8221; the cloud whereas you, the customer, are responsible
for security &#8220;in&#8221; the cloud. With EKS, AWS is responsible for managing
of the EKS managed Kubernetes control plane. This includes the
Kubernetes control plane nodes, the ETCD database, and other
infrastructure necessary for AWS to deliver a secure and reliable
service. As a consumer of EKS, you are largely responsible for the
topics in this guide, e.g. IAM, pod security, runtime security, network
security, and so forth.</p>
</div>
<div class="paragraph">
<p>When it comes to infrastructure security, AWS will assume additional
responsibilities as you move from self-managed workers, to managed node
groups, to Fargate. For example, with Fargate, AWS becomes responsible
for securing the underlying instance/runtime used to run your Pods.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/SRM-EKS.jpg" alt="Shared Responsibility Model - Fargate">
</div>
<div class="title">Figure 1. Shared Responsibility Model - Fargate</div>
</div>
<div class="paragraph">
<p>AWS will also assume responsibility of keeping the EKS optimized AMI up
to date with Kubernetes patch versions and security patches. Customers
using Managed Node Groups (MNG) are responsible for upgrading their
Nodegroups to the latest AMI via EKS API, CLI, Cloudformation or AWS
Console. Also unlike Fargate, MNGs will not automatically scale your
infrastructure/cluster. That can be handled by the
<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md">cluster-autoscaler</a>
or other technologies such as <a href="https://karpenter.sh/">Karpenter</a>, native
AWS autoscaling, SpotInst’s
<a href="https://spot.io/solutions/kubernetes-2/">Ocean</a>, or Atlassian’s
<a href="https://github.com/atlassian/escalator">Escalator</a>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/SRM-MNG.jpg" alt="Shared Responsibility Model - MNG">
</div>
<div class="title">Figure 2. Shared Responsibility Model - MNG</div>
</div>
<div class="paragraph">
<p>Before designing your system, it is important to know where the line of
demarcation is between your responsibilities and the provider of the
service (AWS).</p>
</div>
<div class="paragraph">
<p>For additional information about the shared responsibility model, see
<a href="https://aws.amazon.com/compliance/shared-responsibility-model/" class="bare">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_introduction">Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>There are several security best practice areas that are pertinent when
using a managed Kubernetes service like EKS:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Identity and Access Management</p>
</li>
<li>
<p>Pod Security</p>
</li>
<li>
<p>Runtime Security</p>
</li>
<li>
<p>Network Security</p>
</li>
<li>
<p>Multi-tenancy</p>
</li>
<li>
<p>Multi Account for Multi-tenancy</p>
</li>
<li>
<p>Detective Controls</p>
</li>
<li>
<p>Infrastructure Security</p>
</li>
<li>
<p>Data Encryption and Secrets Management</p>
</li>
<li>
<p>Regulatory Compliance</p>
</li>
<li>
<p>Incident Response and Forensics</p>
</li>
<li>
<p>Image Security</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>As part of designing any system, you need to think about its security
implications and the practices that can affect your security posture.
For example, you need to control who can perform actions against a set
of resources. You also need the ability to quickly identify security
incidents, protect your systems and services from unauthorized access,
and maintain the confidentiality and integrity of data through data
protection. Having a well-defined and rehearsed set of processes for
responding to security incidents will improve your security posture too.
These tools and techniques are important because they support objectives
such as preventing financial loss or complying with regulatory
obligations.</p>
</div>
<div class="paragraph">
<p>AWS helps organizations achieve their security and compliance goals by
offering a rich set of security services that have evolved based on
feedback from a broad set of security conscious customers. By offering a
highly secure foundation, customers can spend less time on
&#8220;undifferentiated heavy lifting&#8221; and more time on achieving their
business objectives.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_feedback">Feedback</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This guide is being released on GitHub so as to collect direct feedback
and suggestions from the broader EKS/Kubernetes community. If you have a
best practice that you feel we ought to include in the guide, please
file an issue or submit a PR in the GitHub repository. Our intention is
to update the guide periodically as new features are added to the
service or when a new best practice evolves.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_further_reading">Further Reading</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://github.com/kubernetes/sig-security/blob/main/sig-security-external-audit/security-audit-2019/findings/Kubernetes%20White%20Paper.pdf">Kubernetes
Security Whitepaper</a>, sponsored by the Security Audit Working Group,
this Whitepaper describes key aspects of the Kubernetes attack surface
and security architecture with the aim of helping security practitioners
make sound design and implementation decisions.</p>
</div>
<div class="paragraph">
<p>The CNCF published also a
<a href="https://github.com/cncf/tag-security/blob/efb183dc4f19a1bf82f967586c9dfcb556d87534/security-whitepaper/v2/CNCF_cloud-native-security-whitepaper-May2022-v2.pdf">white
paper</a> on cloud native security. The paper examines how the technology
landscape has evolved and advocates for the adoption of security
practices that align with DevOps processes and agile methodologies.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_tools_and_resources">Tools and resources</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://catalog.workshops.aws/eks-security-immersionday/en-US">Amazon EKS
Security Immersion Workshop</a></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_identity_and_access_management">Identity and Access Management</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html">Identity
and Access Management</a> (IAM) is an AWS service that performs two
essential functions: Authentication and Authorization. Authentication
involves the verification of a identity whereas authorization governs
the actions that can be performed by AWS resources. Within AWS, a
resource can be another AWS service, e.g. EC2, or an AWS
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/intro-structure.html#intro-structure-principal">principal</a>
such as an
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html#id_iam-users">IAM
User</a> or
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html#id_iam-roles">Role</a>.
The rules governing the actions that a resource is allowed to perform
are expressed as
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html">IAM
policies</a>.</p>
</div>
<div class="sect2">
<h3 id="_controlling_access_to_eks_clusters">Controlling Access to EKS Clusters</h3>
<div class="paragraph">
<p>The Kubernetes project supports a variety of different strategies to
authenticate requests to the kube-apiserver service, e.g. Bearer Tokens,
X.509 certificates, OIDC, etc. EKS currently has native support for
<a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">webhook
token authentication</a>,
<a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#service-account-tokens">service
account tokens</a>, and as of February 21, 2021, OIDC authentication.</p>
</div>
<div class="paragraph">
<p>The webhook authentication strategy calls a webhook that verifies bearer
tokens. On EKS, these bearer tokens are generated by the AWS CLI or the
<a href="https://github.com/kubernetes-sigs/aws-iam-authenticator">aws-iam-authenticator</a>
client when you run <code>kubectl</code> commands. As you execute commands, the
token is passed to the kube-apiserver which forwards it to the
authentication webhook. If the request is well-formed, the webhook calls
a pre-signed URL embedded in the token’s body. This URL validates the
request’s signature and returns information about the user, e.g. the
user’s account, Arn, and UserId to the kube-apiserver.</p>
</div>
<div class="paragraph">
<p>To manually generate a authentication token, type the following command
in a terminal window:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">aws eks get-token --cluster-name &lt;cluster_name&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can also get a token programmatically. Below is an example written
in Go:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-golang" data-lang="golang">package main

import (
  "fmt"
  "log"
  "sigs.k8s.io/aws-iam-authenticator/pkg/token"
)

func main()  {
  g, _ := token.NewGenerator(false, false)
  tk, err := g.Get("&lt;cluster_name&gt;")
  if err != nil {
    log.Fatal(err)
  }
  fmt.Println(tk)
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The output should resemble this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "kind": "ExecCredential",
  "apiVersion": "client.authentication.k8s.io/v1alpha1",
  "spec": {},
  "status": {
    "expirationTimestamp": "2020-02-19T16:08:27Z",
    "token": "k8s-aws-v1.aHR0cHM6Ly9zdHMuYW1hem9uYXdzLmNvbS8_QWN0aW9uPUdldENhbGxlcklkZW50aXR5JlZlcnNpb249MjAxMS0wNi0xNSZYLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFKTkdSSUxLTlNSQzJXNVFBJTJGMjAyMDAyMTklMkZ1cy1lYXN0LTElMkZzdHMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIwMDIxOVQxNTU0MjdaJlgtQW16LUV4cGlyZXM9NjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JTNCeC1rOHMtYXdzLWlkJlgtQW16LVNpZ25hdHVyZT0yMjBmOGYzNTg1ZTMyMGRkYjVlNjgzYTVjOWE0MDUzMDFhZDc2NTQ2ZjI0ZjI4MTExZmRhZDA5Y2Y2NDhhMzkz"
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Each token starts with <code>k8s-aws-v1.</code> followed by a base64 encoded
string. The string, when decoded, should resemble to something similar
to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">https://sts.amazonaws.com/?Action=GetCallerIdentity&amp;Version=2011-06-15&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=XXXXJPFRILKNSRC2W5QA%2F20200219%2Fus-xxxx-1%2Fsts%2Faws4_request&amp;X-Amz-Date=20200219T155427Z&amp;X-Amz-Expires=60&amp;X-Amz-SignedHeaders=host%3Bx-k8s-aws-id&amp;X-Amz-Signature=XXXf8f3285e320ddb5e683a5c9a405301ad76546f24f28111fdad09cf648a393</code></pre>
</div>
</div>
<div class="paragraph">
<p>The token consists of a pre-signed URL that includes an Amazon
credential and signature. For additional details see
<a href="https://docs.aws.amazon.com/STS/latest/APIReference/API_GetCallerIdentity.html" class="bare">https://docs.aws.amazon.com/STS/latest/APIReference/API_GetCallerIdentity.html</a>.</p>
</div>
<div class="paragraph">
<p>The token has a time to live (TTL) of 15 minutes after which a new token
will need to be generated. This is handled automatically when you use a
client like <code>kubectl</code>, however, if you’re using the Kubernetes
dashboard, you will need to generate a new token and re-authenticate
each time the token expires.</p>
</div>
<div class="paragraph">
<p>Once the user’s identity has been authenticated by the AWS IAM service,
the kube-apiserver reads the <code>aws-auth</code> ConfigMap in the
<code>kube-system</code> Namespace to determine the RBAC group to associate with
the user. The <code>aws-auth</code> ConfigMap is used to create a static mapping
between IAM principals, i.e. IAM Users and Roles, and Kubernetes RBAC
groups. RBAC groups can be referenced in Kubernetes RoleBindings or
ClusterRoleBindings. They are similar to IAM Roles in that they define a
set of actions (verbs) that can be performed against a collection of
Kubernetes resources (objects).</p>
</div>
<div class="sect3">
<h4 id="_cluster_access_manager">Cluster Access Manager</h4>
<div class="paragraph">
<p>Cluster Access Manager, now the preferred way to manage access of AWS
IAM principals to Amazon EKS clusters, is a functionality of the AWS API
and is an opt-in feature for EKS v1.23 and later clusters (new or
existing). It simplifies identity mapping between AWS IAM and Kubernetes
RBACs, eliminating the need to switch between AWS and Kubernetes APIs or
editing the <code>aws-auth</code> ConfigMap for access management, reducing
operational overhead, and helping address misconfigurations. The tool
also enables cluster administrators to revoke or refine
<code>cluster-admin</code> permissions automatically granted to the AWS IAM
principal used to create the cluster.</p>
</div>
<div class="paragraph">
<p>This API relies on two concepts:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Access Entries:</strong> A cluster identity directly linked to an AWS IAM
principal (user or role) allowed to authenticate to an Amazon EKS
cluster.</p>
</li>
<li>
<p><strong>Access Policies:</strong> Are Amazon EKS specific policies that provides the
authorization for an Access Entry to perform actions in the Amazon EKS
cluster.</p>
</li>
</ul>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>At launch Amazon EKS supports only predefined and AWS managed policies.
Access policies are not IAM entities and are defined and managed by
Amazon EKS.</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>Cluster Access Manager allows the combination of upstream RBAC with
Access Policies supporting allow and pass (but not deny) on Kubernetes
AuthZ decisions regarding API server requests. A deny decision will
happen when both, the upstream RBAC and Amazon EKS authorizers can’t
determine the outcome of a request evaluation.</p>
</div>
<div class="paragraph">
<p>With this feature, Amazon EKS supports three modes of authentication:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><code>CONFIG_MAP</code> to continue using <code>aws-auth</code> configMap exclusively.</p>
</li>
<li>
<p><code>API_AND_CONFIG_MAP</code> to source authenticated IAM principals from
both EKS Access Entry APIs and the <code>aws-auth</code> configMap, prioritizing
the Access Entries. Ideal to migrate existing <code>aws-auth</code> permissions
to Access Entries.</p>
</li>
<li>
<p><code>API</code> to exclusively rely on EKS Access Entry APIs. This is the new
<strong>recommended approach</strong>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>To get started, cluster administrators can create or update Amazon EKS
clusters, setting the preferred authentication to <code>API_AND_CONFIG_MAP</code>
or <code>API</code> method and define Access Entries to grant access the desired
AWS IAM principals.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ aws eks create-cluster \
    --name &lt;CLUSTER_NAME&gt; \
    --role-arn &lt;CLUSTER_ROLE_ARN&gt; \
    --resources-vpc-config subnetIds=&lt;value&gt;,endpointPublicAccess=true,endpointPrivateAccess=true \
    --logging '{"clusterLogging":[{"types":["api","audit","authenticator","controllerManager","scheduler"],"enabled":true}]}' \
    --access-config authenticationMode=API_AND_CONFIG_MAP,bootstrapClusterCreatorAdminPermissions=false</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above command is an example to create an Amazon EKS cluster already
without the admin permissions of the cluster creator.</p>
</div>
<div class="paragraph">
<p>It is possible to update Amazon EKS clusters configuration to enable
<code>API</code> authenticationMode using the <code>update-cluster-config</code> command,
to do that on existing clusters using <code>CONFIG_MAP</code> you will have to
first update to <code>API_AND_CONFIG_MAP</code> and then to <code>API</code>. <strong>These
operations cannot be reverted</strong>, meaning that’s not possible to switch
from <code>API</code> to <code>API_AND_CONFIG_MAP</code> or <code>CONFIG_MAP</code>, and also from
<code>API_AND_CONFIG_MAP</code> to <code>CONFIG_MAP</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ aws eks update-cluster-config \
    --name &lt;CLUSTER_NAME&gt; \
    --access-config authenticationMode=API</code></pre>
</div>
</div>
<div class="paragraph">
<p>The API support commands to add and revoke access to the cluster, as
well as validate the existing Access Policies and Access Entries for the
specified cluster. The default policies are created to match Kubernetes
RBACs as follows.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">EKS Access Policy</th>
<th class="tableblock halign-left valign-top">Kubernetes RBAC</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">AmazonEKSClusterAdminPolicy</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">cluster-admin</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">AmazonEKSAdminPolicy</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">admin</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">AmazonEKSEditPolicy</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">edit</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">AmazonEKSViewPolicy</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">view</p></td>
</tr>
</tbody>
</table>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ aws eks list-access-policies
{
    "accessPolicies": [
        {
            "name": "AmazonEKSAdminPolicy",
            "arn": "arn:aws:eks::aws:cluster-access-policy/AmazonEKSAdminPolicy"
        },
        {
            "name": "AmazonEKSClusterAdminPolicy",
            "arn": "arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy"
        },
        {
            "name": "AmazonEKSEditPolicy",
            "arn": "arn:aws:eks::aws:cluster-access-policy/AmazonEKSEditPolicy"
        },
        {
            "name": "AmazonEKSViewPolicy",
            "arn": "arn:aws:eks::aws:cluster-access-policy/AmazonEKSViewPolicy"
        }
    ]
}

$ aws eks list-access-entries --cluster-name &lt;CLUSTER_NAME&gt;

{
    "accessEntries": []
}</code></pre>
</div>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>No Access Entries are available when the cluster is created without the
cluster creator admin permission, which is the only entry created by
default.</p>
</div>
</blockquote>
</div>
</div>
<div class="sect3">
<h4 id="_the_aws_auth_configmap_deprecated">The <code>aws-auth</code> ConfigMap <em>(deprecated)</em></h4>
<div class="paragraph">
<p>One way Kubernetes integration with AWS authentication can be done is
via the <code>aws-auth</code> ConfigMap, which resides in the <code>kube-system</code>
Namespace. It is responsible for mapping the AWS IAM Identities (Users,
Groups, and Roles) authentication, to Kubernetes role-based access
control (RBAC) authorization. The <code>aws-auth</code> ConfigMap is
automatically created in your Amazon EKS cluster during its provisioning
phase. It was initially created to allow nodes to join your cluster, but
as mentioned you can also use this ConfigMap to add RBACs access to IAM
principals.</p>
</div>
<div class="paragraph">
<p>To check your cluster’s <code>aws-auth</code> ConfigMap, you can use the
following command.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">kubectl -n kube-system get configmap aws-auth -o yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>This is a sample of a default configuration of the <code>aws-auth</code>
ConfigMap.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: v1
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      - system:node-proxier
      rolearn: arn:aws:iam::&lt;AWS_ACCOUNT_ID&gt;:role/kube-system-&lt;SELF_GENERATED_UUID&gt;
      username: system:node:{{SessionName}}
kind: ConfigMap
metadata:
  creationTimestamp: "2023-10-22T18:19:30Z"
  name: aws-auth
  namespace: kube-system</code></pre>
</div>
</div>
<div class="paragraph">
<p>The main session of this ConfigMap, is under <code>data</code> in the
<code>mapRoles</code> block, which is basically composed by 3 parameters.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>groups:</strong> The Kubernetes group(s) to map the IAM Role to. This can be
a default group, or a custom group specified in a <code>clusterrolebinding</code>
or <code>rolebinding</code>. In the above example we have just system groups
declared.</p>
</li>
<li>
<p><strong>rolearn:</strong> The ARN of the AWS IAM Role be mapped to the Kubernetes
group(s) add, using the following format
<code>arn:&lt;PARTITION&gt;:iam::&lt;AWS_ACCOUNT_ID&gt;:role/role-name</code>.</p>
</li>
<li>
<p><strong>username:</strong> The username within Kubernetes to map to the AWS IAM role.
This can be any custom name.</p>
</li>
</ul>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>It is also possible to map permissions for AWS IAM Users, defining a new
configuration block for <code>mapUsers</code>, under <code>data</code> in the <code>aws-auth</code>
ConfigMap, replacing the <strong>rolearn</strong> parameter for <strong>userarn</strong>, however as a
<strong>Best Practice</strong> it’s always recommended to user <code>mapRoles</code> instead.</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>To manage permissions, you can edit the <code>aws-auth</code> ConfigMap adding or
removing access to your Amazon EKS cluster. Although it’s possible to
edit the <code>aws-auth</code> ConfigMap manually, it’s recommended using tools
like <code>eksctl</code>, since this is a very senstitive configuration, and an
inaccurate configuration can lock you outside your Amazon EKS Cluster.
Check the subsection
<a href="https://aws.github.io/aws-eks-best-practices/security/docs/iam/#use-tools-to-make-changes-to-the-aws-auth-configmap">Use
tools to make changes to the aws-auth ConfigMap</a> below for more details.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_cluster_access_recommendations">Cluster Access Recommendations</h3>
<div class="sect3">
<h4 id="_make_the_eks_cluster_endpoint_private">Make the EKS Cluster Endpoint private</h4>
<div class="paragraph">
<p>By default when you provision an EKS cluster, the API cluster endpoint
is set to public, i.e. it can be accessed from the Internet. Despite
being accessible from the Internet, the endpoint is still considered
secure because it requires all API requests to be authenticated by IAM
and then authorized by Kubernetes RBAC. That said, if your corporate
security policy mandates that you restrict access to the API from the
Internet or prevents you from routing traffic outside the cluster VPC,
you can:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Configure the EKS cluster endpoint to be private. See
<a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html">Modifying
Cluster Endpoint Access</a> for further information on this topic.</p>
</li>
<li>
<p>Leave the cluster endpoint public and specify which CIDR blocks can
communicate with the cluster endpoint. The blocks are effectively a
whitelisted set of public IP addresses that are allowed to access the
cluster endpoint.</p>
</li>
<li>
<p>Configure public access with a set of whitelisted CIDR blocks and set
private endpoint access to enabled. This will allow public access from a
specific range of public IPs while forcing all network traffic between
the kubelets (workers) and the Kubernetes API through the cross-account
ENIs that get provisioned into the cluster VPC when the control plane is
provisioned.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_dont_use_a_service_account_token_for_authentication">Don’t use a service account token for authentication</h4>
<div class="paragraph">
<p>A service account token is a long-lived, static credential. If it is
compromised, lost, or stolen, an attacker may be able to perform all the
actions associated with that token until the service account is deleted.
At times, you may need to grant an exception for applications that have
to consume the Kubernetes API from outside the cluster, e.g. a CI/CD
pipeline application. If such applications run on AWS infrastructure,
like EC2 instances, consider using an instance profile and mapping that
to a Kubernetes RBAC role.</p>
</div>
</div>
<div class="sect3">
<h4 id="_employ_least_privileged_access_to_aws_resources">Employ least privileged access to AWS Resources</h4>
<div class="paragraph">
<p>An IAM User does not need to be assigned privileges to AWS resources to
access the Kubernetes API. If you need to grant an IAM user access to an
EKS cluster, create an entry in the <code>aws-auth</code> ConfigMap for that user
that maps to a specific Kubernetes RBAC group.</p>
</div>
</div>
<div class="sect3">
<h4 id="_remove_the_cluster_admin_permissions_from_the_cluster_creator_principal">Remove the cluster-admin permissions from the cluster creator principal</h4>
<div class="paragraph">
<p>By default Amazon EKS clusters are created with a permanent
<code>cluster-admin</code> permission bound to the cluster creator principal.
With the Cluster Access Manager API, it’s possible to create clusters
without this permission setting the
<code>--access-config bootstrapClusterCreatorAdminPermissions</code> to
<code>false</code>, when using <code>API_AND_CONFIG_MAP</code> or <code>API</code> authentication
mode. Revoke this access considered a best practice to avoid any
unwanted changes to the cluster configuration. The process to revoke
this access, follows the same process to revoke any other access to the
cluster.</p>
</div>
<div class="paragraph">
<p>The API gives you flexibility to only disassociate an IAM principal from
an Access Policy, in this case the <code>AmazonEKSClusterAdminPolicy</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ aws eks list-associated-access-policies \
    --cluster-name &lt;CLUSTER_NAME&gt; \
    --principal-arn &lt;IAM_PRINCIPAL_ARN&gt;

$ aws eks disassociate-access-policy --cluster-name &lt;CLUSTER_NAME&gt; \
    --principal-arn &lt;IAM_PRINCIPAL_ARN. \
    --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or completely removing the Access Entry associated with the
<code>cluster-admin</code> permission.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ aws eks list-access-entries --cluster-name &lt;CLUSTER_NAME&gt;

{
    "accessEntries": []
}

$ aws eks delete-access-entry --cluster-name &lt;CLUSTER_NAME&gt; \
  --principal-arn &lt;IAM_PRINCIPAL_ARN&gt;</code></pre>
</div>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>This access can be granted again if needed during an incident, emergency
or break glass scenario where the cluster is otherwise inaccessible.</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>If the cluster still configured with the <code>CONFIG_MAP</code> authentication
method, all additional users should be granted access to the cluster
through the <code>aws-auth</code> ConfigMap, and after <code>aws-auth</code> ConfigMap is
configured, the role assigned to the entity that created the cluster,
can be deleted and only recreated in case of an incident, emergency or
break glass scenario, or where the <code>aws-auth</code> ConfigMap is corrupted
and the cluster is otherwise inaccessible. This can be particularly
useful in production clusters.</p>
</div>
</div>
<div class="sect3">
<h4 id="_use_iam_roles_when_multiple_users_need_identical_access_to_the_cluster">Use IAM Roles when multiple users need identical access to the cluster</h4>
<div class="paragraph">
<p>Rather than creating an entry for each individual IAM User, allow those
users to assume an IAM Role and map that role to a Kubernetes RBAC
group. This will be easier to maintain, especially as the number of
users that require access grows.</p>
</div>
<div class="paragraph">
<p>!!! attention When accessing the EKS cluster with the IAM entity mapped
by <code>aws-auth</code> ConfigMap, the username described is recorded in the
user field of the Kubernetes audit log. If you’re using an IAM role, the
actual users who assume that role aren’t recorded and can’t be audited.</p>
</div>
<div class="paragraph">
<p>If still using the <code>aws-auth</code> configMap as the authentication method,
when assigning K8s RBAC permissions to an IAM role, you should include
\{{SessionName}} in your username. That way, the audit log will record
the session name so you can track who the actual user assume this role
along with the CloudTrail log.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">- rolearn: arn:aws:iam::XXXXXXXXXXXX:role/testRole
  username: testRole:{{SessionName}}
  groups:
    - system:masters</code></pre>
</div>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>In Kubernetes 1.20 and above, this change is no longer required, since
<code>user.extra.sessionName.0</code> was added to the Kubernetes audit log.</p>
</div>
</blockquote>
</div>
</div>
<div class="sect3">
<h4 id="_employ_least_privileged_access_when_creating_rolebindings_and_clusterrolebindings">Employ least privileged access when creating RoleBindings and ClusterRoleBindings</h4>
<div class="paragraph">
<p>Like the earlier point about granting access to AWS Resources,
RoleBindings and ClusterRoleBindings should only include the set of
permissions necessary to perform a specific function. Avoid using
<code>["*"]</code> in your Roles and ClusterRoles unless it’s absolutely
necessary. If you’re unsure what permissions to assign, consider using a
tool like <a href="https://github.com/liggitt/audit2rbac">audit2rbac</a> to
automatically generate Roles and binding based on the observed API calls
in the Kubernetes Audit Log.</p>
</div>
</div>
<div class="sect3">
<h4 id="_create_cluster_using_an_automated_process">Create cluster using an automated process</h4>
<div class="paragraph">
<p>As seen in earlier steps, when creating an Amazon EKS cluster, if not
using the using <code>API_AND_CONFIG_MAP</code> or <code>API</code> authentication mode,
and not opting out to delegate <code>cluster-admin</code> permissions to the
cluster creator, the IAM entity user or role, such as a federated
user that creates the cluster, is automatically
granted <code>system:masters</code> permissions in the cluster’s RBAC
configuration. Even being a best practice to remove this permission, as
described
<a href="#remove-the-cluster-admin-permissions-from-the-cluster-creator-principal">here</a>
if using the <code>CONFIG_MAP</code> authentication method, relying on
<code>aws-auth</code> ConfigMap, this access cannot be revoked. Therefore it is a
good idea to create the cluster with an infrastructure automation
pipeline tied to dedicated IAM role, with no permissions to be assumed
by other users or entities and regularly audit this role’s permissions,
policies, and who has access to trigger the pipeline. Also, this role
should not be used to perform routine actions on the cluster, and be
exclusively used to cluster level actions triggered by the pipeline, via
SCM code changes for example.</p>
</div>
</div>
<div class="sect3">
<h4 id="_create_the_cluster_with_a_dedicated_iam_role">Create the cluster with a dedicated IAM role</h4>
<div class="paragraph">
<p>When you create an Amazon EKS cluster, the IAM entity user or role, such
as a federated user that creates the cluster, is automatically
granted <code>system:masters</code> permissions in the cluster’s RBAC
configuration. This access cannot be removed and is not managed through
the <code>aws-auth</code> ConfigMap. Therefore it is a good idea to create the
cluster with a dedicated IAM role and regularly audit who can assume
this role. This role should not be used to perform routine actions on
the cluster, and instead additional users should be granted access to
the cluster through the <code>aws-auth</code> ConfigMap for this purpose. After
the <code>aws-auth</code> ConfigMap is configured, the role should be secured and
only used in temporary elevated privilege mode / break glass for
scenarios where the cluster is otherwise inaccessible. This can be
particularly useful in clusters which do not have direct user access
configured.</p>
</div>
</div>
<div class="sect3">
<h4 id="_regularly_audit_access_to_the_cluster">Regularly audit access to the cluster</h4>
<div class="paragraph">
<p>Who requires access is likely to change over time. Plan to periodically
audit the <code>aws-auth</code> ConfigMap to see who has been granted access and
the rights they’ve been assigned. You can also use open source tooling
like <a href="https://github.com/aquasecurity/kubectl-who-can">kubectl-who-can</a>,
or <a href="https://github.com/FairwindsOps/rbac-lookup">rbac-lookup</a> to examine
the roles bound to a particular service account, user, or group. We’ll
explore this topic further when we get to the section on
<a href="detective.md">auditing</a>. Additional ideas can be found in this
<a href="https://www.nccgroup.trust/us/about-us/newsroom-and-events/blog/2019/august/tools-and-methods-for-auditing-kubernetes-rbac-policies/?mkt_tok=eyJpIjoiWWpGa056SXlNV1E0WWpRNSIsInQiOiJBT1hyUTRHYkg1TGxBV0hTZnRibDAyRUZ0VzBxbndnRzNGbTAxZzI0WmFHckJJbWlKdE5WWDdUQlBrYVZpMnNuTFJ1R3hacVYrRCsxYWQ2RTRcL2pMN1BtRVA1ZFZcL0NtaEtIUDdZV3pENzNLcE1zWGVwUndEXC9Pb2tmSERcL1pUaGUifQ%3D%3D">article</a>
from NCC Group.</p>
</div>
</div>
<div class="sect3">
<h4 id="_if_relying_on_aws_auth_configmap_use_tools_to_make_changes">If relying on <code>aws-auth</code> configMap use tools to make changes</h4>
<div class="paragraph">
<p>An improperly formatted aws-auth ConfigMap may cause you to lose access
to the cluster. If you need to make changes to the ConfigMap, use a
tool.</p>
</div>
<div class="paragraph">
<p><strong>eksctl</strong> The <code>eksctl</code> CLI includes a command for adding identity
mappings to the aws-auth ConfigMap.</p>
</div>
<div class="paragraph">
<p>View CLI Help:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ eksctl create iamidentitymapping --help
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>Check the identities mapped to your Amazon EKS Cluster.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ eksctl get iamidentitymapping --cluster $CLUSTER_NAME --region $AWS_REGION
ARN                                                                   USERNAME                        GROUPS                                                  ACCOUNT
arn:aws:iam::788355785855:role/kube-system-&lt;SELF_GENERATED_UUID&gt;      system:node:{{SessionName}}     system:bootstrappers,system:nodes,system:node-proxier</code></pre>
</div>
</div>
<div class="paragraph">
<p>Make an IAM Role a Cluster Admin:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ eksctl create iamidentitymapping --cluster  &lt;CLUSTER_NAME&gt; --region=&lt;region&gt; --arn arn:aws:iam::123456:role/testing --group system:masters --username admin
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>For more information, review
<a href="https://eksctl.io/usage/iam-identity-mappings/"><code>eksctl</code> docs</a></p>
</div>
<div class="paragraph">
<p><strong><a href="https://github.com/keikoproj/aws-auth">aws-auth</a> by keikoproj</strong></p>
</div>
<div class="paragraph">
<p><code>aws-auth</code> by keikoproj includes both a cli and a go library.</p>
</div>
<div class="paragraph">
<p>Download and view help CLI help:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ go get github.com/keikoproj/aws-auth
...
$ aws-auth help
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>Alternatively, install <code>aws-auth</code> with the
<a href="https://krew.sigs.k8s.io">krew plugin manager</a> for kubectl.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ kubectl krew install aws-auth
...
$ kubectl aws-auth
...</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="https://github.com/keikoproj/aws-auth/blob/master/README.md">Review the
aws-auth docs on GitHub</a> for more information, including the go library.</p>
</div>
<div class="paragraph">
<p><strong><a href="https://github.com/kubernetes-sigs/aws-iam-authenticator/tree/master/cmd/aws-iam-authenticator">AWS
IAM Authenticator CLI</a></strong></p>
</div>
<div class="paragraph">
<p>The <code>aws-iam-authenticator</code> project includes a CLI for updating the
ConfigMap.</p>
</div>
<div class="paragraph">
<p><a href="https://github.com/kubernetes-sigs/aws-iam-authenticator/releases">Download
a release</a> on GitHub.</p>
</div>
<div class="paragraph">
<p>Add cluster permissions to an IAM Role:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ ./aws-iam-authenticator add role --rolearn arn:aws:iam::185309785115:role/lil-dev-role-cluster --username lil-dev-user --groups system:masters --kubeconfig ~/.kube/config
...</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_alternative_approaches_to_authentication_and_access_management">Alternative Approaches to Authentication and Access Management</h4>
<div class="paragraph">
<p>While IAM is the preferred way to authenticate users who need access to
an EKS cluster, it is possible to use an OIDC identity provider such as
GitHub using an authentication proxy and Kubernetes
<a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#user-impersonation">impersonation</a>.
Posts for two such solutions have been published on the AWS Open Source
blog:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://aws.amazon.com/blogs/opensource/authenticating-eks-github-credentials-teleport/">Authenticating
to EKS Using GitHub Credentials with Teleport</a></p>
</li>
<li>
<p><a href="https://aws.amazon.com/blogs/opensource/consistent-oidc-authentication-across-multiple-eks-clusters-using-kube-oidc-proxy/">Consistent
OIDC authentication across multiple EKS clusters using kube-oidc-proxy</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>!!! attention EKS natively supports OIDC authentication without using a
proxy. For further information, please read the launch blog,
<a href="https://aws.amazon.com/blogs/containers/introducing-oidc-identity-provider-authentication-amazon-eks/">Introducing
OIDC identity provider authentication for Amazon EKS</a>. For an example
showing how to configure EKS with Dex, a popular open source OIDC
provider with connectors for a variety of different authention methods,
see
<a href="https://aws.amazon.com/blogs/containers/using-dex-dex-k8s-authenticator-to-authenticate-to-amazon-eks/">Using
Dex &amp; dex-k8s-authenticator to authenticate to Amazon EKS</a>. As described
in the blogs, the username/group of users authenticated by an OIDC
provider will appear in the Kubernetes audit log.</p>
</div>
<div class="paragraph">
<p>You can also use
<a href="https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html">AWS
SSO</a> to federate AWS with an external identity provider, e.g. Azure AD.
If you decide to use this, the AWS CLI v2.0 includes an option to create
a named profile that makes it easy to associate an SSO session with your
current CLI session and assume an IAM role. Know that you must assume a
role <em>prior</em> to running <code>kubectl</code> as the IAM role is used to determine
the user’s Kubernetes RBAC group.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_identities_and_credentials_for_eks_pods">Identities and Credentials for EKS pods</h3>
<div class="paragraph">
<p>Certain applications that run within a Kubernetes cluster need
permission to call the Kubernetes API to function properly. For example,
the <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller">AWS
Load Balancer Controller</a> needs to be able to list a Service’s
Endpoints. The controller also needs to be able to invoke AWS APIs to
provision and configure an ALB. In this section we will explore the best
practices for assigning rights and privileges to Pods.</p>
</div>
<div class="sect3">
<h4 id="_kubernetes_service_accounts">Kubernetes Service Accounts</h4>
<div class="paragraph">
<p>A service account is a special type of object that allows you to assign
a Kubernetes RBAC role to a pod. A default service account is created
automatically for each Namespace within a cluster. When you deploy a pod
into a Namespace without referencing a specific service account, the
default service account for that Namespace will automatically get
assigned to the Pod and the Secret, i.e. the service account (JWT) token
for that service account, will get mounted to the pod as a volume at
<code>/var/run/secrets/kubernetes.io/serviceaccount</code>. Decoding the service
account token in that directory will reveal the following metadata:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "iss": "kubernetes/serviceaccount",
  "kubernetes.io/serviceaccount/namespace": "default",
  "kubernetes.io/serviceaccount/secret.name": "default-token-5pv4z",
  "kubernetes.io/serviceaccount/service-account.name": "default",
  "kubernetes.io/serviceaccount/service-account.uid": "3b36ddb5-438c-11ea-9438-063a49b60fba",
  "sub": "system:serviceaccount:default:default"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The default service account has the following permissions to the
Kubernetes API.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2020-01-30T18:13:25Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:discovery
  resourceVersion: "43"
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Adiscovery
  uid: 350d2ab8-438c-11ea-9438-063a49b60fba
rules:
- nonResourceURLs:
  - /api
  - /api/*
  - /apis
  - /apis/*
  - /healthz
  - /openapi
  - /openapi/*
  - /version
  - /version/
  verbs:
  - get</code></pre>
</div>
</div>
<div class="paragraph">
<p>This role authorizes unauthenticated and authenticated users to read API
information and is deemed safe to be publicly accessible.</p>
</div>
<div class="paragraph">
<p>When an application running within a Pod calls the Kubernetes APIs, the
Pod needs to be assigned a service account that explicitly grants it
permission to call those APIs. Similar to guidelines for user access,
the Role or ClusterRole bound to a service account should be restricted
to the API resources and methods that the application needs to function
and nothing else. To use a non-default service account simply set the
<code>spec.serviceAccountName</code> field of a Pod to the name of the service
account you wish to use. For additional information about creating
service accounts, see
<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions" class="bare">https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions</a>.</p>
</div>
<div class="paragraph">
<p>!!! note Prior to Kubernetes 1.24, Kubernetes would automatically create
a secret for each a service account. This secret was mounted to the pod
at /var/run/secrets/kubernetes.io/serviceaccount and would be used by
the pod to authenticate to the Kubernetes API server. In Kubernetes
1.24, a service account token is dynamically generated when the pod runs
and is only valid for an hour by default. A secret for the service
account will not be created. If you have an application that runs
outside the cluster that needs to authenticate to the Kubernetes API,
e.g. Jenkins, you will need to create a secret of type
<code>kubernetes.io/service-account-token</code> along with an annotation that
references the service account such as
<code>metadata.annotations.kubernetes.io/service-account.name: &lt;SERVICE_ACCOUNT_NAME&gt;</code>.
Secrets created in this way do not expire.</p>
</div>
</div>
<div class="sect3">
<h4 id="_iam_roles_for_service_accounts_irsa">IAM Roles for Service Accounts (IRSA)</h4>
<div class="paragraph">
<p>IRSA is a feature that allows you to assign an IAM role to a Kubernetes
service account. It works by leveraging a Kubernetes feature known as
<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection">Service
Account Token Volume Projection</a>. When Pods are configured with a
Service Account that references an IAM Role, the Kubernetes API server
will call the public OIDC discovery endpoint for the cluster on startup.
The endpoint cryptographically signs the OIDC token issued by Kubernetes
and the resulting token mounted as a volume. This signed token allows
the Pod to call the AWS APIs associated IAM role. When an AWS API is
invoked, the AWS SDKs calls <code>sts:AssumeRoleWithWebIdentity</code>. After
validating the token’s signature, IAM exchanges the Kubernetes issued
token for a temporary AWS role credential.</p>
</div>
<div class="paragraph">
<p>When using IRSA, it is important to
<a href="#reuse-aws-sdk-sessions-with-irsa">reuse AWS SDK sessions</a> to avoid
unneeded calls to AWS STS.</p>
</div>
<div class="paragraph">
<p>Decoding the (JWT) token for IRSA will produce output similar to the
example you see below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "aud": [
    "sts.amazonaws.com"
  ],
  "exp": 1582306514,
  "iat": 1582220114,
  "iss": "https://oidc.eks.us-west-2.amazonaws.com/id/D43CF17C27A865933144EA99A26FB128",
  "kubernetes.io": {
    "namespace": "default",
    "pod": {
      "name": "alpine-57b5664646-rf966",
      "uid": "5a20f883-5407-11ea-a85c-0e62b7a4a436"
    },
    "serviceaccount": {
      "name": "s3-read-only",
      "uid": "a720ba5c-5406-11ea-9438-063a49b60fba"
    }
  },
  "nbf": 1582220114,
  "sub": "system:serviceaccount:default:s3-read-only"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>This particular token grants the Pod view-only privileges to S3 by
assuming an IAM role. When the application attempts to read from S3, the
token is exchanged for a temporary set of IAM credentials that resembles
this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
    "AssumedRoleUser": {
        "AssumedRoleId": "AROA36C6WWEJULFUYMPB6:abc",
        "Arn": "arn:aws:sts::123456789012:assumed-role/eksctl-winterfell-addon-iamserviceaccount-de-Role1-1D61LT75JH3MB/abc"
    },
    "Audience": "sts.amazonaws.com",
    "Provider": "arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-west-2.amazonaws.com/id/D43CF17C27A865933144EA99A26FB128",
    "SubjectFromWebIdentityToken": "system:serviceaccount:default:s3-read-only",
    "Credentials": {
        "SecretAccessKey": "ORJ+8Adk+wW+nU8FETq7+mOqeA8Z6jlPihnV8hX1",
        "SessionToken": "FwoGZXIvYXdzEGMaDMLxAZkuLpmSwYXShiL9A1S0X87VBC1mHCrRe/pB2oes+l1eXxUYnPJyC9ayOoXMvqXQsomq0xs6OqZ3vaa5Iw1HIyA4Cv1suLaOCoU3hNvOIJ6C94H1vU0siQYk7DIq9Av5RZe+uE2FnOctNBvYLd3i0IZo1ajjc00yRK3v24VRq9nQpoPLuqyH2jzlhCEjXuPScPbi5KEVs9fNcOTtgzbVf7IG2gNiwNs5aCpN4Bv/Zv2A6zp5xGz9cWj2f0aD9v66vX4bexOs5t/YYhwuwAvkkJPSIGvxja0xRThnceHyFHKtj0H+bi/PWAtlI8YJcDX69cM30JAHDdQH+ltm/4scFptW1hlvMaP+WReCAaCrsHrAT+yka7ttw5YlUyvZ8EPog+j6fwHlxmrXM9h1BqdikomyJU00gm1++FJelfP+1zAwcyrxCnbRl3ARFrAt8hIlrT6Vyu8WvWtLxcI8KcLcJQb/LgkW+sCTGlYcY8z3zkigJMbYn07ewTL5Ss7LazTJJa758I7PZan/v3xQHd5DEc5WBneiV3iOznDFgup0VAMkIviVjVCkszaPSVEdK2NU7jtrh6Jfm7bU/3P6ZG+CkyDLIa8MBn9KPXeJd/y+jTk5Ii+fIwO/+mDpGNUribg6TPxhzZ8b/XdZO1kS1gVgqjXyVC+M+BRBh6C4H21w/eMzjCtDIpoxt5rGKL6Nu/IFMipoC4fgx6LIIHwtGYMG7SWQi7OsMAkiwZRg0n68/RqWgLzBt/4pfjSRYuk=",
        "Expiration": "2020-02-20T18:49:50Z",
        "AccessKeyId": "XXXX36C6WWEJUMHA3L7Z"
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>A mutating webhook that runs as part of the EKS control plane injects
the AWS Role ARN and the path to a web identity token file into the Pod
as environment variables. These values can also be supplied manually.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">AWS_ROLE_ARN=arn:aws:iam::AWS_ACCOUNT_ID:role/IAM_ROLE_NAME
AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token</code></pre>
</div>
</div>
<div class="paragraph">
<p>The kubelet will automatically rotate the projected token when it is
older than 80% of its total TTL, or after 24 hours. The AWS SDKs are
responsible for reloading the token when it rotates. For further
information about IRSA, see
<a href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-technical-overview.html" class="bare">https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-technical-overview.html</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_eks_pod_identities">EKS Pod Identities</h4>
<div class="paragraph">
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html">EKS
Pod Identities</a> is a feature launched at re:Invent 2023 that allows you
to assign an IAM role to a kubernetes service account, without the need
to configure an Open Id Connect (OIDC) identity provider(IDP) for each
cluster in your AWS account. To use EKS Pod Identity, you must deploy an
agent which runs as a DaemonSet pod on every eligible worker node. This
agent is made available to you as an EKS Add-on and is a pre-requisite
to use EKS Pod Identity feature. Your applications must use a
<a href="https://docs.aws.amazon.com/eks/latest/userguide/pod-id-minimum-sdk.html">supported
version of the AWS SDK</a> to use this feature.</p>
</div>
<div class="paragraph">
<p>When EKS Pod Identities are configured for a Pod, EKS will mount and
refresh a pod identity token at
<code>/var/run/secrets/pods.eks.amazonaws.com/serviceaccount/eks-pod-identity-token</code>.
This token will be used by the AWS SDK to communicate with the EKS Pod
Identity Agent, which uses the pod identity token and the agent’s IAM
role to create temporary credentials for your pods by calling the
<a href="https://docs.aws.amazon.com/eks/latest/APIReference/API_auth_AssumeRoleForPodIdentity.html">AssumeRoleForPodIdentity
API</a>. The pod identity token delivered to your pods is a JWT issued from
your EKS cluster and cryptographically signed, with appropriate JWT
claims for use with EKS Pod Identities.</p>
</div>
<div class="paragraph">
<p>To learn more about EKS Pod Identities, please see
<a href="https://aws.amazon.com/blogs/containers/amazon-eks-pod-identity-a-new-way-for-applications-on-eks-to-obtain-iam-credentials/">this
blog</a>.</p>
</div>
<div class="paragraph">
<p>You do not have to make any modifications to your application code to
use EKS Pod Identities. Supported AWS SDK versions will automatically
discover credentials made available with EKS Pod Identities by using the
<a href="https://docs.aws.amazon.com/sdkref/latest/guide/standardized-credentials.html">credential
provider chain</a>. Like IRSA, EKS pod identities sets variables within
your pods to direct them how to find AWS credentials.</p>
</div>
<div class="sect4">
<h5 id="_working_with_iam_roles_for_eks_pod_identities">Working with IAM roles for EKS Pod Identities</h5>
<div class="ulist">
<ul>
<li>
<p>EKS Pod Identities can only directly assume an IAM role that belongs
to the same AWS account as the EKS cluster. To access an IAM role in
another AWS account, you must assume that role by
<a href="https://docs.aws.amazon.com/sdkref/latest/guide/feature-assume-role-credentials.html">configuring
a profile in your SDK configuration</a>, or in your
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/sts_example_sts_AssumeRole_section.html">application’s
code</a>.</p>
</li>
<li>
<p>When EKS Pod Identities are being configured for Service Accounts, the
person or process configuring the Pod Identity Association must have the
<code>iam:PassRole</code> entitlement for that role.</p>
</li>
<li>
<p>Each Service Account may only have one IAM role associated with it
through EKS Pod Identities, however you can associate the same IAM role
with multiple service accounts.</p>
</li>
<li>
<p>IAM roles used with EKS Pod Identities must allow the
<code>pods.eks.amazonaws.com</code> Service Principal to assume them, <em>and</em> set
session tags. The following is an example role trust policy which allows
EKS Pod Identities to use an IAM role:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "pods.eks.amazonaws.com"
      },
      "Action": [
        "sts:AssumeRole",
        "sts:TagSession"
      ],
      "Condition": {
        "StringEquals": {
          "aws:SourceOrgId": "${aws:ResourceOrgId}"
        }
      }
    }
  ]
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>AWS recommends using condition keys like <code>aws:SourceOrgId</code> to help
protect against the
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/confused-deputy.html#cross-service-confused-deputy-prevention">cross-service
confused deputy problem</a>. In the above example role trust policy, the
<code>ResourceOrgId</code> is a variable equal to the AWS Organizations
Organization ID of the AWS Organization that the AWS account belongs to.
EKS will pass in a value for <code>aws:SourceOrgId</code> equal to that when
assuming a role with EKS Pod Identities.</p>
</div>
</div>
<div class="sect4">
<h5 id="_abac_and_eks_pod_identities">ABAC and EKS Pod Identities</h5>
<div class="paragraph">
<p>When EKS Pod Identities assumes an IAM role, it sets the following
session tags:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">EKS Pod Identities Session Tag</th>
<th class="tableblock halign-left valign-top">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">kubernetes-namespace</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The namespace the pod associated with EKS Pod
Identities runs in.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">kubernetes-service-account</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The name of the kubernetes service account
associated with EKS Pod Identities</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">eks-cluster-arn</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The ARN of the EKS cluster,
e.g. <code>arn:${Partition}:eks:${Region}:${Account}:cluster/${ClusterName}</code>.
The cluster ARN is unique, but if a cluster is deleted and recreated in
the same region with the same name, within the same AWS account, it will
have the same ARN.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">eks-cluster-name</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The name of the EKS cluster. Please note that EKS
cluster names can be same within your AWS account, and EKS clusters in
other AWS accounts.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">kubernetes-pod-name</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The name of the pod in EKS.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">kubernetes-pod-uid</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The UID of the pod in EKS.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>These session tags allow you to use
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html">Attribute
Based Access Control(ABAC)</a> to grant access to your AWS resources to
only specific kubernetes service accounts. When doing so, it is <em>very
important</em> to understand that kubernetes service accounts are only
unique within a namespace, and kubernetes namespaces are only unique
within an EKS cluster. These session tags can be accessed in AWS
policies by using the <code>aws:PrincipalTag/&lt;tag-key&gt;</code> global condition
key, such as <code>aws:PrincipalTag/eks-cluster-arn</code></p>
</div>
<div class="paragraph">
<p>For example, if you wanted to grant access to only a specific service
account to access an AWS resource in your account with an IAM or
resource policy, you would need to check <code>eks-cluster-arn</code> and
<code>kubernetes-namespace</code> tags as well as the
<code>kubernetes-service-account</code> to ensure that only that service accounts
from the intended cluster have access to that resource as other clusters
could have identical <code>kubernetes-service-accounts</code> and
<code>kubernetes-namespaces</code>.</p>
</div>
<div class="paragraph">
<p>This example S3 Bucket policy only grants access to objects in the S3
bucket it’s attached to, only if <code>kubernetes-service-account</code>,
<code>kubernetes-namespace</code>, <code>eks-cluster-arn</code> all meet their expected
values, where the EKS cluster is hosted in the AWS account
<code>111122223333</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::111122223333:root"
            },
            "Action": "s3:*",
            "Resource":            [
                "arn:aws:s3:::ExampleBucket/*"
            ],
            "Condition": {
                "StringEquals": {
                    "aws:PrincipalTag/kubernetes-service-account": "s3objectservice",
                    "aws:PrincipalTag/eks-cluster-arn": "arn:aws:eks:us-west-2:111122223333:cluster/ProductionCluster",
                    "aws:PrincipalTag/kubernetes-namespace": "s3datanamespace"
                }
            }
        }
    ]
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_eks_pod_identities_compared_to_irsa">EKS Pod Identities compared to IRSA</h4>
<div class="paragraph">
<p>Both EKS Pod Identities and IRSA are preferred ways to deliver temporary
AWS credentials to your EKS pods. Unless you have specific usecases for
IRSA, we recommend you use EKS Pod Identities when using EKS. This table
helps compare the two features.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 34%;">
<col style="width: 33%;">
<col style="width: 33%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">#</th>
<th class="tableblock halign-left valign-top">EKS Pod Identities</th>
<th class="tableblock halign-left valign-top">IRSA</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Requires permission to create an OIDC IDP in your AWS accounts?</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yes</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Requires unique IDP setup per cluster</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yes</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Sets relevant session tags for use with ABAC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Requires an iam:PassRole Check?</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Uses AWS STS Quota from your AWS account?</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yes</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Can access other AWS accounts</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Indirectly with role chaining</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Directly
with sts:AssumeRoleWithWebIdentity</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Compatible with AWS SDKs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yes</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Requires Pod Identity Agent Daemonset on nodes?</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_identities_and_credentials_for_eks_pods_recommendations">Identities and Credentials for EKS pods Recommendations</h3>
<div class="sect3">
<h4 id="_update_the_aws_node_daemonset_to_use_irsa">Update the aws-node daemonset to use IRSA</h4>
<div class="paragraph">
<p>At present, the aws-node daemonset is configured to use a role assigned
to the EC2 instances to assign IPs to pods. This role includes several
AWS managed policies, e.g. AmazonEKS_CNI_Policy and
EC2ContainerRegistryReadOnly that effectively allow <strong>all</strong> pods running
on a node to attach/detach ENIs, assign/unassign IP addresses, or pull
images from ECR. Since this presents a risk to your cluster, it is
recommended that you update the aws-node daemonset to use IRSA. A script
for doing this can be found in the
<a href="https://github.com/aws/aws-eks-best-practices/tree/master/projects/enable-irsa/src">repository</a>
for this guide.</p>
</div>
<div class="paragraph">
<p>The aws-node daemonset does not support EKS Pod Identities at this time.</p>
</div>
</div>
<div class="sect3">
<h4 id="_restrict_access_to_the_instance_profile_assigned_to_the_worker_node">Restrict access to the instance profile assigned to the worker node</h4>
<div class="paragraph">
<p>When you use IRSA or EKS Pod Identities, it updates the credential chain
of the pod to use IRSA or EKS Pod Identities first, however, the pod
<em>can still inherit the rights of the instance profile assigned to the
worker node</em>. When using IRSA or EKS Pod Identities, it is <strong>strongly</strong>
recommended that you block access
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html">instance
metadata</a> to help ensure that your applications only have the
permissions they require, and not their nodes.</p>
</div>
<div class="paragraph">
<p>!!! caution Blocking access to instance metadata will prevent pods that
do not use IRSA or EKS Pod Identities from inheriting the role assigned
to the worker node.</p>
</div>
<div class="paragraph">
<p>You can block access to instance metadata by requiring the instance to
use IMDSv2 only and updating the hop count to 1 as in the example below.
You can also include these settings in the node group’s launch template.
Do <strong>not</strong> disable instance metadata as this will prevent components like
the node termination handler and other things that rely on instance
metadata from working properly.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ aws ec2 modify-instance-metadata-options --instance-id &lt;value&gt; --http-tokens required --http-put-response-hop-limit 1
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you are using Terraform to create launch templates for use with
Managed Node Groups, add the metadata block to configure the hop count
as seen in this code snippet:</p>
</div>
<div class="paragraph">
<p><code>tf hl_lines="7" resource "aws_launch_template" "foo" {   name = "foo"   ...     metadata_options {     http_endpoint               = "enabled"     http_tokens                 = "required"     http_put_response_hop_limit = 1     instance_metadata_tags      = "enabled"   }   ...</code></p>
</div>
<div class="paragraph">
<p>You can also block a pod’s access to EC2 metadata by manipulating
iptables on the node. For further information about this method, see
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html#instance-metadata-limiting-access">Limiting
access to the instance metadata service</a>.</p>
</div>
<div class="paragraph">
<p>If you have an application that is using an older version of the AWS SDK
that doesn’t support IRSA or EKS Pod Identities, you should update the
SDK version.</p>
</div>
</div>
<div class="sect3">
<h4 id="_scope_the_iam_role_trust_policy_for_irsa_roles_to_the_service_account_name_namespace_and_cluster">Scope the IAM Role trust policy for IRSA Roles to the service account name, namespace, and cluster</h4>
<div class="paragraph">
<p>The trust policy can be scoped to a Namespace or a specific service
account within a Namespace. When using IRSA it’s best to make the role
trust policy as explicit as possible by including the service account
name. This will effectively prevent other Pods within the same Namespace
from assuming the role. The CLI <code>eksctl</code> will do this automatically
when you use it to create service accounts/IAM roles. See
<a href="https://eksctl.io/usage/iamserviceaccounts/" class="bare">https://eksctl.io/usage/iamserviceaccounts/</a> for further information.</p>
</div>
<div class="paragraph">
<p>When working with IAM directly, this is adding condition into the role’s
trust policy that uses conditions to ensure the <code>:sub</code> claim are the
namespace and service account you expect. As an example, before we had
an IRSA token with a sub claim of
&#8220;system:serviceaccount:default:s3-read-only&#8221; . This is the <code>default</code>
namespace and the service account is <code>s3-read-only</code>. You would use a
condition like the following to ensure that only your service account in
a given namespace from your cluster can assume that role:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">  "Condition": {
      "StringEquals": {
          "oidc.eks.us-west-2.amazonaws.com/id/D43CF17C27A865933144EA99A26FB128:aud": "sts.amazonaws.com",
          "oidc.eks.us-west-2.amazonaws.com/id/D43CF17C27A865933144EA99A26FB128:sub": "system:serviceaccount:default:s3-read-only"
      }
  }</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_use_one_iam_role_per_application">Use one IAM role per application</h4>
<div class="paragraph">
<p>With both IRSA and EKS Pod Identity, it is a best practice to give each
application its own IAM role. This gives you improved isolation as you
can modify one application without impacting another, and allows you to
apply the principal of least privilege by only granting an application
the permissions it needs.</p>
</div>
<div class="paragraph">
<p>When using ABAC with EKS Pod Identity, you may use a common IAM role
across multiple service accounts and rely on their session attributes
for access control. This is especially useful when operating at scale,
as ABAC allows you to operate with fewer IAM roles.</p>
</div>
</div>
<div class="sect3">
<h4 id="_when_your_application_needs_access_to_imds_use_imdsv2_and_increase_the_hop_limit_on_ec2_instances_to_2">When your application needs access to IMDS, use IMDSv2 and increase the hop limit on EC2 instances to 2</h4>
<div class="paragraph">
<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html">IMDSv2</a>
requires you use a PUT request to get a session token. The initial PUT
request has to include a TTL for the session token. Newer versions of
the AWS SDKs will handle this and the renewal of said token
automatically. It’s also important to be aware that the default hop
limit on EC2 instances is intentionally set to 1 to prevent IP
forwarding. As a consequence, Pods that request a session token that are
run on EC2 instances may eventually time out and fallback to using the
IMDSv1 data flow. EKS adds support IMDSv2 by <em>enabling</em> both v1 and v2
and changing the hop limit to 2 on nodes provisioned by eksctl or with
the official CloudFormation templates.</p>
</div>
</div>
<div class="sect3">
<h4 id="_disable_auto_mounting_of_service_account_tokens">Disable auto-mounting of service account tokens</h4>
<div class="paragraph">
<p>If your application doesn’t need to call the Kubernetes API set the
<code>automountServiceAccountToken</code> attribute to <code>false</code> in the PodSpec
for your application or patch the default service account in each
namespace so that it’s no longer mounted to pods automatically. For
example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">kubectl patch serviceaccount default -p $'automountServiceAccountToken: false'</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_use_dedicated_service_accounts_for_each_application">Use dedicated service accounts for each application</h4>
<div class="paragraph">
<p>Each application should have its own dedicated service account. This
applies to service accounts for the Kubernetes API as well as IRSA and
EKS Pod Identity.</p>
</div>
<div class="paragraph">
<p>!!! attention If you employ a blue/green approach to cluster upgrades
instead of performing an in-place cluster upgrade when using IRSA, you
will need to update the trust policy of each of the IRSA IAM roles with
the OIDC endpoint of the new cluster. A blue/green cluster upgrade is
where you create a cluster running a newer version of Kubernetes
alongside the old cluster and use a load balancer or a service mesh to
seamlessly shift traffic from services running on the old cluster to the
new cluster. When using blue/green cluster upgrades with EKS Pod
Identity, you would create pod identity associations between the IAM
roles and service accounts in the new cluster. And update the IAM role
trust policy if you have a <code>sourceArn</code> condition.</p>
</div>
</div>
<div class="sect3">
<h4 id="_run_the_application_as_a_non_root_user">Run the application as a non-root user</h4>
<div class="paragraph">
<p>Containers run as root by default. While this allows them to read the
web identity token file, running a container as root is not considered a
best practice. As an alternative, consider adding the
<code>spec.securityContext.runAsUser</code> attribute to the PodSpec. The value
of <code>runAsUser</code> is arbitrary value.</p>
</div>
<div class="paragraph">
<p>In the following example, all processes within the Pod will run under
the user ID specified in the <code>runAsUser</code> field.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo
spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
  containers:
  - name: sec-ctx-demo
    image: busybox
    command: [ "sh", "-c", "sleep 1h" ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>When you run a container as a non-root user, it prevents the container
from reading the IRSA service account token because the token is
assigned 0600 [root] permissions by default. If you update the
securityContext for your container to include fsgroup=65534 [Nobody] it
will allow the container to read the token.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">spec:
  securityContext:
    fsGroup: 65534</code></pre>
</div>
</div>
<div class="paragraph">
<p>In Kubernetes 1.19 and above, this change is no longer required and
applications can read the IRSA service account token without adding them
to the Nobody group.</p>
</div>
</div>
<div class="sect3">
<h4 id="_grant_least_privileged_access_to_applications">Grant least privileged access to applications</h4>
<div class="paragraph">
<p><a href="https://github.com/princespaghetti/actionhero">Action Hero</a> is a utility
that you can run alongside your application to identify the AWS API
calls and corresponding IAM permissions your application needs to
function properly. It is similar to
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor.html">IAM
Access Advisor</a> in that it helps you gradually limit the scope of IAM
roles assigned to applications. Consult the documentation on granting
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege">least
privileged access</a> to AWS resources for further information.</p>
</div>
<div class="paragraph">
<p>Consider setting a
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html">permissions
boundary</a> on IAM roles used with IRSA and Pod Identities. You can use
the permissions boundary to ensure that the roles used by IRSA or Pod
Identities can not exceed a maximum level of permissions. For an example
guide on getting started with permissions boundaries with an example
permissions boundary policy, please see this
<a href="https://github.com/aws-samples/example-permissions-boundary">github
repo</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_review_and_revoke_unnecessary_anonymous_access_to_your_eks_cluster">Review and revoke unnecessary anonymous access to your EKS cluster</h4>
<div class="paragraph">
<p>Ideally anonymous access should be disabled for all API actions.
Anonymous access is granted by creating a RoleBinding or
ClusterRoleBinding for the Kubernetes built-in user system:anonymous.
You can use the <a href="https://github.com/FairwindsOps/rbac-lookup">rbac-lookup</a>
tool to identify permissions that system:anonymous user has on your
cluster:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">./rbac-lookup | grep -P 'system:(anonymous)|(unauthenticated)'
system:anonymous               cluster-wide        ClusterRole/system:discovery
system:unauthenticated         cluster-wide        ClusterRole/system:discovery
system:unauthenticated         cluster-wide        ClusterRole/system:public-info-viewer</code></pre>
</div>
</div>
<div class="paragraph">
<p>Any role or ClusterRole other than system:public-info-viewer should not
be bound to system:anonymous user or system:unauthenticated group.</p>
</div>
<div class="paragraph">
<p>There may be some legitimate reasons to enable anonymous access on
specific APIs. If this is the case for your cluster ensure that only
those specific APIs are accessible by anonymous user and exposing those
APIs without authentication doesn’t make your cluster vulnerable.</p>
</div>
<div class="paragraph">
<p>Prior to Kubernetes/EKS Version 1.14, system:unauthenticated group was
associated to system:discovery and system:basic-user ClusterRoles by
default. Note that even if you have updated your cluster to version 1.14
or higher, these permissions may still be enabled on your cluster, since
cluster updates do not revoke these permissions. To check which
ClusterRoles have &#8220;system:unauthenticated&#8221; except
system:public-info-viewer you can run the following command (requires jq
util):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">kubectl get ClusterRoleBinding -o json | jq -r '.items[] | select(.subjects[]?.name =="system:unauthenticated") | select(.metadata.name != "system:public-info-viewer") | .metadata.name'</code></pre>
</div>
</div>
<div class="paragraph">
<p>And &#8220;system:unauthenticated&#8221; can be removed from all the roles except
&#8220;system:public-info-viewer&#8221; using:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">kubectl get ClusterRoleBinding -o json | jq -r '.items[] | select(.subjects[]?.name =="system:unauthenticated") | select(.metadata.name != "system:public-info-viewer") | del(.subjects[] | select(.name =="system:unauthenticated"))' | kubectl apply -f -</code></pre>
</div>
</div>
<div class="paragraph">
<p>Alternatively, you can check and remove it manually by kubectl describe
and kubectl edit. To check if system:unauthenticated group has
system:discovery permissions on your cluster run the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">kubectl describe clusterrolebindings system:discovery

Name:         system:discovery
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  system:discovery
Subjects:
  Kind   Name                    Namespace
  ----   ----                    ---------
  Group  system:authenticated
  Group  system:unauthenticated</code></pre>
</div>
</div>
<div class="paragraph">
<p>To check if system:unauthenticated group has system:basic-user
permission on your cluster run the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">kubectl describe clusterrolebindings system:basic-user

Name:         system:basic-user
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  system:basic-user
Subjects:
  Kind   Name                    Namespace
  ----   ----                    ---------
  Group  system:authenticated
  Group  system:unauthenticated</code></pre>
</div>
</div>
<div class="paragraph">
<p>If system:unauthenticated group is bound to system:discovery and/or
system:basic-user ClusterRoles on your cluster, you should disassociate
these roles from system:unauthenticated group. Edit system:discovery
ClusterRoleBinding using the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">kubectl edit clusterrolebindings system:discovery</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above command will open the current definition of system:discovery
ClusterRoleBinding in an editor as shown below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2021-06-17T20:50:49Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:discovery
  resourceVersion: "24502985"
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Adiscovery
  uid: b7936268-5043-431a-a0e1-171a423abeb6
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:discovery
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:authenticated
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:unauthenticated</code></pre>
</div>
</div>
<div class="paragraph">
<p>Delete the entry for system:unauthenticated group from the &#8220;subjects&#8221;
section in the above editor screen.</p>
</div>
<div class="paragraph">
<p>Repeat the same steps for system:basic-user ClusterRoleBinding.</p>
</div>
</div>
<div class="sect3">
<h4 id="_reuse_aws_sdk_sessions_with_irsa">Reuse AWS SDK sessions with IRSA</h4>
<div class="paragraph">
<p>When you use IRSA, applications written using the AWS SDK use the token
delivered to your pods to call <code>sts:AssumeRoleWithWebIdentity</code> to
generate temporary AWS credentials. This is different from other AWS
compute services, where the compute service delivers temporary AWS
credentials directly to the AWS compute resource, such as a lambda
function. This means that every time an AWS SDK session is initialized,
a call to AWS STS for <code>AssumeRoleWithWebIdentity</code> is made. If your
application scales rapidly and initializes many AWS SDK sessions, you
may experience throttling from AWS STS as your code will be making many
calls for <code>AssumeRoleWithWebIdentity</code>.</p>
</div>
<div class="paragraph">
<p>To avoid this scenario, we recommend reusing AWS SDK sessions within
your application so that unnecessary calls to
<code>AssumeRoleWithWebIdentity</code> are not made.</p>
</div>
<div class="paragraph">
<p>In the following example code, a session is created using the boto3
python SDK, and that same session is used to create clients and interact
with both Amazon S3 and Amazon SQS. <code>AssumeRoleWithWebIdentity</code> is
only called once, and the AWS SDK will refresh the credentials of
<code>my_session</code> when they expire automatically.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-py hl_lines="`4 7 8`" +" data-lang="py hl_lines="`4 7 8`" +">import boto3

= Create your own session

my_session = boto3.session.Session()

= Now we can create low-level clients from our session

sqs = my_session.client('`sqs`') s3 = my_session.client('`s3`')

s3response = s3.list_buckets() sqsresponse = sqs.list_queues()

#print the response from the S3 and SQS APIs print("`s3 response:`")
print(s3response) print("`—`") print("`sqs response:`")
print(sqsresponse) ```

If you’re migrating an application from another AWS compute service,
such as EC2, to EKS with IRSA, this is a particularly important detail.
On other compute services initializing an AWS SDK session does not call
AWS STS unless you instruct it to.

=== Alternative approaches

While IRSA and EKS Pod Identities are the _preferred ways_ to assign an
AWS identity to a pod, they require that you include recent version of
the AWS SDKs in your application. For a complete listing of the SDKs
that currently support IRSA, see
https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-minimum-sdk.html,
for EKS Pod Identities, see
https://docs.aws.amazon.com/eks/latest/userguide/pod-id-minimum-sdk.html.
If you have an application that you can’t immediately update with a
compatible SDK, there are several community-built solutions available
for assigning IAM roles to Kubernetes pods, including
https://github.com/jtblin/kube2iam[kube2iam] and
https://github.com/uswitch/kiam[kiam]. Although AWS doesn’t endorse,
condone, nor support the use of these solutions, they are frequently
used by the community at large to achieve similar results as IRSA and
EKS Pod Identities.

If you need to use one of these non-aws provided solutions, please
exercise due diligence and ensure you understand security implications
of doing so.

== Tools and Resources

* https://catalog.workshops.aws/eks-security-immersionday/en-US/2-identity-and-access-management[Amazon
EKS Security Immersion Workshop - Identity and Access Management]
* https://github.com/aws-ia/terraform-aws-eks-blueprints/tree/main/patterns/fully-private-cluster[Terraform
EKS Blueprints Pattern - Fully Private Amazon EKS Cluster]
* https://github.com/aws-ia/terraform-aws-eks-blueprints/tree/main/patterns/sso-iam-identity-center[Terraform
EKS Blueprints Pattern - IAM Identity Center Single Sign-On for Amazon
EKS Cluster]
* https://github.com/aws-ia/terraform-aws-eks-blueprints/tree/main/patterns/sso-okta[Terraform
EKS Blueprints Pattern - Okta Single Sign-On for Amazon EKS Cluster]
* https://github.com/liggitt/audit2rbac[audit2rbac]
* https://github.com/mhausenblas/rbac.dev[rbac.dev] A list of additional
resources, including blogs and tools, for Kubernetes RBAC
* https://github.com/princespaghetti/actionhero[Action Hero]
* https://github.com/jtblin/kube2iam[kube2iam]
* https://github.com/uswitch/kiam[kiam]

:leveloffset!:
:leveloffset: +1

= Pod Security

The pod specification includes a variety of different attributes that
can strengthen or weaken your overall security posture. As a Kubernetes
practitioner your chief concern should be preventing a process that’s
running in a container from escaping the isolation boundaries of the
container runtime and gaining access to the underlying host.

== Linux Capabilities

The processes that run within a container run under the context of the
[Linux] root user by default. Although the actions of root within a
container are partially constrained by the set of Linux capabilities
that the container runtime assigns to the containers, these default
privileges could allow an attacker to escalate their privileges and/or
gain access to sensitive information bound to the host, including
Secrets and ConfigMaps. Below is a list of the default capabilities
assigned to containers. For additional information about each
capability, see
http://man7.org/linux/man-pages/man7/capabilities.7.html.

`+CAP_AUDIT_WRITE, CAP_CHOWN, CAP_DAC_OVERRIDE, CAP_FOWNER, CAP_FSETID, CAP_KILL, CAP_MKNOD, CAP_NET_BIND_SERVICE, CAP_NET_RAW, CAP_SETGID, CAP_SETUID, CAP_SETFCAP, CAP_SETPCAP, CAP_SYS_CHROOT+`

!!! Info

EC2 and Fargate pods are assigned the aforementioned capabilities by
default. Additionally, Linux capabilities can only be dropped from
Fargate pods.

Pods that are run as privileged, inherit _all_ of the Linux capabilities
associated with root on the host. This should be avoided if possible.

=== Node Authorization

All Kubernetes worker nodes use an authorization mode called
https://kubernetes.io/docs/reference/access-authn-authz/node/[Node
Authorization]. Node Authorization authorizes all API requests that
originate from the kubelet and allows nodes to perform the following
actions:

Read operations:

* services
* endpoints
* nodes
* pods
* secrets, configmaps, persistent volume claims and persistent volumes
related to pods bound to the kubelet’s node

Write operations:

* nodes and node status (enable the `+NodeRestriction+` admission plugin
to limit a kubelet to modify its own node)
* pods and pod status (enable the `+NodeRestriction+` admission plugin
to limit a kubelet to modify pods bound to itself)
* events

Auth-related operations:

* Read/write access to the CertificateSigningRequest (CSR) API for TLS
bootstrapping
* the ability to create TokenReview and SubjectAccessReview for
delegated authentication/authorization checks

EKS uses the
https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction[node
restriction admission controller] which only allows the node to modify a
limited set of node attributes and pod objects that are bound to the
node. Nevertheless, an attacker who manages to get access to the host
will still be able to glean sensitive information about the environment
from the Kubernetes API that could allow them to move laterally within
the cluster.

== Pod Security Solutions

=== Pod Security Policy (PSP)

In the past,
https://kubernetes.io/docs/concepts/policy/pod-security-policy/[Pod
Security Policy (PSP)] resources were used to specify a set of
requirements that pods had to meet before they could be created. As of
Kubernetes version 1.21, PSP have been deprecated. They are scheduled
for removal in Kubernetes version 1.25.

!!! Attention

https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/[PSPs
are deprecated] in Kubernetes version 1.21. You will have until version
1.25 or roughly 2 years to transition to an alternative. This
https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/2579-psp-replacement/README.md#motivation[document]
explains the motivation for this deprecation.

=== Migrating to a new pod security solution

Since PSPs have been removed as of Kubernetes v1.25, cluster
administrators and operators must replace those security controls. Two
solutions can fill this need:

* Policy-as-code (PAC) solutions from the Kubernetes ecosystem
* Kubernetes
https://kubernetes.io/docs/concepts/security/pod-security-standards/[Pod
Security Standards (PSS)]

Both the PAC and PSS solutions can coexist with PSP; they can be used in
clusters before PSP is removed. This eases adoption when migrating from
PSP. Please see this
https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/[document]
when considering migrating from PSP to PSS.

Kyverno, one of the PAC solutions outlined below, has specific guidance
outlined in a
https://kyverno.io/blog/2023/05/24/podsecuritypolicy-migration-with-kyverno/[blog
post] when migrating from PSPs to its solution including analogous
policies, feature comparisons, and a migration procedure. Additional
information and guidance on migration to Kyverno with respect to Pod
Security Admission (PSA) has been published on the AWS blog
https://aws.amazon.com/blogs/containers/managing-pod-security-on-amazon-eks-with-kyverno/[here].

=== Policy-as-code (PAC)

Policy-as-code (PAC) solutions provide guardrails to guide cluster
users, and prevent unwanted behaviors, through prescribed and automated
controls. PAC uses
https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/[Kubernetes
Dynamic Admission Controllers] to intercept the Kubernetes API server
request flow, via a webhook call, and mutate and validate request
payloads, based on policies written and stored as code. Mutation and
validation happens before the API server request results in a change to
the cluster. PAC solutions use policies to match and act on API server
request payloads, based on taxonomy and values.

There are several open source PAC solutions available for Kubernetes.
These solutions are not part of the Kubernetes project; they are sourced
from the Kubernetes ecosystem. Some PAC solutions are listed below.

* https://open-policy-agent.github.io/gatekeeper/website/docs/[OPA/Gatekeeper]
* https://www.openpolicyagent.org/[Open Policy Agent (OPA)]
* https://kyverno.io/[Kyverno]
* https://www.kubewarden.io/[Kubewarden]
* https://www.jspolicy.com/[jsPolicy]

For further information about PAC solutions and how to help you select
the appropriate solution for your needs, see the links below.

* https://aws.amazon.com/blogs/containers/policy-based-countermeasures-for-kubernetes-part-1/[Policy-based
countermeasures for Kubernetes – Part 1]
* https://aws.amazon.com/blogs/containers/policy-based-countermeasures-for-kubernetes-part-2/[Policy-based
countermeasures for Kubernetes – Part 2]

=== Pod Security Standards (PSS) and Pod Security Admission (PSA)

In response to the PSP deprecation and the ongoing need to control pod
security out-of-the-box, with a built-in Kubernetes solution, the
Kubernetes
https://github.com/kubernetes/community/tree/master/sig-auth[Auth
Special Interest Group] created the
https://kubernetes.io/docs/concepts/security/pod-security-standards/[Pod
Security Standards (PSS)] and
https://kubernetes.io/docs/concepts/security/pod-security-admission/[Pod
Security Admission (PSA)]. The PSA effort includes an
https://github.com/kubernetes/pod-security-admission#pod-security-admission[admission
controller webhook project] that implements the controls defined in the
PSS. This admission controller approach resembles that used in the PAC
solutions.

According to the Kubernetes documentation, the PSS _"`define three
different policies to broadly cover the security spectrum. These
policies are cumulative and range from highly-permissive to
highly-restrictive.`"_

These policies are defined as:

* *Privileged:* Unrestricted (unsecure) policy, providing the widest
possible level of permissions. This policy allows for known privilege
escalations. It is the absence of a policy. This is good for
applications such as logging agents, CNIs, storage drivers, and other
system wide applications that need privileged access.
* *Baseline:* Minimally restrictive policy which prevents known
privilege escalations. Allows the default (minimally specified) Pod
configuration. The baseline policy prohibits use of hostNetwork,
hostPID, hostIPC, hostPath, hostPort, the inability to add Linux
capabilities, along with several other restrictions.
* *Restricted:* Heavily restricted policy, following current Pod
hardening best practices. This policy inherits from the baseline and
adds further restrictions such as the inability to run as root or a
root-group. Restricted policies may impact an application’s ability to
function. They are primarily targeted at running security critical
applications.

These policies define
https://kubernetes.io/docs/concepts/security/pod-security-standards/#profile-details[profiles
for pod execution], arranged into three levels of privileged
vs. restricted access.

To implement the controls defined by the PSS, PSA operates in three
modes:

* *enforce:* Policy violations will cause the pod to be rejected.
* *audit:* Policy violations will trigger the addition of an audit
annotation to the event recorded in the audit log, but are otherwise
allowed.
* *warn:* Policy violations will trigger a user-facing warning, but are
otherwise allowed.

These modes and the profile (restriction) levels are configured at the
Kubernetes Namespace level, using labels, as seen in the below example.

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: policy-test
  labels:
    pod-security.kubernetes.io/enforce: restricted
----

When used independently, these operational modes have different
responses that result in different user experiences. The _enforce_ mode
will prevent pods from being created if respective podSpecs violate the
configured restriction level. However, in this mode, non-pod Kubernetes
objects that create pods, such as Deployments, will not be prevented
from being applied to the cluster, even if the podSpec therein violates
the applied PSS. In this case the Deployment will be applied, while the
pod(s) will be prevented from being applied.

This is a difficult user experience, as there is no immediate indication
that the successfully applied Deployment object belies failed pod
creation. The offending podSpecs will not create pods. Inspecting the
Deployment resource with `+kubectl get deploy &lt;DEPLOYMENT_NAME&gt; -oyaml+`
will expose the message from the failed pod(s) `+.status.conditions+`
element, as seen below.

[source,yaml]
----
...
status:
  conditions:
    - lastTransitionTime: "2022-01-20T01:02:08Z"
      lastUpdateTime: "2022-01-20T01:02:08Z"
      message: 'pods "test-688f68dc87-tw587" is forbidden: violates PodSecurity "restricted:latest":
        allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false),
        unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]),
        runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true),
        seccompProfile (pod or container "test" must set securityContext.seccompProfile.type
        to "RuntimeDefault" or "Localhost")'
      reason: FailedCreate
      status: "True"
      type: ReplicaFailure
...
----

In both the _audit_ and _warn_ modes, the pod restrictions do not
prevent violating pods from being created and started. However, in these
modes audit annotations on API server audit log events and warnings to
API server clients, such as _kubectl_, are triggered, respectively, when
pods, as well as objects that create pods, contain podSpecs with
violations. A `+kubectl+` _Warning_ message is seen below.

[source,bash]
----
Warning: would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
deployment.apps/test created
----

The PSA _audit_ and _warn_ modes are useful when introducing the PSS
without negatively impacting cluster operations.

The PSA operational modes are not mutually exclusive, and can be used in
a cumulative manner. As seen below, the multiple modes can be configured
in a single namespace.

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: policy-test
  labels:
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/warn: restricted
----

In the above example, the user-friendly warnings and audit annotations
are provided when applying Deployments, while the enforce of violations
are also provided at the pod level. In fact multiple PSA labels can use
different profile levels, as seen below.

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: policy-test
  labels:
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/warn: restricted
----

In the above example, PSA is configured to allow the creation of all
pods that satisfy the _baseline_ profile level, and then _warn_ on pods
(and objects that create pods) that violate the _restricted_ profile
level. This is a useful approach to determine the possible impacts when
changing from the _baseline_ to _restricted_ profiles.

==== Existing Pods

If a namespace with existing pods is modified to use a more restrictive
PSS profile, the _audit_ and _warn_ modes will produce appropriate
messages; however, _enforce_ mode will not delete the pods. The warning
messages are seen below.

[source,bash]
----
Warning: existing pods in namespace "policy-test" violate the new PodSecurity enforce level "restricted:latest"
Warning: test-688f68dc87-htm8x: allowPrivilegeEscalation != false, unrestricted capabilities, runAsNonRoot != true, seccompProfile
namespace/policy-test configured
----

==== Exemptions

PSA uses _Exemptions_ to exclude enforcement of violations against pods
that would have otherwise been applied. These exemptions are listed
below.

* *Usernames:* requests from users with an exempt authenticated (or
impersonated) username are ignored.
* *RuntimeClassNames:* pods and workload resources specifying an exempt
runtime class name are ignored.
* *Namespaces:* pods and workload resources in an exempt namespace are
ignored.

These exemptions are applied statically in the
https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller[PSA
admission controller configuration] as part of the API server
configuration.

In the _Validating Webhook_ implementation the exemptions can be
configured within a Kubernetes
https://github.com/kubernetes/pod-security-admission/blob/master/webhook/manifests/20-configmap.yaml[ConfigMap]
resource that gets mounted as a volume into the
https://github.com/kubernetes/pod-security-admission/blob/master/webhook/manifests/50-deployment.yaml[pod-security-webhook]
container.

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: pod-security-webhook
  namespace: pod-security-webhook
data:
  podsecurityconfiguration.yaml: |
    apiVersion: pod-security.admission.config.k8s.io/v1
    kind: PodSecurityConfiguration
    defaults:
      enforce: "restricted"
      enforce-version: "latest"
      audit: "restricted"
      audit-version: "latest"
      warn: "restricted"
      warn-version: "latest"
    exemptions:
      # Array of authenticated usernames to exempt.
      usernames: []
      # Array of runtime class names to exempt.
      runtimeClasses: []
      # Array of namespaces to exempt.
      namespaces: ["kube-system","policy-test1"]
----

As seen in the above ConfigMap YAML the cluster-wide default PSS level
has been set to _restricted_ for all PSA modes, _audit_, _enforce_, and
_warn_. This affects all namespaces, except those exempted:
`+namespaces: ["kube-system","policy-test1"]+`. Additionally, in the
_ValidatingWebhookConfiguration_ resource, seen below, the
_pod-security-webhook_ namespace is also exempted from configured PSS.

[source,yaml]
----
...
webhooks:
  # Audit annotations will be prefixed with this name
  - name: "pod-security-webhook.kubernetes.io"
    # Fail-closed admission webhooks can present operational challenges.
    # You may want to consider using a failure policy of Ignore, but should
    # consider the security tradeoffs.
    failurePolicy: Fail
    namespaceSelector:
      # Exempt the webhook itself to avoid a circular dependency.
      matchExpressions:
        - key: kubernetes.io/metadata.name
          operator: NotIn
          values: ["pod-security-webhook"]
...
----

!!! Attention

Pod Security Admissions graduated to stable in Kubernetes v1.25. If you
wanted to use the Pod Security Admission feature prior to it being
enabled by default, you needed to install the dynamic admission
controller (mutating webhook). The instructions for installing and
configuring the webhook can be found
https://github.com/kubernetes/pod-security-admission/tree/master/webhook[here].

=== Choosing between policy-as-code and Pod Security Standards

The Pod Security Standards (PSS) were developed to replace the Pod
Security Policy (PSP), by providing a solution that was built-in to
Kubernetes and did not require solutions from the Kubernetes ecosystem.
That being said, policy-as-code (PAC) solutions are considerably more
flexible.

The following list of Pros and Cons is designed help you make a more
informed decision about your pod security solution.

==== Policy-as-code (as compared to Pod Security Standards)

Pros:

* More flexible and more granular (down to attributes of resources if
need be)
* Not just focused on pods, can be used against different resources and
actions
* Not just applied at the namespace level
* More mature than the Pod Security Standards
* Decisions can be based on anything in the API server request payload,
as well as existing cluster resources and external data (solution
dependent)
* Supports mutating API server requests before validation (solution
dependent)
* Can generate complementary policies and Kubernetes resources (solution
dependent - From pod policies, Kyverno can
https://kyverno.io/docs/writing-policies/autogen/[auto-gen] policies for
higher-level controllers, such as Deployments. Kyverno can also generate
additional Kubernetes resources _"`when a new resource is created or
when the source is updated`"_ by using
https://kyverno.io/docs/writing-policies/generate/[Generate Rules].)
* Can be used to shift left, into CICD pipelines, before making calls to
the Kubernetes API server (solution dependent)
* Can be used to implement behaviors that are not necessarily security
related, such as best practices, organizational standards, etc.
* Can be used in non-Kubernetes use cases (solution dependent)
* Because of flexibility, the user experience can be tuned to users’
needs

Cons:

* Not built into Kubernetes
* More complex to learn, configure, and support
* Policy authoring may require new skills/languages/capabilities

==== Pod Security Admission (as compared to policy-as-code)

Pros:

* Built into Kubernetes
* Simpler to configure
* No new languages to use or policies to author
* If the cluster default admission level is configured to _privileged_,
namespace labels can be used to opt namespaces into the pod security
profiles.

Cons:

* Not as flexible or granular as policy-as-code
* Only 3 levels of restrictions
* Primarily focused on pods

==== Summary

If you currently do not have a pod security solution, beyond PSP, and
your required pod security posture fits the model defined in the Pod
Security Standards (PSS), then an easier path may be to adopt the PSS,
in lieu of a policy-as-code solution. However, if your pod security
posture does not fit the PSS model, or you envision adding additional
controls, beyond that defined by PSS, then a policy-as-code solution
would seem a better fit.

== Recommendations

=== Use multiple Pod Security Admission (PSA) modes for a better user experience

As mentioned earlier, PSA _enforce_ mode prevents pods with PSS
violations from being applied, but does not stop higher-level
controllers, such as Deployments. In fact, the Deployment will be
applied successfully without any indication that the pods failed to be
applied. While you can use _kubectl_ to inspect the Deployment object,
and discover the failed pods message from the PSA, the user experience
could be better. To make the user experience better, multiple PSA modes
(audit, enforce, warn) should be used.

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: policy-test
  labels:
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/warn: restricted
----

In the above example, with _enforce_ mode defined, when a Deployment
manifest with PSS violations in the respective podSpec is attempted to
be applied to the Kubernetes API server, the Deployment will be
successfully applied, but the pods will not. And, since the _audit_ and
_warn_ modes are also enabled, the API server client will receive a
warning message and the API server audit log event will be annotated
with a message as well.

=== Restrict the containers that can run as privileged

As mentioned, containers that run as privileged inherit all of the Linux
capabilities assigned to root on the host. Seldom do containers need
these types of privileges to function properly. There are multiple
methods that can be used to restrict the permissions and capabilities of
containers.

!!! Attention

Fargate is a launch type that enables you to run "`serverless`"
container(s) where the containers of a pod are run on infrastructure
that AWS manages. With Fargate, you cannot run a privileged container or
configure your pod to use hostNetwork or hostPort.

=== Do not run processes in containers as root

All containers run as root by default. This could be problematic if an
attacker is able to exploit a vulnerability in the application and get
shell access to the running container. You can mitigate this risk a
variety of ways. First, by removing the shell from the container image.
Second, adding the USER directive to your Dockerfile or running the
containers in the pod as a non-root user. The Kubernetes podSpec
includes a set of fields, under `+spec.securityContext+`, that let you
specify the user and/or group under which to run your application. These
fields are `+runAsUser+` and `+runAsGroup+` respectively.

To enforce the use of the `+spec.securityContext+`, and its associated
elements, within the Kubernetes podSpec, policy-as-code or Pod Security
Standards can be added to clusters. These solutions allow you to write
and/or use policies or profiles that can validate inbound Kubernetes API
server request payloads, before they are persisted into etcd.
Furthermore, policy-as-code solutions can mutate inbound requests, and
in some cases, generate new requests.

=== Never run Docker in Docker or mount the socket in the container

While this conveniently lets you to build/run images in Docker
containers, you’re basically relinquishing complete control of the node
to the process running in the container. If you need to build container
images on Kubernetes use
https://github.com/GoogleContainerTools/kaniko[Kaniko],
https://github.com/containers/buildah[buildah], or a build service like
https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html[CodeBuild]
instead.

!!! Tip

Kubernetes clusters used for CICD processing, such as building container
images, should be isolated from clusters running more generalized
workloads.

=== Restrict the use of hostPath or if hostPath is necessary restrict which prefixes can be used and configure the volume as read-only

`+hostPath+` is a volume that mounts a directory from the host directly
to the container. Rarely will pods need this type of access, but if they
do, you need to be aware of the risks. By default pods that run as root
will have write access to the file system exposed by hostPath. This
could allow an attacker to modify the kubelet settings, create symbolic
links to directories or files not directly exposed by the hostPath,
e.g. /etc/shadow, install ssh keys, read secrets mounted to the host,
and other malicious things. To mitigate the risks from hostPath,
configure the `+spec.containers.volumeMounts+` as `+readOnly+`, for
example:

[source,yaml]
----
volumeMounts:
- name: hostPath-volume
    readOnly: true
    mountPath: /host-path
----

You should also use policy-as-code solutions to restrict the directories
that can be used by `+hostPath+` volumes, or prevent `+hostPath+` usage
altogether. You can use the Pod Security Standards _Baseline_ or
_Restricted_ policies to prevent the use of `+hostPath+`.

For further information about the dangers of privileged escalation, read
Seth Art’s blog
https://labs.bishopfox.com/tech-blog/bad-pods-kubernetes-pod-privilege-escalation[Bad
Pods: Kubernetes Pod Privilege Escalation].

=== Set requests and limits for each container to avoid resource contention and DoS attacks

A pod without requests or limits can theoretically consume all of the
resources available on a host. As additional pods are scheduled onto a
node, the node may experience CPU or memory pressure which can cause the
Kubelet to terminate or evict pods from the node. While you can’t
prevent this from happening all together, setting requests and limits
will help minimize resource contention and mitigate the risk from poorly
written applications that consume an excessive amount of resources.

The `+podSpec+` allows you to specify requests and limits for CPU and
memory. CPU is considered a compressible resource because it can be
oversubscribed. Memory is incompressible, i.e. it cannot be shared among
multiple containers.

When you specify _requests_ for CPU or memory, you’re essentially
designating the amount of _memory_ that containers are guaranteed to
get. Kubernetes aggregates the requests of all the containers in a pod
to determine which node to schedule the pod onto. If a container exceeds
the requested amount of memory it may be subject to termination if
there’s memory pressure on the node.

_Limits_ are the maximum amount of CPU and memory resources that a
container is allowed to consume and directly corresponds to the
`+memory.limit_in_bytes+` value of the cgroup created for the container.
A container that exceeds the memory limit will be OOM killed. If a
container exceeds its CPU limit, it will be throttled.

!!! Tip

When using container `+resources.limits+` it is strongly recommended
that container resource usage (a.k.a. Resource Footprints) be
data-driven and accurate, based on load testing. Absent an accurate and
trusted resource footprint, container `+resources.limits+` can be
padded. For example, `+resources.limits.memory+` could be padded 20-30%
higher than observable maximums, to account for potential memory
resource limit inaccuracies.

Kubernetes uses three Quality of Service (QoS) classes to prioritize the
workloads running on a node. These include:

* guaranteed
* burstable
* best-effort

If limits and requests are not set, the pod is configured as
_best-effort_ (lowest priority). Best-effort pods are the first to get
killed when there is insufficient memory. If limits are set on _all_
containers within the pod, or if the requests and limits are set to the
same values and not equal to 0, the pod is configured as _guaranteed_
(highest priority). Guaranteed pods will not be killed unless they
exceed their configured memory limits. If the limits and requests are
configured with different values and not equal to 0, or one container
within the pod sets limits and the others don’t or have limits set for
different resources, the pods are configured as _burstable_ (medium
priority). These pods have some resource guarantees, but can be killed
once they exceed their requested memory.

!!! Attention

Requests don’t affect the `+memory_limit_in_bytes+` value of the
container’s cgroup; the cgroup limit is set to the amount of memory
available on the host. Nevertheless, setting the requests value too low
could cause the pod to be targeted for termination by the kubelet if the
node undergoes memory pressure.

[width="100%",cols="&lt;25%,&lt;25%,&lt;25%,&lt;25%",options="header",]
|===
|Class |Priority |Condition |Kill Condition
|Guaranteed |highest |limit = request != 0 |Only exceed memory limits

|Burstable |medium |limit != request != 0 |Can be killed if exceed
request memory

|Best-Effort |lowest |limit &amp; request Not Set |First to get killed when
there’s insufficient memory
|===

For additional information about resource QoS, please refer to the
https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/[Kubernetes
documentation].

You can force the use of requests and limits by setting a
https://kubernetes.io/docs/concepts/policy/resource-quotas/[resource
quota] on a namespace or by creating a
https://kubernetes.io/docs/concepts/policy/limit-range/[limit range]. A
resource quota allows you to specify the total amount of resources,
e.g. CPU and RAM, allocated to a namespace. When it’s applied to a
namespace, it forces you to specify requests and limits for all
containers deployed into that namespace. By contrast, limit ranges give
you more granular control of the allocation of resources. With limit
ranges you can min/max for CPU and memory resources per pod or per
container within a namespace. You can also use them to set default
request/limit values if none are provided.

Policy-as-code solutions can be used enforce requests and limits. or to
even create the resource quotas and limit ranges when namespaces are
created.

=== Do not allow privileged escalation

Privileged escalation allows a process to change the security context
under which its running. Sudo is a good example of this as are binaries
with the SUID or SGID bit. Privileged escalation is basically a way for
users to execute a file with the permissions of another user or group.
You can prevent a container from using privileged escalation by
implementing a policy-as-code mutating policy that sets
`+allowPrivilegeEscalation+` to `+false+` or by setting
`+securityContext.allowPrivilegeEscalation+` in the `+podSpec+`.
Policy-as-code policies can also be used to prevent API server requests
from succeeding if incorrect settings are detected. Pod Security
Standards can also be used to prevent pods from using privilege
escalation.

=== Disable ServiceAccount token mounts

For pods that do not need to access the Kubernetes API, you can disable
the automatic mounting of a ServiceAccount token on a pod spec, or for
all pods that use a particular ServiceAccount.

!!! Attention

Disabling ServiceAccount mounting does not prevent a pod from having
network access to the Kubernetes API. To prevent a pod from having any
network access to the Kubernetes API, you will need to modify the
https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html[EKS
cluster endpoint access] and use a
link:../network/#network-policy[NetworkPolicy] to block pod access.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-no-automount
spec:
  automountServiceAccountToken: false
----

[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-no-automount
automountServiceAccountToken: false
----

=== Disable service discovery

For pods that do not need to lookup or call in-cluster services, you can
reduce the amount of information given to a pod. You can set the Pod’s
DNS policy to not use CoreDNS, and not expose services in the pod’s
namespace as environment variables. See the
https://kubernetes.io/docs/concepts/services-networking/service/#environment-variables[Kubernetes
docs on environment variables] for more information on service links.
The default value for a pod’s DNS policy is "`ClusterFirst`" which uses
in-cluster DNS, while the non-default value "`Default`" uses the
underlying node’s DNS resolution. See the
https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy[Kubernetes
docs on Pod DNS policy] for more information.

!!! Attention

Disabling service links and changing the pod’s DNS policy does not
prevent a pod from having network access to the in-cluster DNS service.
An attacker can still enumerate services in a cluster by reaching the
in-cluster DNS service. (ex:
`+dig SRV *.*.svc.cluster.local @$CLUSTER_DNS_IP+`) To prevent
in-cluster service discovery, use a
link:../network/#network-policy[NetworkPolicy] to block pod access

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-no-service-info
spec:
    dnsPolicy: Default # "Default" is not the true default value
    enableServiceLinks: false
----

=== Configure your images with read-only root file system

Configuring your images with a read-only root file system prevents an
attacker from overwriting a binary on the file system that your
application uses. If your application has to write to the file system,
consider writing to a temporary directory or attach and mount a volume.
You can enforce this by setting the pod’s SecurityContext as follows:

[source,yaml]
----
...
securityContext:
  readOnlyRootFilesystem: true
...
----

Policy-as-code and Pod Security Standards can be used to enforce this
behavior.

!!! Info

As per https://kubernetes.io/docs/concepts/windows/intro/[Windows
containers in Kubernetes] `+securityContext.readOnlyRootFilesystem+`
cannot be set to `+true+` for a container running on Windows as write
access is required for registry and system processes to run inside the
container.

== Tools and resources

* https://catalog.workshops.aws/eks-security-immersionday/en-US/3-pod-security[Amazon
EKS Security Immersion Workshop - Pod Security]
* https://github.com/open-policy-agent/gatekeeper-library[open-policy-agent/gatekeeper-library:
The OPA Gatekeeper policy library] a library of OPA/Gatekeeper policies
that you can use as a substitute for PSPs.
* https://kyverno.io/policies/[Kyverno Policy Library]
* A collection of common OPA and Kyverno
https://github.com/aws/aws-eks-best-practices/tree/master/policies[policies]
for EKS.
* https://aws.amazon.com/blogs/containers/policy-based-countermeasures-for-kubernetes-part-1/[Policy
based countermeasures: part 1]
* https://aws.amazon.com/blogs/containers/policy-based-countermeasures-for-kubernetes-part-2/[Policy
based countermeasures: part 2]
* https://appvia.github.io/psp-migration/[Pod Security Policy Migrator]
a tool that converts PSPs to OPA/Gatekeeper, KubeWarden, or Kyverno
policies
* https://www.suse.com/neuvector/[NeuVector by SUSE] open source,
zero-trust container security platform, provides process and filesystem
policies as well as admission control rules.

:leveloffset: 1
:leveloffset: +1

= Tenant Isolation

When we think of multi-tenancy, we often want to isolate a user or
application from other users or applications running on a shared
infrastructure.

Kubernetes is a _single tenant orchestrator_, i.e. a single instance of
the control plane is shared among all the tenants within a cluster.
There are, however, various Kubernetes objects that you can use to
create the semblance of multi-tenancy. For example, Namespaces and
Role-based access controls (RBAC) can be implemented to logically
isolate tenants from each other. Similarly, Quotas and Limit Ranges can
be used to control the amount of cluster resources each tenant can
consume. Nevertheless, the cluster is the only construct that provides a
strong security boundary. This is because an attacker that manages to
gain access to a host within the cluster can retrieve _all_ Secrets,
ConfigMaps, and Volumes, mounted on that host. They could also
impersonate the Kubelet which would allow them to manipulate the
attributes of the node and/or move laterally within the cluster.

The following sections will explain how to implement tenant isolation
while mitigating the risks of using a single tenant orchestrator like
Kubernetes.

== Soft multi-tenancy

With soft multi-tenancy, you use native Kubernetes constructs,
e.g. namespaces, roles and role bindings, and network policies, to
create logical separation between tenants. RBAC, for example, can
prevent tenants from accessing or manipulate each other’s resources.
Quotas and limit ranges control the amount of cluster resources each
tenant can consume while network policies can help prevent applications
deployed into different namespaces from communicating with each other.

None of these controls, however, prevent pods from different tenants
from sharing a node. If stronger isolation is required, you can use a
node selector, anti-affinity rules, and/or taints and tolerations to
force pods from different tenants to be scheduled onto separate nodes;
often referred to as _sole tenant nodes_. This could get rather
complicated, and cost prohibitive, in an environment with many tenants.

!!! attention Soft multi-tenancy implemented with Namespaces does not
allow you to provide tenants with a filtered list of Namespaces because
Namespaces are a globally scoped Type. If a tenant has the ability to
view a particular Namespace, it can view all Namespaces within the
cluster.

!!! warning With soft-multi-tenancy, tenants retain the ability to query
CoreDNS for all services that run within the cluster by default. An
attacker could exploit this by running dig SRV `+*.*.svc.cluster.local+`
from any pod in the cluster. If you need to restrict access to DNS
records of services that run within your clusters, consider using the
Firewall or Policy plugins for CoreDNS. For additional information, see
https://github.com/coredns/policy#kubernetes-metadata-multi-tenancy-policy.

https://github.com/kiosk-sh/kiosk[Kiosk] is an open source project that
can aid in the implementation of soft multi-tenancy. It is implemented
as a series of CRDs and controllers that provide the following
capabilities:

* *Accounts &amp; Account Users* to separate tenants in a shared Kubernetes
cluster
* *Self-Service Namespace Provisioning* for account users
* *Account Limits* to ensure quality of service and fairness when
sharing a cluster
* *Namespace Templates* for secure tenant isolation and self-service
namespace initialization

https://loft.sh[Loft] is a commercial offering from the maintainers of
Kiosk and https://github.com/devspace-cloud/devspace[DevSpace] that adds
the following capabilities:

* *Multi-cluster access* for granting access to spaces in different
clusters
* *Sleep mode* scales down deployments in a space during periods of
inactivity
* *Single sign-on* with OIDC authentication providers like GitHub

There are three primary use cases that can be addressed by soft
multi-tenancy.

=== Enterprise Setting

The first is in an Enterprise setting where the "`tenants`" are
semi-trusted in that they are employees, contractors, or are otherwise
authorized by the organization. Each tenant will typically align to an
administrative division such as a department or team.

In this type of setting, a cluster administrator will usually be
responsible for creating namespaces and managing policies. They may also
implement a delegated administration model where certain individuals are
given oversight of a namespace, allowing them to perform CRUD operations
for non-policy related objects like deployments, services, pods, jobs,
etc.

The isolation provided by a container runtime may be acceptable within
this setting or it may need to be augmented with additional controls for
pod security. It may also be necessary to restrict communication between
services in different namespaces if stricter isolation is required.

=== Kubernetes as a Service

By contrast, soft multi-tenancy can be used in settings where you want
to offer Kubernetes as a service (KaaS). With KaaS, your application is
hosted in a shared cluster along with a collection of controllers and
CRDs that provide a set of PaaS services. Tenants interact directly with
the Kubernetes API server and are permitted to perform CRUD operations
on non-policy objects. There is also an element of self-service in that
tenants may be allowed to create and manage their own namespaces. In
this type of environment, tenants are assumed to be running untrusted
code.

To isolate tenants in this type of environment, you will likely need to
implement strict network policies as well as _pod sandboxing_.
Sandboxing is where you run the containers of a pod inside a micro VM
like Firecracker or in a user-space kernel. Today, you can create
sandboxed pods with EKS Fargate.

=== Software as a Service (SaaS)

The final use case for soft multi-tenancy is in a Software-as-a-Service
(SaaS) setting. In this environment, each tenant is associated with a
particular _instance_ of an application that’s running within the
cluster. Each instance often has its own data and uses separate access
controls that are usually independent of Kubernetes RBAC.

Unlike the other use cases, the tenant in a SaaS setting does not
directly interface with the Kubernetes API. Instead, the SaaS
application is responsible for interfacing with the Kubernetes API to
create the necessary objects to support each tenant.

== Kubernetes Constructs

In each of these instances the following constructs are used to isolate
tenants from each other:

=== Namespaces

Namespaces are fundamental to implementing soft multi-tenancy. They
allow you to divide the cluster into logical partitions. Quotas, network
policies, service accounts, and other objects needed to implement
multi-tenancy are scoped to a namespace.

=== Network policies

By default, all pods in a Kubernetes cluster are allowed to communicate
with each other. This behavior can be altered using network policies.

Network policies restrict communication between pods using labels or IP
address ranges. In a multi-tenant environment where strict network
isolation between tenants is required, we recommend starting with a
default rule that denies communication between pods, and another rule
that allows all pods to query the DNS server for name resolution. With
that in place, you can begin adding more permissive rules that allow for
communication within a namespace. This can be further refined as
required.

!!! note Amazon
https://aws.amazon.com/blogs/containers/amazon-vpc-cni-now-supports-kubernetes-network-policies/[VPC
CNI now supports Kubernetes Network Policies] to create policies that
can isolate sensitive workloads and protect them from unauthorized
access when running Kubernetes on AWS. This means that you can use all
the capabilities of the Network Policy API within your Amazon EKS
cluster. This level of granular control enables you to implement the
principle of least privilege, which ensures that only authorized pods
are allowed to communicate with each other.

!!! attention Network policies are necessary but not sufficient. The
enforcement of network policies requires a policy engine such as Calico
or Cilium.

=== Role-based access control (RBAC)

Roles and role bindings are the Kubernetes objects used to enforce
role-based access control (RBAC) in Kubernetes. *Roles* contain lists of
actions that can be performed against objects in your cluster. *Role
bindings* specify the individuals or groups to whom the roles apply. In
the enterprise and KaaS settings, RBAC can be used to permit
administration of objects by selected groups or individuals.

=== Quotas

Quotas are used to define limits on workloads hosted in your cluster.
With quotas, you can specify the maximum amount of CPU and memory that a
pod can consume, or you can limit the number of resources that can be
allocated in a cluster or namespace. *Limit ranges* allow you to declare
minimum, maximum, and default values for each limit.

Overcommitting resources in a shared cluster is often beneficial because
it allows you maximize your resources. However, unbounded access to a
cluster can cause resource starvation, which can lead to performance
degradation and loss of application availability. If a pod’s requests
are set too low and the actual resource utilization exceeds the capacity
of the node, the node will begin to experience CPU or memory pressure.
When this happens, pods may be restarted and/or evicted from the node.

To prevent this from happening, you should plan to impose quotas on
namespaces in a multi-tenant environment to force tenants to specify
requests and limits when scheduling their pods on the cluster. It will
also mitigate a potential denial of service by constraining the amount
of resources a pod can consume.

You can also use quotas to apportion the cluster’s resources to align
with a tenant’s spend. This is particularly useful in the KaaS scenario.

=== Pod priority and preemption

Pod priority and preemption can be useful when you want to provide more
importance to a Pod relative to other Pods. For example, with pod
priority you can configure pods from customer A to run at a higher
priority than customer B. When there’s insufficient capacity available,
the scheduler will evict the lower-priority pods from customer B to
accommodate the higher-priority pods from customer A. This can be
especially handy in a SaaS environment where customers willing to pay a
premium receive a higher priority.

!!! attention Pods priority can have an undesired effect on other Pods
with lower priority. For example, although the victim pods are
terminated gracefully but the PodDisruptionBudget is not guaranteed,
which could break a application with lower priority that relies on a
quorum of Pods, see
https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#limitations-of-preemption[Limitations
of preemption].

== Mitigating controls

Your chief concern as an administrator of a multi-tenant environment is
preventing an attacker from gaining access to the underlying host. The
following controls should be considered to mitigate this risk:

=== Sandboxed execution environments for containers

Sandboxing is a technique by which each container is run in its own
isolated virtual machine. Technologies that perform pod sandboxing
include https://firecracker-microvm.github.io/[Firecracker] and Weave’s
https://www.weave.works/blog/firekube-fast-and-secure-kubernetes-clusters-using-weave-ignite[Firekube].

For additional information about the effort to make Firecracker a
supported runtime for EKS, see
https://threadreaderapp.com/thread/1238496944684597248.html.

=== Open Policy Agent (OPA) &amp; Gatekeeper

https://github.com/open-policy-agent/gatekeeper[Gatekeeper] is a
Kubernetes admission controller that enforces policies created with
https://www.openpolicyagent.org/[OPA]. With OPA you can create a policy
that runs pods from tenants on separate instances or at a higher
priority than other tenants. A collection of common OPA policies can be
found in the GitHub
https://github.com/aws/aws-eks-best-practices/tree/master/policies/opa[repository]
for this project.

There is also an experimental https://github.com/coredns/coredns-opa[OPA
plugin for CoreDNS] that allows you to use OPA to filter/control the
records returned by CoreDNS.

=== Kyverno

https://kyverno.io[Kyverno] is a Kubernetes native policy engine that
can validate, mutate, and generate configurations with policies as
Kubernetes resources. Kyverno uses Kustomize-style overlays for
validation, supports JSON Patch and strategic merge patch for mutation,
and can clone resources across namespaces based on flexible triggers.

You can use Kyverno to isolate namespaces, enforce pod security and
other best practices, and generate default configurations such as
network policies. Several examples are included in the GitHub
https://github.com/aws/aws-eks-best-practices/tree/master/policies/kyverno[repository]
for this project. Many others are included in the
https://kyverno.io/policies/[policy library] on the Kyverno website.

=== Isolating tenant workloads to specific nodes

Restricting tenant workloads to run on specific nodes can be used to
increase isolation in the soft multi-tenancy model. With this approach,
tenant-specific workloads are only run on nodes provisioned for the
respective tenants. To achieve this isolation, native Kubernetes
properties (node affinity, and taints and tolerations) are used to
target specific nodes for pod scheduling, and prevent pods, from other
tenants, from being scheduled on the tenant-specific nodes.

==== Part 1 - Node affinity

Kubernetes
https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity[node
affinity] is used to target nodes for scheduling, based on node
https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/[labels].
With node affinity rules, the pods are attracted to specific nodes that
match the selector terms. In the below pod specification, the
`+requiredDuringSchedulingIgnoredDuringExecution+` node affinity is
applied to the respective pod. The result is that the pod will target
nodes that are labeled with the following key/value:
`+node-restriction.kubernetes.io/tenant: tenants-x+`.

[source,yaml]
----
...
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-restriction.kubernetes.io/tenant
            operator: In
            values:
            - tenants-x
...
----

With this node affinity, the label is required during scheduling, but
not during execution; if the underlying nodes’ labels change, the pods
will not be evicted due solely to that label change. However, future
scheduling could be impacted.

!!! Warning The label prefix of `+node-restriction.kubernetes.io/+` has
special meaning in Kubernetes.
https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction[NodeRestriction]
which is enabled for EKS clusters prevents `+kubelet+` from
adding/removing/updating labels with this prefix. Attackers aren’t able
to use the `+kubelet+`’s credentials to update the node object or modify
the system setup to pass these labels into `+kubelet+` as `+kubelet+`
isn’t allowed to modify these labels. If this prefix is used for all pod
to node scheduling, it prevents scenarios where an attacker may want to
attract a different set of workloads to a node by modifying the node
labels.

!!! Info Instead of node affinity, we could have used the
https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector[node
selector]. However, node affinity is more expressive and allows for more
conditions to be considered during pod scheduling. For additional
information about the differences and more advanced scheduling choices,
please see this CNCF blog post on
https://www.cncf.io/blog/2021/07/27/advanced-kubernetes-pod-to-node-scheduling/[Advanced
Kubernetes pod to node scheduling].

==== Part 2 - Taints and tolerations

Attracting pods to nodes is just the first part of this three-part
approach. For this approach to work, we must repel pods from scheduling
onto nodes for which the pods are not authorized. To repel unwanted or
unauthorized pods, Kubernetes uses node
https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/[taints].
Taints are used to place conditions on nodes that prevent pods from
being scheduled. The below taint uses a key-value pair of
`+tenant: tenants-x+`.

[source,yaml]
----
...
    taints:
      - key: tenant
        value: tenants-x
        effect: NoSchedule
...
----

Given the above node `+taint+`, only pods that _tolerate_ the taint will
be allowed to be scheduled on the node. To allow authorized pods to be
scheduled onto the node, the respective pod specifications must include
a `+toleration+` to the taint, as seen below.

[source,yaml]
----
...
  tolerations:
  - effect: NoSchedule
    key: tenant
    operator: Equal
    value: tenants-x
...
----

Pods with the above `+toleration+` will not be stopped from scheduling
on the node, at least not because of that specific taint. Taints are
also used by Kubernetes to temporarily stop pod scheduling during
certain conditions, like node resource pressure. With node affinity, and
taints and tolerations, we can effectively attract the desired pods to
specific nodes and repel unwanted pods.

!!! attention Certain Kubernetes pods are required to run on all nodes.
Examples of these pods are those started by the
https://github.com/containernetworking/cni[Container Network Interface
(CNI)] and
https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/[kube-proxy]
https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/[daemonsets].
To that end, the specifications for these pods contain very permissive
tolerations, to tolerate different taints. Care should be taken to not
change these tolerations. Changing these tolerations could result in
incorrect cluster operation. Additionally, policy-management tools, such
as https://github.com/open-policy-agent/gatekeeper[OPA/Gatekeeper] and
https://kyverno.io/[Kyverno] can be used to write validating policies
that prevent unauthorized pods from using these permissive tolerations.

==== Part 3 - Policy-based management for node selection

There are several tools that can be used to help manage the node
affinity and tolerations of pod specifications, including enforcement of
rules in CICD pipelines. However, enforcement of isolation should also
be done at the Kubernetes cluster level. For this purpose,
policy-management tools can be used to _mutate_ inbound Kubernetes API
server requests, based on request payloads, to apply the respective node
affinity rules and tolerations mentioned above.

For example, pods destined for the _tenants-x_ namespace can be
_stamped_ with the correct node affinity and toleration to permit
scheduling on the _tenants-x_ nodes. Utilizing policy-management tools
configured using the Kubernetes
https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook[Mutating
Admission Webhook], policies can be used to mutate the inbound pod
specifications. The mutations add the needed elements to allow desired
scheduling. An example OPA/Gatekeeper policy that adds a node affinity
is seen below.

[source,yaml]
----
apiVersion: mutations.gatekeeper.sh/v1alpha1
kind: Assign
metadata:
  name: mutator-add-nodeaffinity-pod
  annotations:
    aws-eks-best-practices/description: &gt;-
      Adds Node affinity - https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
spec:
  applyTo:
  - groups: [""]
    kinds: ["Pod"]
    versions: ["v1"]
  match:
    namespaces: ["tenants-x"]
  location: "spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms"
  parameters:
    assign:
      value:
        - matchExpressions:
          - key: "tenant"
            operator: In
            values:
            - "tenants-x"
----

The above policy is applied to a Kubernetes API server request, to apply
a pod to the _tenants-x_ namespace. The policy adds the
`+requiredDuringSchedulingIgnoredDuringExecution+` node affinity rule,
so that pods are attracted to nodes with the `+tenant: tenants-x+`
label.

A second policy, seen below, adds the toleration to the same pod
specification, using the same matching criteria of target namespace and
groups, kinds, and versions.

[source,yaml]
----
apiVersion: mutations.gatekeeper.sh/v1alpha1
kind: Assign
metadata:
  name: mutator-add-toleration-pod
  annotations:
    aws-eks-best-practices/description: &gt;-
      Adds toleration - https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
spec:
  applyTo:
  - groups: [""]
    kinds: ["Pod"]
    versions: ["v1"]
  match:
    namespaces: ["tenants-x"]
  location: "spec.tolerations"
  parameters:
    assign:
      value:
      - key: "tenant"
        operator: "Equal"
        value: "tenants-x"
        effect: "NoSchedule"
----

The above policies are specific to pods; this is due to the paths to the
mutated elements in the policies’ `+location+` elements. Additional
policies could be written to handle resources that create pods, like
Deployment and Job resources. The listed policies and other examples can
been seen in the companion
https://github.com/aws/aws-eks-best-practices/tree/master/policies/opa/gatekeeper/node-selector[GitHub
project] for this guide.

The result of these two mutations is that pods are attracted to the
desired node, while at the same time, not repelled by the specific node
taint. To verify this, we can see the snippets of output from two
`+kubectl+` calls to get the nodes labeled with `+tenant=tenants-x+`,
and get the pods in the `+tenants-x+` namespace.

[source,bash]
----
kubectl get nodes -l tenant=tenants-x
NAME
ip-10-0-11-255...
ip-10-0-28-81...
ip-10-0-43-107...

kubectl -n tenants-x get pods -owide
NAME                                  READY   STATUS    RESTARTS   AGE   IP            NODE
tenant-test-deploy-58b895ff87-2q7xw   1/1     Running   0          13s   10.0.42.143   ip-10-0-43-107...
tenant-test-deploy-58b895ff87-9b6hg   1/1     Running   0          13s   10.0.18.145   ip-10-0-28-81...
tenant-test-deploy-58b895ff87-nxvw5   1/1     Running   0          13s   10.0.30.117   ip-10-0-28-81...
tenant-test-deploy-58b895ff87-vw796   1/1     Running   0          13s   10.0.3.113    ip-10-0-11-255...
tenant-test-pod                       1/1     Running   0          13s   10.0.35.83    ip-10-0-43-107...
----

As we can see from the above outputs, all the pods are scheduled on the
nodes labeled with `+tenant=tenants-x+`. Simply put, the pods will only
run on the desired nodes, and the other pods (without the required
affinity and tolerations) will not. The tenant workloads are effectively
isolated.

An example mutated pod specification is seen below.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: tenant-test-pod
  namespace: tenants-x
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: tenant
            operator: In
            values:
            - tenants-x
...
  tolerations:
  - effect: NoSchedule
    key: tenant
    operator: Equal
    value: tenants-x
...
----

!!! attention Policy-management tools that are integrated to the
Kubernetes API server request flow, using mutating and validating
admission webhooks, are designed to respond to the API server’s request
within a specified timeframe. This is usually 3 seconds or less. If the
webhook call fails to return a response within the configured time, the
mutation and/or validation of the inbound API sever request may or may
not occur. This behavior is based on whether the admission webhook
configurations are set to
https://open-policy-agent.github.io/gatekeeper/website/docs/#admission-webhook-fail-open-by-default[Fail
Open or Fail Close].

In the above examples, we used policies written for OPA/Gatekeeper.
However, there are other policy management tools that handle our
node-selection use case as well. For example, this
https://kyverno.io/policies/other/add_node_affinity/add_node_affinity/[Kyverno
policy] could be used to handle the node affinity mutation.

!!! tip If operating correctly, mutating policies will effect the
desired changes to inbound API server request payloads. However,
validating policies should also be included to verify that the desired
changes occur, before changes are allowed to persist. This is especially
important when using these policies for tenant-to-node isolation. It is
also a good idea to include _Audit_ policies to routinely check your
cluster for unwanted configurations.

=== References

* https://github.com/cruise-automation/k-rail[k-rail] Designed to help
you secure a multi-tenant environment through the enforcement of certain
policies.
* https://d1.awsstatic.com/whitepapers/security-practices-for-multi-tenant-saas-apps-using-eks.pdf[Security
Practices for MultiTenant SaaS Applications using Amazon EKS]

== Hard multi-tenancy

Hard multi-tenancy can be implemented by provisioning separate clusters
for each tenant. While this provides very strong isolation between
tenants, it has several drawbacks.

First, when you have many tenants, this approach can quickly become
expensive. Not only will you have to pay for the control plane costs for
each cluster, you will not be able to share compute resources between
clusters. This will eventually cause fragmentation where a subset of
your clusters are underutilized while others are overutilized.

Second, you will likely need to buy or build special tooling to manage
all of these clusters. In time, managing hundreds or thousands of
clusters may simply become too unwieldy.

Finally, creating a cluster per tenant will be slow relative to a
creating a namespace. Nevertheless, a hard-tenancy approach may be
necessary in highly-regulated industries or in SaaS environments where
strong isolation is required.

== Future directions

The Kubernetes community has recognized the current shortcomings of soft
multi-tenancy and the challenges with hard multi-tenancy. The
https://github.com/kubernetes-sigs/multi-tenancy[Multi-Tenancy Special
Interest Group (SIG)] is attempting to address these shortcomings
through several incubation projects, including Hierarchical Namespace
Controller (HNC) and Virtual Cluster.

The HNC proposal (KEP) describes a way to create parent-child
relationships between namespaces with [policy] object inheritance along
with an ability for tenant administrators to create sub-namespaces.

The Virtual Cluster proposal describes a mechanism for creating separate
instances of the control plane services, including the API server, the
controller manager, and scheduler, for each tenant within the cluster
(also known as "`Kubernetes on Kubernetes`").

The
https://github.com/kubernetes-sigs/multi-tenancy/blob/master/benchmarks/README.md[Multi-Tenancy
Benchmarks] proposal provides guidelines for sharing clusters using
namespaces for isolation and segmentation, and a command line tool
https://github.com/kubernetes-sigs/multi-tenancy/blob/master/benchmarks/kubectl-mtb/README.md[kubectl-mtb]
to validate conformance to the guidelines.

== Multi-cluster management tools and resources

* https://banzaicloud.com/[Banzai Cloud]
* https://d2iq.com/solutions/ksphere/kommander[Kommander]
* https://github.com/lensapp/lens[Lens]
* https://nirmata.com[Nirmata]
* https://rafay.co/[Rafay]
* https://rancher.com/products/rancher/[Rancher]
* https://www.weave.works/oss/flux/[Weave Flux]

:leveloffset: 1
:leveloffset: +1

= Auditing and logging

Collecting and analyzing [audit] logs is useful for a variety of
different reasons. Logs can help with root cause analysis and
attribution, i.e. ascribing a change to a particular user. When enough
logs have been collected, they can be used to detect anomalous behaviors
too. On EKS, the audit logs are sent to Amazon Cloudwatch Logs. The
audit policy for EKS is as follows:

[source,yaml]
----
apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
  # Log aws-auth configmap changes
  - level: RequestResponse
    namespaces: ["kube-system"]
    verbs: ["update", "patch", "delete"]
    resources:
      - group: "" # core
        resources: ["configmaps"]
        resourceNames: ["aws-auth"]
    omitStages:
      - "RequestReceived"
  - level: None
    users: ["system:kube-proxy"]
    verbs: ["watch"]
    resources:
      - group: "" # core
        resources: ["endpoints", "services", "services/status"]
  - level: None
    users: ["kubelet"] # legacy kubelet identity
    verbs: ["get"]
    resources:
      - group: "" # core
        resources: ["nodes", "nodes/status"]
  - level: None
    userGroups: ["system:nodes"]
    verbs: ["get"]
    resources:
      - group: "" # core
        resources: ["nodes", "nodes/status"]
  - level: None
    users:
      - system:kube-controller-manager
      - system:kube-scheduler
      - system:serviceaccount:kube-system:endpoint-controller
    verbs: ["get", "update"]
    namespaces: ["kube-system"]
    resources:
      - group: "" # core
        resources: ["endpoints"]
  - level: None
    users: ["system:apiserver"]
    verbs: ["get"]
    resources:
      - group: "" # core
        resources: ["namespaces", "namespaces/status", "namespaces/finalize"]
  - level: None
    users:
      - system:kube-controller-manager
    verbs: ["get", "list"]
    resources:
      - group: "metrics.k8s.io"
  - level: None
    nonResourceURLs:
      - /healthz*
      - /version
      - /swagger*
  - level: None
    resources:
      - group: "" # core
        resources: ["events"]
  - level: Request
    users: ["kubelet", "system:node-problem-detector", "system:serviceaccount:kube-system:node-problem-detector"]
    verbs: ["update","patch"]
    resources:
      - group: "" # core
        resources: ["nodes/status", "pods/status"]
    omitStages:
      - "RequestReceived"
  - level: Request
    userGroups: ["system:nodes"]
    verbs: ["update","patch"]
    resources:
      - group: "" # core
        resources: ["nodes/status", "pods/status"]
    omitStages:
      - "RequestReceived"
  - level: Request
    users: ["system:serviceaccount:kube-system:namespace-controller"]
    verbs: ["deletecollection"]
    omitStages:
      - "RequestReceived"
  # Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data,
  # so only log at the Metadata level.
  - level: Metadata
    resources:
      - group: "" # core
        resources: ["secrets", "configmaps"]
      - group: authentication.k8s.io
        resources: ["tokenreviews"]
    omitStages:
      - "RequestReceived"
  - level: Request
    resources:
      - group: ""
        resources: ["serviceaccounts/token"]
  - level: Request
    verbs: ["get", "list", "watch"]
    resources:
      - group: "" # core
      - group: "admissionregistration.k8s.io"
      - group: "apiextensions.k8s.io"
      - group: "apiregistration.k8s.io"
      - group: "apps"
      - group: "authentication.k8s.io"
      - group: "authorization.k8s.io"
      - group: "autoscaling"
      - group: "batch"
      - group: "certificates.k8s.io"
      - group: "extensions"
      - group: "metrics.k8s.io"
      - group: "networking.k8s.io"
      - group: "policy"
      - group: "rbac.authorization.k8s.io"
      - group: "scheduling.k8s.io"
      - group: "settings.k8s.io"
      - group: "storage.k8s.io"
    omitStages:
      - "RequestReceived"
  # Default level for known APIs
  - level: RequestResponse
    resources:
      - group: "" # core
      - group: "admissionregistration.k8s.io"
      - group: "apiextensions.k8s.io"
      - group: "apiregistration.k8s.io"
      - group: "apps"
      - group: "authentication.k8s.io"
      - group: "authorization.k8s.io"
      - group: "autoscaling"
      - group: "batch"
      - group: "certificates.k8s.io"
      - group: "extensions"
      - group: "metrics.k8s.io"
      - group: "networking.k8s.io"
      - group: "policy"
      - group: "rbac.authorization.k8s.io"
      - group: "scheduling.k8s.io"
      - group: "settings.k8s.io"
      - group: "storage.k8s.io"
    omitStages:
      - "RequestReceived"
  # Default level for all other requests.
  - level: Metadata
    omitStages:
      - "RequestReceived"
----

== Recommendations

=== Enable audit logs

The audit logs are part of the EKS managed Kubernetes control plane logs
that are managed by EKS. Instructions for enabling/disabling the control
plane logs, which includes the logs for the Kubernetes API server, the
controller manager, and the scheduler, along with the audit log, can be
found here,
https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html#enabling-control-plane-log-export.

!!! info When you enable control plane logging, you will incur
https://aws.amazon.com/cloudwatch/pricing/[costs] for storing the logs
in CloudWatch. This raises a broader issue about the ongoing cost of
security. Ultimately you will have to weigh those costs against the cost
of a security breach, e.g. financial loss, damage to your reputation,
etc. You may find that you can adequately secure your environment by
implementing only some of the recommendations in this guide.

!!! warning The maximum size for a CloudWatch Logs entry is
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/cloudwatch_limits_cwl.html[256KB]
whereas the maximum Kubernetes API request size is 1.5MiB. Log entries
greater than 256KB will either be truncated or only include the request
metadata.

=== Utilize audit metadata

Kubernetes audit logs include two annotations that indicate whether or
not a request was authorized `+authorization.k8s.io/decision+` and the
reason for the decision `+authorization.k8s.io/reason+`. Use these
attributes to ascertain why a particular API call was allowed.

=== Create alarms for suspicious events

Create an alarm to automatically alert you where there is an increase in
403 Forbidden and 401 Unauthorized responses, and then use attributes
like `+host+`, `+sourceIPs+`, and `+k8s_user.username+` to find out
where those requests are coming from.

=== Analyze logs with Log Insights

Use CloudWatch Log Insights to monitor changes to RBAC objects,
e.g. Roles, RoleBindings, ClusterRoles, and ClusterRoleBindings. A few
sample queries appear below:

Lists updates to the `+aws-auth+` ConfigMap:

[source,bash]
----
fields @timestamp, @message
| filter @logStream like "kube-apiserver-audit"
| filter verb in ["update", "patch"]
| filter objectRef.resource = "configmaps" and objectRef.name = "aws-auth" and objectRef.namespace = "kube-system"
| sort @timestamp desc
----

Lists creation of new or changes to validation webhooks:

[source,bash]
----
fields @timestamp, @message
| filter @logStream like "kube-apiserver-audit"
| filter verb in ["create", "update", "patch"] and responseStatus.code = 201
| filter objectRef.resource = "validatingwebhookconfigurations"
| sort @timestamp desc
----

Lists create, update, delete operations to Roles:

[source,bash]
----
fields @timestamp, @message
| sort @timestamp desc
| limit 100
| filter objectRef.resource="roles" and verb in ["create", "update", "patch", "delete"]
----

Lists create, update, delete operations to RoleBindings:

[source,bash]
----
fields @timestamp, @message
| sort @timestamp desc
| limit 100
| filter objectRef.resource="rolebindings" and verb in ["create", "update", "patch", "delete"]
----

Lists create, update, delete operations to ClusterRoles:

[source,bash]
----
fields @timestamp, @message
| sort @timestamp desc
| limit 100
| filter objectRef.resource="clusterroles" and verb in ["create", "update", "patch", "delete"]
----

Lists create, update, delete operations to ClusterRoleBindings:

[source,bash]
----
fields @timestamp, @message
| sort @timestamp desc
| limit 100
| filter objectRef.resource="clusterrolebindings" and verb in ["create", "update", "patch", "delete"]
----

Plots unauthorized read operations against Secrets:

[source,bash]
----
fields @timestamp, @message
| sort @timestamp desc
| limit 100
| filter objectRef.resource="secrets" and verb in ["get", "watch", "list"] and responseStatus.code="401"
| stats count() by bin(1m)
----

List of failed anonymous requests:

[source,bash]
----
fields @timestamp, @message, sourceIPs.0
| sort @timestamp desc
| limit 100
| filter user.username="system:anonymous" and responseStatus.code in ["401", "403"]
----

=== Audit your CloudTrail logs

AWS APIs called by pods that are utilizing IAM Roles for Service
Accounts (IRSA) are automatically logged to CloudTrail along with the
name of the service account. If the name of a service account that
wasn’t explicitly authorized to call an API appears in the log, it may
be an indication that the IAM role’s trust policy was misconfigured.
Generally speaking, Cloudtrail is a great way to ascribe AWS API calls
to specific IAM principals.

=== Use CloudTrail Insights to unearth suspicious activity

CloudTrail insights automatically analyzes write management events from
CloudTrail trails and alerts you of unusual activity. This can help you
identify when there’s an increase in call volume on write APIs in your
AWS account, including from pods that use IRSA to assume an IAM role.
See
https://aws.amazon.com/blogs/aws/announcing-cloudtrail-insights-identify-and-respond-to-unusual-api-activity/[Announcing
CloudTrail Insights: Identify and Response to Unusual API Activity] for
further information.

=== Additional resources

As the volume of logs increases, parsing and filtering them with Log
Insights or another log analysis tool may become ineffective. As an
alternative, you might want to consider running
https://github.com/falcosecurity/falco[Sysdig Falco] and
https://github.com/sysdiglabs/ekscloudwatch[ekscloudwatch]. Falco
analyzes audit logs and flags anomalies or abuse over an extended period
of time. The ekscloudwatch project forwards audit log events from
CloudWatch to Falco for analysis. Falco provides a set of
https://github.com/falcosecurity/plugins/blob/master/plugins/k8saudit/rules/k8s_audit_rules.yaml[default
audit rules] along with the ability to add your own.

Yet another option might be to store the audit logs in S3 and use the
SageMaker
https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html[Random
Cut Forest] algorithm to anomalous behaviors that warrant further
investigation.

== Tools and resources

The following commercial and open source projects can be used to assess
your cluster’s alignment with established best practices:

* https://catalog.workshops.aws/eks-security-immersionday/en-US/5-detective-controls[Amazon
EKS Security Immersion Workshop - Detective Controls]
* https://github.com/Shopify/kubeaudit[kubeaudit]
* https://github.com/octarinesec/kube-scan[kube-scan] Assigns a risk
score to the workloads running in your cluster in accordance with the
Kubernetes Common Configuration Scoring System framework
* https://kubesec.io/[kubesec.io]
* https://github.com/FairwindsOps/polaris[polaris]
* https://github.com/aquasecurity/starboard[Starboard]
* https://support.snyk.io/hc/en-us/articles/360003916138-Kubernetes-integration-overview[Snyk]
* https://github.com/kubescape/kubescape[Kubescape] Kubescape is an open
source kubernetes security tool that scans clusters, YAML files, and
Helm charts. It detects misconfigurations according to multiple
frameworks (including
https://www.armosec.io/blog/kubernetes-hardening-guidance-summary-by-armo/?utm_source=github&amp;utm_medium=repository[NSA-CISA]
and
https://www.microsoft.com/security/blog/2021/03/23/secure-containerized-environments-with-updated-threat-matrix-for-kubernetes/[MITRE
ATT&amp;CK®].)

:leveloffset: 1
:leveloffset: +1

= Network security

Network security has several facets. The first involves the application
of rules which restrict the flow of network traffic between services.
The second involves the encryption of traffic while it is in transit.
The mechanisms to implement these security measures on EKS are varied
but often include the following items:

== Traffic control

* Network Policies
* Security Groups

== Network encryption

* Service Mesh
* Container Network Interfaces (CNIs)
* Ingress Controllers and Load Balancers
* Nitro Instances
* ACM Private CA with cert-manager

== Network policy

Within a Kubernetes cluster, all Pod to Pod communication is allowed by
default. While this flexibility may help promote experimentation, it is
not considered secure. Kubernetes network policies give you a mechanism
to restrict network traffic between Pods (often referred to as East/West
traffic) as well as between Pods and external services. Kubernetes
network policies operate at layers 3 and 4 of the OSI model. Network
policies use pod, namespace selectors and labels to identify source and
destination pods, but can also include IP addresses, port numbers,
protocols, or a combination of these. Network Policies can be applied to
both Inbound or Outbound connections to the pod, often called Ingress
and Egress rules.

With native network policy support of Amazon VPC CNI Plugin, you can
implement network policies to secure network traffic in kubernetes
clusters. This integrates with the upstream Kubernetes Network Policy
API, ensuring compatibility and adherence to Kubernetes standards. You
can define policies using different
https://kubernetes.io/docs/concepts/services-networking/network-policies/[identifiers]
supported by the upstream API. By default, all ingress and egress
traffic is allowed to a pod. When a network policy with a policyType
Ingress is specified, only allowed connections into the pod are those
from the pod’s node and those allowed by the ingress rules. Same applies
for egress rules. If multiple rules are defined, then union of all rules
are taken into account when making the decision. Thus, order of
evaluation does not affect the policy result.

!!! attention When you first provision an EKS cluster, VPC CNI Network
Policy functionality is not enabled by default. Ensure you deployed
supported VPC CNI Add-on version and set `+ENABLE_NETWORK_POLICY+` flag
to `+true+` on the vpc-cni add-on to enable this. Refer
https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html[Amazon
EKS User guide] for detailed instructions.

== Recommendations

=== Getting Started with Network Policies - Follow Principle of Least Privilege

==== Create a default deny policy

As with RBAC policies, it is recommended to follow least privileged
access principles with network policies. Start by creating a deny all
policy that restricts all inbound and outbound traffic with in a
namespace.

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
----

.default-deny
image::./images/default-deny.jpg[default-deny]

!!! tip The image above was created by the network policy viewer from
https://orca.tufin.io/netpol/[Tufin].

==== Create a rule to allow DNS queries

Once you have the default deny all rule in place, you can begin layering
on additional rules, such as a rule that allows pods to query CoreDNS
for name resolution.

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-access
  namespace: default
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
----

.allow-dns-access
image::./images/allow-dns-access.jpg[allow-dns-access]

==== Incrementally add rules to selectively allow the flow of traffic between namespaces/pods

Understand the application requirements and create fine-grained ingress
and egress rules as needed. Below example shows how to restrict ingress
traffic on port 80 to `+app-one+` from `+client-one+`. This helps
minimize the attack surface and reduces the risk of unauthorized access.

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-app-one
  namespace: default
spec:
  podSelector:
    matchLabels:
      k8s-app: app-one
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          k8s-app: client-one
    ports:
    - protocol: TCP
      port: 80
----

.allow-ingress-app-one
image::./images/allow-ingress-app-one.png[allow-ingress-app-one]

=== Monitoring network policy enforcement

* *Use Network Policy editor*
** https://networkpolicy.io/[Network policy editor] helps with
visualizations, security score, autogenerates from network flow logs
** Build network policies in an interactive way
* *Audit Logs*
** Regularly review audit logs of your EKS cluster
** Audit logs provide wealth of information about what actions have been
performed on your cluster including changes to network policies
** Use this information to track changes to your network policies over
time and detect any unauthorized or unexpected changes
* *Automated testing*
** Implement automated testing by creating a test environment that
mirrors your production environment and periodically deploy workloads
that attempt to violate your network policies.
* *Monitoring metrics*
** Configure your observability agents to scrape the prometheus metrics
from the VPC CNI node agents, that allows to monitor the agent health,
and sdk errors.
* *Audit Network Policies regularly*
** Periodically audit your Network Policies to make sure that they meet
your current application requirements. As your application evolves, an
audit gives you the opportunity to remove redundant ingress, egress
rules and make sure that your applications don’t have excessive
permissions.
* *Ensure Network Policies exists using Open Policy Agent (OPA)*
** Use OPA Policy like shown below to ensure Network Policy always
exists before onboarding application pods. This policy denies onboarding
k8s pods with a label `+k8s-app: sample-app+` if corresponding network
policy does not exist.

[source,javascript]
----
package kubernetes.admission
import data.kubernetes.networkpolicies

deny[msg] {
    input.request.kind.kind == "Pod"
    pod_label_value := {v["k8s-app"] | v := input.request.object.metadata.labels}
    contains_label(pod_label_value, "sample-app")
    np_label_value := {v["k8s-app"] | v := networkpolicies[_].spec.podSelector.matchLabels}
    not contains_label(np_label_value, "sample-app")
    msg:= sprintf("The Pod %v could not be created because it is missing an associated Network Policy.", [input.request.object.metadata.name])
}
contains_label(arr, val) {
    arr[_] == val
}
----

=== Troubleshooting

==== Monitor the vpc-network-policy-controller, node-agent logs

Enable the EKS Control plane controller manager logs to diagnose the
network policy functionality. You can stream the control plane logs to a
CloudWatch log group and use
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html[CloudWatch
Log insights] to perform advanced queries. From the logs, you can view
what pod endpoint objects are resolved to a Network Policy,
reconcilation status of the policies, and debug if the policy is working
as expected.

In addition, Amazon VPC CNI allows you to enable the collection and
export of policy enforcement logs to
https://aws.amazon.com/cloudwatch/[Amazon Cloudwatch] from the EKS
worker nodes. Once enabled, you can leverage
https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ContainerInsights.html[CloudWatch
Container Insights] to provide insights on your usage related to Network
Policies.

Amazon VPC CNI also ships an SDK that provides an interface to interact
with eBPF programs on the node. The SDK is installed when the
`+aws-node+` is deployed onto the nodes. You can find the SDK binary
installed under `+/opt/cni/bin+` directory on the node. At launch, the
SDK provides support for fundamental functionalities such as inspecting
eBPF programs and maps.

[source,shell]
----
sudo /opt/cni/bin/aws-eks-na-cli ebpf progs
----

==== Log network traffic metadata

https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html[AWS VPC
Flow Logs] captures metadata about the traffic flowing through a VPC,
such as source and destination IP address and port along with
accepted/dropped packets. This information could be analyzed to look for
suspicious or unusual activity between resources within the VPC,
including Pods. However, since the IP addresses of pods frequently
change as they are replaced, Flow Logs may not be sufficient on its own.
Calico Enterprise extends the Flow Logs with pod labels and other
metadata, making it easier to decipher the traffic flows between pods.

== Security groups

EKS uses
https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html[AWS
VPC Security Groups] (SGs) to control the traffic between the Kubernetes
control plane and the cluster’s worker nodes. Security groups are also
used to control the traffic between worker nodes, and other VPC
resources, and external IP addresses. When you provision an EKS cluster
(with Kubernetes version 1.14-eks.3 or greater), a cluster security
group is automatically created for you. This security group allows
unfettered communication between the EKS control plane and the nodes
from managed node groups. For simplicity, it is recommended that you add
the cluster SG to all node groups, including unmanaged node groups.

Prior to Kubernetes version 1.14 and EKS version eks.3, there were
separate security groups configured for the EKS control plane and node
groups. The minimum and suggested rules for the control plane and node
group security groups can be found at
https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html.
The minimum rules for the _control plane security group_ allows port 443
inbound from the worker node SG. This rule is what allows the kubelets
to communicate with the Kubernetes API server. It also includes port
10250 for outbound traffic to the worker node SG; 10250 is the port that
the kubelets listen on. Similarly, the minimum _node group_ rules allow
port 10250 inbound from the control plane SG and 443 outbound to the
control plane SG. Finally there is a rule that allows unfettered
communication between nodes within a node group.

If you need to control communication between services that run within
the cluster and service the run outside the cluster such as an RDS
database, consider
https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html[security
groups for pods]. With security groups for pods, you can assign an
*existing* security group to a collection of pods.

!!! warning If you reference a security group that does not exist prior
to the creation of the pods, the pods will not get scheduled.

You can control which pods are assigned to a security group by creating
a `+SecurityGroupPolicy+` object and specifying a `+PodSelector+` or a
`+ServiceAccountSelector+`. Setting the selectors to `+{}+` will assign
the SGs referenced in the `+SecurityGroupPolicy+` to all pods in a
namespace or all Service Accounts in a namespace. Be sure you’ve
familiarized yourself with all the
https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html#security-groups-pods-considerations[considerations]
before implementing security groups for pods.

!!! important If you use SGs for pods you *must* create SGs that allow
port 53 outbound to the cluster security group. Similarly, you *must*
update the cluster security group to accept port 53 inbound traffic from
the pod security group.

!!! important The
https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html#vpc-limits-security-groups[limits
for security groups] still apply when using security groups for pods so
use them judiciously.

!!! important You *must* create rules for inbound traffic from the
cluster security group (kubelet) for all of the probes configured for
pod.

!!! important Security groups for pods relies on a feature known as
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-eni.html[ENI
trunking] which was created to increase the ENI density of an EC2
instance. When a pod is assigned to an SG, a VPC controller associates a
branch ENI from the node group with the pod. If there aren’t enough
branch ENIs available in a node group at the time the pod is scheduled,
the pod will stay in pending state. The number of branch ENIs an
instance can support varies by instance type/family. See
https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html#supported-instance-types
for further details.

While security groups for pods offers an AWS-native way to control
network traffic within and outside of your cluster without the overhead
of a policy daemon, other options are available. For example, the Cilium
policy engine allows you to reference a DNS name in a network policy.
Calico Enterprise includes an option for mapping network policies to AWS
security groups. If you’ve implemented a service mesh like Istio, you
can use an egress gateway to restrict network egress to specific, fully
qualified domains or IP addresses. For further information about this
option, read the three part series on
https://istio.io/blog/2019/egress-traffic-control-in-istio-part-1/[egress
traffic control in Istio].

== When to use Network Policy vs Security Group for Pods?

=== When to use Kubernetes network policy

* *Controlling pod-to-pod traffic*
** Suitable for controlling network traffic between pods inside a
cluster (east-west traffic)
* *Control traffic at the IP address or port level (OSI layer 3 or 4)*

=== When to use AWS Security groups for pods (SGP)

* *Leverage existing AWS configurations*
** If you already have complex set of EC2 security groups that manage
access to AWS services and you are migrating applications from EC2
instances to EKS, SGPs can be a very good choice allowing you to reuse
security group resources and apply them to your pods.
* *Control access to AWS services*
** Your applications running within an EKS cluster wants to communicate
with other AWS services (RDS database), use SGPs as an efficient
mechanism to control the traffic from the pods to AWS services.
* *Isolation of Pod &amp; Node traffic*
** If you want to completely separate pod traffic from the rest of the
node traffic, use SGP in `+POD_SECURITY_GROUP_ENFORCING_MODE=strict+`
mode.

=== Best practices using `+Security groups for pods+` and `+Network Policy+`

* *Layered security*
** Use a combination of SGP and kubernetes network policy for a layered
security approach
** Use SGPs to limit network level access to AWS services that are not
part of a cluster, while kubernetes network policies can restrict
network traffic between pods inside the cluster
* *Principle of least privilege*
** Only allow necessary traffic between pods or namespaces
* *Segment your applications*
** Wherever possible, segment applications by the network policy to
reduce the blast radius if an application is compromised
* *Keep policies simple and clear*
** Kubernetes network policies can be quite granular and complex, its
best to keep them as simple as possible to reduce the risk of
misconfiguration and ease the management overhead
* *Reduce the attack surface*
** Minimize the attack surface by limiting the exposure of your
applications

!!! attention Security Groups for pods provides two enforcing modes:
`+strict+` and `+standard+`. You must use `+standard+` mode when using
both Network Policy and Security Groups for pods features in an EKS
cluster.

When it comes to network security, a layered approach is often the most
effective solution. Using kubernetes network policy and SGP in
combination can provide a robust defense-in-depth strategy for your
applications running in EKS.

== Service Mesh Policy Enforcement or Kubernetes network policy

A `+service mesh+` is a dedicated infrastructure layer that you can add
to your applications. It allows you to transparently add capabilities
like observability, traffic management, and security, without adding
them to your own code.

Service mesh enforces policies at Layer 7 (application) of OSI model
whereas kubernetes network policies operate at Layer 3 (network) and
Layer 4 (transport). There are many offerings in this space like AWS
AppMesh, Istio, Linkerd, etc.,

=== When to use Service mesh for policy enforcement

* Have existing investment in a service mesh
* Need more advanced capabilities like traffic management, observability
&amp; security
** Traffic control, load balancing, circuit breaking, rate limiting,
timeouts etc.
** Detailed insights into how your services are performing (latency,
error rates, requests per second, request volumes etc.)
** You want to implement and leverage service mesh for security features
like mTLS

=== Choose Kubernetes network policy for simpler use cases

* Limit which pods can communicate with each other
* Network policies require fewer resources than a service mesh making
them a good fit for simpler use cases or for smaller clusters where the
overhead of running and managing a service mesh might not be justified

!!! tip Network policies and Service mesh can also be used together. Use
network policies to provide a baseline level of security and isolation
between your pods and then use a service mesh to add additional
capabilities like traffic management, observability and security.

== ThirdParty Network Policy Engines

Consider a Third Party Network Policy Engine when you have advanced
policy requirements like Global Network Policies, support for DNS
Hostname based rules, Layer 7 rules, ServiceAccount based rules, and
explicit deny/log actions, etc.,
https://docs.projectcalico.org/introduction/[Calico], is an open source
policy engine from https://tigera.io[Tigera] that works well with EKS.
In addition to implementing the full set of Kubernetes network policy
features, Calico supports extended network polices with a richer set of
features, including support for layer 7 rules, e.g. HTTP, when
integrated with Istio. Calico policies can be scoped to Namespaces,
Pods, service accounts, or globally. When policies are scoped to a
service account, it associates a set of ingress/egress rules with that
service account. With the proper RBAC rules in place, you can prevent
teams from overriding these rules, allowing IT security professionals to
safely delegate administration of namespaces. Isovalent, the maintainers
of https://cilium.readthedocs.io/en/stable/intro/[Cilium], have also
extended the network policies to include partial support for layer 7
rules, e.g. HTTP. Cilium also has support for DNS hostnames which can be
useful for restricting traffic between Kubernetes Services/Pods and
resources that run within or outside of your VPC. By contrast, Calico
Enterprise includes a feature that allows you to map a Kubernetes
network policy to an AWS security group, as well as DNS hostnames.

You can find a list of common Kubernetes network policies at
https://github.com/ahmetb/kubernetes-network-policy-recipes. A similar
set of rules for Calico are available at
https://docs.projectcalico.org/security/calico-network-policy.

=== Migration to Amazon VPC CNI Network Policy Engine

To maintain consistency and avoid unexpected pod communication behavior,
it is recommended to deploy only one Network Policy Engine in your
cluster. If you want to migrate from 3P to VPC CNI Network Policy
Engine, we recommend converting your existing 3P NetworkPolicy CRDs to
the Kubernetes NetworkPolicy resources before enabling VPC CNI network
policy support. And, test the migrated policies in a separate test
cluster before applying them in you production environment. This allows
you to identify and address any potential issues or inconsistencies in
pod communication behavior.

==== Migration Tool

To assist in your migration process, we have developed a tool called
https://github.com/awslabs/k8s-network-policy-migrator[K8s Network
Policy Migrator] that converts your existing Calico/Cilium network
policy CRDs to Kubernetes native network policies. After conversion you
can directly test the converted network policies on your new clusters
running VPC CNI network policy controller. The tool is designed to help
you streamline the migration process and ensure a smooth transition.

!!! Important Migration tool will only convert 3P policies that are
compatible with native kubernetes network policy api. If you are using
advanced network policy features offered by 3P plugins, Migration tool
will skip and report them.

Please note that migration tool is currently not supported by AWS VPC
CNI Network policy engineering team, it is made available to customers
on a best-effort basis. We encourage you to utilize this tool to
facilitate your migration process. In the event that you encounter any
issues or bugs with the tool, we kindly ask you create a
https://github.com/awslabs/k8s-network-policy-migrator/issues[GitHub
issue]. Your feedback is invaluable to us and will assist in the
continuous improvement of our services.

=== Additional Resources

* https://youtu.be/lEY2WnRHYpg[Kubernetes &amp; Tigera: Network
Policies&amp;#44; Security&amp;#44; and Audit]
* https://www.tigera.io/tigera-products/calico-enterprise/[Calico
Enterprise]
* https://cilium.readthedocs.io/en/stable/intro/[Cilium]
* https://cilium.io/blog/2021/02/10/network-policy-editor[NetworkPolicy
Editor] an interactive policy editor from Cilium
* https://www.inspektor-gadget.io/docs/latest/gadgets/advise/network-policy/[Inspektor
Gadget advise network-policy gadget] Suggests network policies based on
an analysis of network traffic

== Encryption in transit

Applications that need to conform to PCI, HIPAA, or other regulations
may need to encrypt data while it is in transit. Nowadays TLS is the de
facto choice for encrypting traffic on the wire. TLS, like it’s
predecessor SSL, provides secure communications over a network using
cryptographic protocols. TLS uses symmetric encryption where the keys to
encrypt the data are generated based on a shared secret that is
negotiated at the beginning of the session. The following are a few ways
that you can encrypt data in a Kubernetes environment.

=== Nitro Instances

Traffic exchanged between the following Nitro instance types, e.g. C5n,
G4, I3en, M5dn, M5n, P3dn, R5dn, and R5n, is automatically encrypted by
default. When there’s an intermediate hop, like a transit gateway or a
load balancer, the traffic is not encrypted. See
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/data-protection.html#encryption-transit[Encryption
in transit] for further details on encryption in transit as well as the
complete list of instances types that support network encryption by
default.

=== Container Network Interfaces (CNIs)

https://www.weave.works/oss/net/[WeaveNet] can be configured to
automatically encrypt all traffic using NaCl encryption for sleeve
traffic, and IPsec ESP for fast datapath traffic.

=== Service Mesh

Encryption in transit can also be implemented with a service mesh like
App Mesh, Linkerd v2, and Istio. AppMesh supports
https://docs.aws.amazon.com/app-mesh/latest/userguide/mutual-tls.html[mTLS]
with X.509 certificates or Envoy’s Secret Discovery Service(SDS).
Linkerd and Istio both have support for mTLS.

The https://github.com/aws/aws-app-mesh-examples[aws-app-mesh-examples]
GitHub repository provides walkthroughs for configuring mTLS using X.509
certificates and SPIRE as SDS provider with your Envoy container:

* https://github.com/aws/aws-app-mesh-examples/tree/main/walkthroughs/howto-k8s-mtls-file-based[Configuring
mTLS using X.509 certificates]
* https://github.com/aws/aws-app-mesh-examples/tree/main/walkthroughs/howto-k8s-mtls-sds-based[Configuring
TLS using SPIRE (SDS)]

App Mesh also supports
https://docs.aws.amazon.com/app-mesh/latest/userguide/virtual-node-tls.html[TLS
encryption] with a private certificate issued by
https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html[AWS
Certificate Manager] (ACM) or a certificate stored on the local file
system of the virtual node.

The https://github.com/aws/aws-app-mesh-examples[aws-app-mesh-examples]
GitHub repository provides walkthroughs for configuring TLS using
certificates issued by ACM and certificates that are packaged with your
Envoy container:

* https://github.com/aws/aws-app-mesh-examples/tree/master/walkthroughs/howto-tls-file-provided[Configuring
TLS with File Provided TLS Certificates]
* https://github.com/aws/aws-app-mesh-examples/tree/master/walkthroughs/tls-with-acm[Configuring
TLS with AWS Certificate Manager]

=== Ingress Controllers and Load Balancers

Ingress controllers are a way for you to intelligently route HTTP/S
traffic that emanates from outside the cluster to services running
inside the cluster. Oftentimes, these Ingresses are fronted by a layer 4
load balancer, like the Classic Load Balancer or the Network Load
Balancer (NLB). Encrypted traffic can be terminated at different places
within the network, e.g. at the load balancer, at the ingress resource,
or the Pod. How and where you terminate your SSL connection will
ultimately be dictated by your organization’s network security policy.
For instance, if you have a policy that requires end-to-end encryption,
you will have to decrypt the traffic at the Pod. This will place
additional burden on your Pod as it will have to spend cycles
establishing the initial handshake. Overall SSL/TLS processing is very
CPU intensive. Consequently, if you have the flexibility, try performing
the SSL offload at the Ingress or the load balancer.

==== Use encryption with AWS Elastic load balancers

The
https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html[AWS
Application Load Balancer] (ALB) and
https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html[Network
Load Balancer] (NLB) both have support for transport encryption (SSL and
TLS). The `+alb.ingress.kubernetes.io/certificate-arn+` annotation for
the ALB lets you to specify which certificates to add to the ALB. If you
omit the annotation the controller will attempt to add certificates to
listeners that require it by matching the available
https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html[AWS
Certificate Manager (ACM)] certificates using the host field. Starting
with EKS v1.15 you can use the
`+service.beta.kubernetes.io/aws-load-balancer-ssl-cert+` annotation
with the NLB as shown in the example below.

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: demo-app
  namespace: default
  labels:
    app: demo-app
  annotations:
     service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
     service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "&lt;certificate ARN&gt;"
     service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "443"
     service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
spec:
  type: LoadBalancer
  ports:
  - port: 443
    targetPort: 80
    protocol: TCP
  selector:
    app: demo-app
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nginx
  namespace: default
  labels:
    app: demo-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo-app
  template:
    metadata:
      labels:
        app: demo-app
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 443
              protocol: TCP
            - containerPort: 80
              protocol: TCP
----

Following are additional examples for SSL/TLS termination.

* https://aws.amazon.com/blogs/containers/securing-eks-ingress-contour-lets-encrypt-gitops/[Securing
EKS Ingress With Contour And Let’s Encrypt The GitOps Way]
* https://aws.amazon.com/premiumsupport/knowledge-center/terminate-https-traffic-eks-acm/[How
do I terminate HTTPS traffic on Amazon EKS workloads with ACM?]

!!! attention Some Ingresses, like the AWS LB controller, implement the
SSL/TLS using Annotations instead of as part of the Ingress Spec.

=== ACM Private CA with cert-manager

You can enable TLS and mTLS to secure your EKS application workloads at
the ingress, on the pod, and between pods using ACM Private Certificate
Authority (CA) and https://cert-manager.io/[cert-manager], a popular
Kubernetes add-on to distribute, renew, and revoke certificates. ACM
Private CA is a highly-available, secure, managed CA without the upfront
and maintenance costs of managing your own CA. If you are using the
default Kubernetes certificate authority, there is an opportunity to
improve your security and meet compliance requirements with ACM Private
CA. ACM Private CA secures private keys in FIPS 140-2 Level 3 hardware
security modules (very secure), compared with the default CA storing
keys encoded in memory (less secure). A centralized CA also gives you
more control and improved auditability for private certificates both
inside and outside of a Kubernetes environment.

==== Short-Lived CA Mode for Mutual TLS Between Workloads

When using ACM Private CA for mTLS in EKS, it is recommended that you
use short lived certificates with _short-lived CA mode_. Although it is
possible to issue out short-lived certificates in the general-purpose CA
mode, using short-lived CA mode works out more cost-effective (~75%
cheaper than general mode) for use cases where new certificates need to
be issued frequently. In addition to this, you should try to align the
validity period of the private certificates with the lifetime of the
pods in your EKS cluster.
https://aws.amazon.com/certificate-manager/private-certificate-authority/[Learn
more about ACM Private CA and its benefits here].

==== ACM Setup Instructions

Start by creating a Private CA by following procedures provided in the
https://docs.aws.amazon.com/acm-pca/latest/userguide/create-CA.html[ACM
Private CA tech docs]. Once you have a Private CA, install cert-manager
using https://cert-manager.io/docs/installation/[regular installation
instructions]. After installing cert-manager, install the Private CA
Kubernetes cert-manager plugin by following the
https://github.com/cert-manager/aws-privateca-issuer#setup[setup
instructions in GitHub]. The plugin lets cert-manager request private
certificates from ACM Private CA.

Now that you have a Private CA and an EKS cluster with cert-manager and
the plugin installed, it’s time to set permissions and create the
issuer. Update IAM permissions of the EKS node role to allow access to
ACM Private CA. Replace the `+&lt;CA_ARN&gt;+` with the value from your
Private CA:

[source,json]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "awspcaissuer",
            "Action": [
                "acm-pca:DescribeCertificateAuthority",
                "acm-pca:GetCertificate",
                "acm-pca:IssueCertificate"
            ],
            "Effect": "Allow",
            "Resource": "&lt;CA_ARN&gt;"
        }
    ]
}
----

https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[Service
Roles for IAM Accounts&amp;#44; or IRSA] can also be used. Please see the
Additional Resources section below for complete examples.

Create an Issuer in Amazon EKS by creating a Custom Resource Definition
file named cluster-issuer.yaml with the following text in it, replacing
`+&lt;CA_ARN&gt;+` and `+&lt;Region&gt;+` information with your Private CA.

[source,yaml]
----
apiVersion: awspca.cert-manager.io/v1beta1
kind: AWSPCAClusterIssuer
metadata:
          name: demo-test-root-ca
spec:
          arn: &lt;CA_ARN&gt;
          region: &lt;Region&gt;
----

Deploy the Issuer you created.

[source,bash]
----
kubectl apply -f cluster-issuer.yaml
----

Your EKS cluster is configured to request certificates from Private CA.
You can now use cert-manager’s `+Certificate+` resource to issue
certificates by changing the `+issuerRef+` field’s values to the Private
CA Issuer you created above. For more details on how to specify and
request Certificate resources, please check cert-manager’s
https://cert-manager.io/docs/usage/certificate/[Certificate Resources
guide].
https://github.com/cert-manager/aws-privateca-issuer/tree/main/config/samples/[See
examples here].

=== ACM Private CA with Istio and cert-manager

If you are running Istio in your EKS cluster, you can disable the Istio
control plane (specifically `+istiod+`) from functioning as the root
Certificate Authority (CA), and configure ACM Private CA as the root CA
for mTLS between workloads. If you’re going with this approach, consider
using the _short-lived CA mode_ in ACM Private CA. Refer to the
link:#short-lived-ca-mode-for-mutual-tls-between-workloads[previous
section] and this
https://aws.amazon.com/blogs/security/how-to-use-aws-private-certificate-authority-short-lived-certificate-mode[blog
post] for more details.

==== How Certificate Signing Works in Istio (Default)

Workloads in Kubernetes are identified using service accounts. If you
don’t specify a service account, Kubernetes will automatically assign
one to your workload. Also, service accounts automatically mount an
associated token. This token is used by the service account for
workloads to authenticate against the Kubernetes API. The service
account may be sufficient as an identity for Kubernetes but Istio has
its own identity management system and CA. When a workload starts up
with its envoy sidecar proxy, it needs an identity assigned from Istio
in order for it to be deemed as trustworthy and allowed to communicate
with other services in the mesh.

To get this identity from Istio, the `+istio-agent+` sends a request
known as a certificate signing request (or CSR) to the Istio control
plane. This CSR contains the service account token so that the
workload’s identity can be verified before being processed. This
verification process is handled by `+istiod+`, which acts as both the
Registration Authority (or RA) and the CA. The RA serves as a gatekeeper
that makes sure only verified CSR makes it through to the CA. Once the
CSR is verified, it will be forwarded to the CA which will then issue a
certificate containing a https://spiffe.io/[SPIFFE] identity with the
service account. This certificate is called a SPIFFE verifiable identity
document (or SVID). The SVID is assigned to the requesting service for
identification purposes and to encrypt the traffic in transit between
the communicating services.

.Default flow for Istio Certificate Signing Requests
image::./images/default-istio-csr-flow.png[Default flow for Istio
Certificate Signing Requests]

==== How Certificate Signing Works in Istio with ACM Private CA

You can use a cert-manager add-on called the Istio Certificate Signing
Request agent
(https://cert-manager.io/docs/projects/istio-csr/[istio-csr]) to
integrate Istio with ACM Private CA. This agent allows Istio workloads
and control plane components to be secured with cert manager issuers, in
this case ACM Private CA. The _istio-csr_ agent exposes the same service
that _istiod_ serves in the default config of validating incoming CSRs.
Except, after verification, it will convert the requests into resources
that cert manager supports (i.e. integrations with external CA issuers).

Whenever there’s a CSR from a workload, it will be forwarded to
_istio-csr_, which will request certificates from ACM Private CA. This
communication between _istio-csr_ and ACM Private CA is enabled by the
https://github.com/cert-manager/aws-privateca-issuer[AWS Private CA
issuer plugin]. Cert manager uses this plugin to request TLS
certificates from ACM Private CA. The issuer plugin will communicate
with the ACM Private CA service to request a signed certificate for the
workload. Once the certificate has been signed, it will be returned to
_istio-csr_, which will read the signed request, and return it to the
workload that initiated the CSR.

.Flow for Istio Certificate Signing Requests with istio-csr
image::./images/istio-csr-with-acm-private-ca.png[Flow for Istio
Certificate Signing Requests with istio-csr]

==== Istio with Private CA Setup Instructions

[arabic]
. Start by following the same
link:#acm-private-ca-with-cert-manager[setup instructions in this
section] to complete the following:
. Create a Private CA
. Install cert-manager
. Install the issuer plugin
. Set permissions and create an issuer. The issuer represents the CA and
is used to sign `+istiod+` and mesh workload certificates. It will
communicate with ACM Private CA.
. Create an `+istio-system+` namespace. This is where the
`+istiod certificate+` and other Istio resources will be deployed.
. Install Istio CSR configured with AWS Private CA Issuer Plugin. You
can preserve the certificate signing requests for workloads to verify
that they get approved and signed
(`+preserveCertificateRequests=true+`).
+
[source,bash]
----
helm install -n cert-manager cert-manager-istio-csr jetstack/cert-manager-istio-csr \
--set "app.certmanager.issuer.group=awspca.cert-manager.io" \
--set "app.certmanager.issuer.kind=AWSPCAClusterIssuer" \
--set "app.certmanager.issuer.name=&lt;the-name-of-the-issuer-you-created&gt;" \
--set "app.certmanager.preserveCertificateRequests=true" \
--set "app.server.maxCertificateDuration=48h" \
--set "app.tls.certificateDuration=24h" \
--set "app.tls.istiodCertificateDuration=24h" \
--set "app.tls.rootCAFile=/var/run/secrets/istio-csr/ca.pem" \
--set "volumeMounts[0].name=root-ca" \
--set "volumeMounts[0].mountPath=/var/run/secrets/istio-csr" \
--set "volumes[0].name=root-ca" \
--set "volumes[0].secret.secretName=istio-root-ca"
----
. Install Istio with custom configurations to replace `+istiod+` with
`+cert-manager istio-csr+` as the certificate provider for the mesh.
This process can be carried out using the
https://tetrate.io/blog/what-is-istio-operator/[Istio Operator].
+
[source,yaml]
----
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio
  namespace: istio-system
spec:
  profile: "demo"
  hub: gcr.io/istio-release
  values:
  global:
    # Change certificate provider to cert-manager istio agent for istio agent
    caAddress: cert-manager-istio-csr.cert-manager.svc:443
  components:
    pilot:
      k8s:
        env:
          # Disable istiod CA Sever functionality
        - name: ENABLE_CA_SERVER
          value: "false"
        overlays:
        - apiVersion: apps/v1
          kind: Deployment
          name: istiod
          patches:

            # Mount istiod serving and webhook certificate from Secret mount
          - path: spec.template.spec.containers.[name:discovery].args[7]
            value: "--tlsCertFile=/etc/cert-manager/tls/tls.crt"
          - path: spec.template.spec.containers.[name:discovery].args[8]
            value: "--tlsKeyFile=/etc/cert-manager/tls/tls.key"
          - path: spec.template.spec.containers.[name:discovery].args[9]
            value: "--caCertFile=/etc/cert-manager/ca/root-cert.pem"

          - path: spec.template.spec.containers.[name:discovery].volumeMounts[6]
            value:
              name: cert-manager
              mountPath: "/etc/cert-manager/tls"
              readOnly: true
          - path: spec.template.spec.containers.[name:discovery].volumeMounts[7]
            value:
              name: ca-root-cert
              mountPath: "/etc/cert-manager/ca"
              readOnly: true

          - path: spec.template.spec.volumes[6]
            value:
              name: cert-manager
              secret:
                secretName: istiod-tls
          - path: spec.template.spec.volumes[7]
            value:
              name: ca-root-cert
              configMap:
                defaultMode: 420
                name: istio-ca-root-cert
----
. Deploy the above custom resource you created.
+
[source,bash]
----
istioctl operator init
kubectl apply -f istio-custom-config.yaml
----
. Now you can deploy a workload to the mesh in your EKS cluster and
https://istio.io/latest/docs/reference/config/security/peer_authentication/[enforce
mTLS].

.Istio certificate signing requests
image::./images/istio-csr-requests.png[Istio certificate signing
requests]

== Tools and resources

* https://catalog.workshops.aws/eks-security-immersionday/en-US/6-network-security[Amazon
EKS Security Immersion Workshop - Network security]
* https://aws.amazon.com/blogs/security/tls-enabled-kubernetes-clusters-with-acm-private-ca-and-amazon-eks-2/[How
to implement cert-manager and the ACM Private CA plugin to enable TLS in
EKS].
* https://aws.amazon.com/blogs/containers/setting-up-end-to-end-tls-encryption-on-amazon-eks-with-the-new-aws-load-balancer-controller/[Setting
up end-to-end TLS encryption on Amazon EKS with the new AWS Load
Balancer Controller and ACM Private CA].
* https://github.com/cert-manager/aws-privateca-issuer[Private CA
Kubernetes cert-manager plugin on GitHub].
* https://docs.aws.amazon.com/acm-pca/latest/userguide/PcaKubernetes.html[Private
CA Kubernetes cert-manager plugin user guide].
* https://aws.amazon.com/blogs/security/how-to-use-aws-private-certificate-authority-short-lived-certificate-mode[How
to use AWS Private Certificate Authority short-lived certificate mode]
* https://itnext.io/verifying-service-mesh-tls-in-kubernetes-using-ksniff-and-wireshark-2e993b26bf95[Verifying
Service Mesh TLS in Kubernetes&amp;#44; Using ksniff and Wireshark]
* https://github.com/eldadru/ksniff[ksniff]
* https://github.com/monzo/egress-operator[egress-operator] An operator
and DNS plugin to control egress traffic from your cluster without
protocol inspection
* https://www.suse.com/neuvector/[NeuVector by SUSE] open source,
zero-trust container security platform, provides policy network rules,
data loss prevention (DLP), web application firewall (WAF) and network
threat signatures.

:leveloffset: 1
:leveloffset: +1

= Data encryption and secrets management

== Encryption at rest

There are three different AWS-native storage options you can use with
Kubernetes:
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html[EBS],
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEFS.html[EFS],
and https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html[FSx
for Lustre]. All three offer encryption at rest using a service managed
key or a customer master key (CMK). For EBS you can use the in-tree
storage driver or the
https://github.com/kubernetes-sigs/aws-ebs-csi-driver[EBS CSI driver].
Both include parameters for encrypting volumes and supplying a CMK. For
EFS, you can use the
https://github.com/kubernetes-sigs/aws-efs-csi-driver[EFS CSI driver],
however, unlike EBS, the EFS CSI driver does not support dynamic
provisioning. If you want to use EFS with EKS, you will need to
provision and configure at-rest encryption for the file system prior to
creating a PV. For further information about EFS file encryption, please
refer to
https://docs.aws.amazon.com/efs/latest/ug/encryption-at-rest.html[Encrypting
Data at Rest]. Besides offering at-rest encryption, EFS and FSx for
Lustre include an option for encrypting data in transit. FSx for Lustre
does this by default. For EFS, you can add transport encryption by
adding the `+tls+` parameter to `+mountOptions+` in your PV as in this
example:

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: efs-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  mountOptions:
    - tls
  csi:
    driver: efs.csi.aws.com
    volumeHandle: &lt;file_system_id&gt;
----

The https://github.com/kubernetes-sigs/aws-fsx-csi-driver[FSx CSI
driver] supports dynamic provisioning of Lustre file systems. It
encrypts data with a service managed key by default, although there is
an option to provide your own CMK as in this example:

[source,yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fsx-sc
provisioner: fsx.csi.aws.com
parameters:
  subnetId: subnet-056da83524edbe641
  securityGroupIds: sg-086f61ea73388fb6b
  deploymentType: PERSISTENT_1
  kmsKeyId: &lt;kms_arn&gt;
----

!!! attention As of May 28, 2020 all data written to the ephemeral
volume in EKS Fargate pods is encrypted by default using an
industry-standard AES-256 cryptographic algorithm. No modifications to
your application are necessary as encryption and decryption are handled
seamlessly by the service.

=== Encrypt data at rest

Encrypting data at rest is considered a best practice. If you’re unsure
whether encryption is necessary, encrypt your data.

=== Rotate your CMKs periodically

Configure KMS to automatically rotate your CMKs. This will rotate your
keys once a year while saving old keys indefinitely so that your data
can still be decrypted. For additional information see
https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html[Rotating
customer master keys]

=== Use EFS access points to simplify access to shared datasets

If you have shared datasets with different POSIX file permissions or
want to restrict access to part of the shared file system by creating
different mount points, consider using EFS access points. To learn more
about working with access points, see
https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html. Today,
if you want to use an access point (AP) you’ll need to reference the AP
in the PV’s `+volumeHandle+` parameter.

!!! attention As of March 23, 2021 the EFS CSI driver supports dynamic
provisioning of EFS Access Points. Access points are
application-specific entry points into an EFS file system that make it
easier to share a file system between multiple pods. Each EFS file
system can have up to 120 PVs. See
https://aws.amazon.com/blogs/containers/introducing-efs-csi-dynamic-provisioning/[Introducing
Amazon EFS CSI dynamic provisioning] for additional information.

== Secrets management

Kubernetes secrets are used to store sensitive information, such as user
certificates, passwords, or API keys. They are persisted in etcd as
base64 encoded strings. On EKS, the EBS volumes for etcd nodes are
encrypted with
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html[EBS
encryption]. A pod can retrieve a Kubernetes secrets objects by
referencing the secret in the `+podSpec+`. These secrets can either be
mapped to an environment variable or mounted as volume. For additional
information on creating secrets, see
https://kubernetes.io/docs/concepts/configuration/secret/.

!!! caution Secrets in a particular namespace can be referenced by all
pods in the secret’s namespace.

!!! caution The node authorizer allows the Kubelet to read all of the
secrets mounted to the node.

=== Use AWS KMS for envelope encryption of Kubernetes secrets

This allows you to encrypt your secrets with a unique data encryption
key (DEK). The DEK is then encrypted using a key encryption key (KEK)
from AWS KMS which can be automatically rotated on a recurring schedule.
With the KMS plugin for Kubernetes, all Kubernetes secrets are stored in
etcd in ciphertext instead of plain text and can only be decrypted by
the Kubernetes API server. For additional details, see
https://aws.amazon.com/blogs/containers/using-eks-encryption-provider-support-for-defense-in-depth/[using
EKS encryption provider support for defense in depth]

=== Audit the use of Kubernetes Secrets

On EKS, turn on audit logging and create a CloudWatch metrics filter and
alarm to alert you when a secret is used (optional). The following is an
example of a metrics filter for the Kubernetes audit log,
`+{($.verb="get") &amp;&amp; ($.objectRef.resource="secret")}+`. You can also
use the following queries with CloudWatch Log Insights:

[source,bash]
----
fields @timestamp, @message
| sort @timestamp desc
| limit 100
| stats count(*) by objectRef.name as secret
| filter verb="get" and objectRef.resource="secrets"
----

The above query will display the number of times a secret has been
accessed within a specific timeframe.

[source,bash]
----
fields @timestamp, @message
| sort @timestamp desc
| limit 100
| filter verb="get" and objectRef.resource="secrets"
| display objectRef.namespace, objectRef.name, user.username, responseStatus.code
----

This query will display the secret, along with the namespace and
username of the user who attempted to access the secret and the response
code.

=== Rotate your secrets periodically

Kubernetes doesn’t automatically rotate secrets. If you have to rotate
secrets, consider using an external secret store, e.g. Vault or AWS
Secrets Manager.

=== Use separate namespaces as a way to isolate secrets from different applications

If you have secrets that cannot be shared between applications in a
namespace, create a separate namespace for those applications.

=== Use volume mounts instead of environment variables

The values of environment variables can unintentionally appear in logs.
Secrets mounted as volumes are instantiated as tmpfs volumes (a RAM
backed file system) that are automatically removed from the node when
the pod is deleted.

=== Use an external secrets provider

There are several viable alternatives to using Kubernetes secrets,
including https://aws.amazon.com/secrets-manager/[AWS Secrets Manager]
and Hashicorp’s
https://www.hashicorp.com/blog/injecting-vault-secrets-into-kubernetes-pods-via-a-sidecar/[Vault].
These services offer features such as fine grained access controls,
strong encryption, and automatic rotation of secrets that are not
available with Kubernetes Secrets. Bitnami’s
https://github.com/bitnami-labs/sealed-secrets[Sealed Secrets] is
another approach that uses asymmetric encryption to create "`sealed
secrets`". A public key is used to encrypt the secret while the private
key used to decrypt the secret is kept within the cluster, allowing you
to safely store sealed secrets in source control systems like Git. See
https://aws.amazon.com/blogs/opensource/managing-secrets-deployment-in-kubernetes-using-sealed-secrets/[Managing
secrets deployment in Kubernetes using Sealed Secrets] for further
information.

As the use of external secrets stores has grown, so has need for
integrating them with Kubernetes. The
https://github.com/kubernetes-sigs/secrets-store-csi-driver[Secret Store
CSI Driver] is a community project that uses the CSI driver model to
fetch secrets from external secret stores. Currently, the Driver has
support for
https://github.com/aws/secrets-store-csi-driver-provider-aws[AWS Secrets
Manager], Azure, Vault, and GCP. The AWS provider supports both AWS
Secrets Manager *and* AWS Parameter Store. It can also be configured to
rotate secrets when they expire and can synchronize AWS Secrets Manager
secrets to Kubernetes Secrets. Synchronization of secrets can be useful
when you need to reference a secret as an environment variable instead
of reading them from a volume.

!!! note When the the secret store CSI driver has to fetch a secret, it
assumes the IRSA role assigned to the pod that references a secret. The
code for this operation can be found
https://github.com/aws/secrets-store-csi-driver-provider-aws/blob/main/auth/auth.go[here].

For additional information about the AWS Secrets &amp; Configuration
Provider (ASCP) refer to the following resources:

* https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/[How
to use AWS Secrets Configuration Provider with Kubernetes Secret Store
CSI Driver]
* https://docs.aws.amazon.com/secretsmanager/latest/userguide/integrating_csi_driver.html[Integrating
Secrets Manager secrets with Kubernetes Secrets Store CSI Driver]

https://github.com/external-secrets/external-secrets[external-secrets]
is yet another way to use an external secret store with Kubernetes. Like
the CSI Driver, external-secrets works against a variety of different
backends, including AWS Secrets Manager. The difference is, rather than
retrieving secrets from the external secret store, external-secrets
copies secrets from these backends to Kubernetes as Secrets. This lets
you manage secrets using your preferred secret store and interact with
secrets in a Kubernetes-native way.

== Tools and resources

* https://catalog.workshops.aws/eks-security-immersionday/en-US/13-data-encryption-and-secret-management[Amazon
EKS Security Immersion Workshop - Data Encryption and Secrets
Management]

:leveloffset: 1
:leveloffset: +1

= Runtime security

Runtime security provides active protection for your containers while
they’re running. The idea is to detect and/or prevent malicious activity
from occurring inside the container. This can be achieved with a number
of mechanisms in the Linux kernel or kernel extensions that are
integrated with Kubernetes, such as Linux capabilities, secure computing
(seccomp), AppArmor, or SELinux. There are also options like Amazon
GuardDuty and third party tools that can assist with establishing
baselines and detecting anomalous activity with less manual
configuration of Linux kernel mechanisms.

!!! attention Kubernetes does not currently provide any native
mechanisms for loading seccomp, AppArmor, or SELinux profiles onto
Nodes. They either have to be loaded manually or installed onto Nodes
when they are bootstrapped. This has to be done prior to referencing
them in your Pods because the scheduler is unaware of which nodes have
profiles. See below how tools like Security Profiles Operator can help
automate provisioning of profiles onto nodes.

== Security contexts and built-in Kubernetes controls

Many Linux runtime security mechanisms are tightly integrated with
Kubernetes and can be configured through Kubernetes
https://kubernetes.io/docs/tasks/configure-pod-container/security-context/[security
contexts]. One such option is the `+privileged+` flag, which is
`+false+` by default and if enabled is essentially equivalent to root on
the host. It is nearly always inappropriate to enable privileged mode in
production workloads, but there are many more controls that can provide
more granular privileges to containers as appropriate.

=== Linux capabilities

Linux capabilities allow you to grant certain capabilities to a Pod or
container without providing all the abilities of the root user. Examples
include `+CAP_NET_ADMIN+`, which allows configuring network interfaces
or firewalls, or `+CAP_SYS_TIME+`, which allows manipulation of the
system clock.

=== Seccomp

With secure computing (seccomp) you can prevent a containerized
application from making certain syscalls to the underlying host
operating system’s kernel. While the Linux operating system has a few
hundred system calls, the lion’s share of them are not necessary for
running containers. By restricting what syscalls can be made by a
container, you can effectively decrease your application’s attack
surface.

Seccomp works by intercepting syscalls and only allowing those that have
been allowlisted to pass through. Docker has a
https://github.com/moby/moby/blob/master/profiles/seccomp/default.json[default]
seccomp profile which is suitable for a majority of general purpose
workloads, and other container runtimes like containerd provide
comparable defaults. You can configure your container or Pod to use the
container runtime’s default seccomp profile by adding the following to
the `+securityContext+` section of the Pod spec:

[source,yaml]
----
securityContext:
  seccompProfile:
    type: RuntimeDefault
----

As of 1.22 (in alpha, stable as of 1.27), the above `+RuntimeDefault+`
can be used for all Pods on a Node using a
https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads[single
kubelet flag], `+--seccomp-default+`. Then the profile specified in
`+securityContext+` is only needed for other profiles.

It’s also possible to create your own profiles for things that require
additional privileges. This can be very tedious to do manually, but
there are tools like
https://github.com/inspektor-gadget/inspektor-gadget[Inspektor Gadget]
(also recommended in the link:../network/[network security section] for
generating network policies) and
https://github.com/inspektor-gadget/inspektor-gadget[Security Profiles
Operator] that support using tools like eBPF or logs to record baseline
privilege requirements as seccomp profiles. Security Profiles Operator
further allows automating the deployment of recorded profiles to nodes
for use by Pods and containers.

=== AppArmor and SELinux

AppArmor and SELinux are known as
https://en.wikipedia.org/wiki/Mandatory_access_control[mandatory access
control or MAC systems]. They are similar in concept to seccomp but with
different APIs and abilities, allowing access control for e.g. specific
filesystem paths or network ports. Support for these tools depends on
the Linux distribution, with Debian/Ubuntu supporting AppArmor and
RHEL/CentOS/Bottlerocket/Amazon Linux 2023 supporting SELinux. Also see
the link:../hosts/#run-selinux[infrastructure security section] for
further discussion of SELinux.

Both AppArmor and SELinux are integrated with Kubernetes, but as of
Kubernetes 1.28 AppArmor profiles must be specified via
https://kubernetes.io/docs/tutorials/security/apparmor/#securing-a-pod[annotations]
while SELinux labels can be set through the
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#selinuxoptions-v1-core[SELinuxOptions]
field on the security context directly.

As with seccomp profiles, the Security Profiles Operator mentioned above
can assist with deploying profiles onto nodes in the cluster. (In the
future, the project also aims to generate profiles for AppArmor and
SELinux as it does for seccomp.)

== Recommendations

=== Use Amazon GuardDuty for runtime monitoring and detecting threats to your EKS environments

If you do not currently have a solution for continuously monitoring EKS
runtimes and analyzing EKS audit logs, and scanning for malware and
other suspicious activity, Amazon strongly recommends the use of
https://aws.amazon.com/guardduty/[Amazon GuardDuty] for customers who
want a simple, fast, secure, scalable, and cost-effective one-click way
to protect their AWS environments. Amazon GuardDuty is a security
monitoring service that analyzes and processes foundational data
sources, such as AWS CloudTrail management events, AWS CloudTrail event
logs, VPC flow logs (from Amazon EC2 instances), Kubernetes audit logs,
and DNS logs. It also includes EKS runtime monitoring. It uses
continuously updated threat intelligence feeds, such as lists of
malicious IP addresses and domains, and machine learning to identify
unexpected, potentially unauthorized, and malicious activity within your
AWS environment. This can include issues like escalation of privileges,
use of exposed credentials, or communication with malicious IP
addresses, domains, presence of malware on your Amazon EC2 instances and
EKS container workloads, or discovery of suspicious API activity.
GuardDuty informs you of the status of your AWS environment by producing
security findings that you can view in the GuardDuty console or through
Amazon EventBridge. GuardDuty also provides support for you to export
your findings to an Amazon Simple Storage Service (S3) bucket, and
integrate with other services such as AWS Security Hub and Detective.

Watch this AWS Online Tech Talk
https://www.youtube.com/watch?v=oNHGRRroJuE["`Enhanced threat detection
for Amazon EKS with Amazon GuardDuty - AWS Online Tech Talks`"] to see
how to enable these additional EKS security features step-by-step in
minutes.

=== Optionally: Use a 3rd party solution for runtime monitoring

Creating and managing seccomp and Apparmor profiles can be difficult if
you’re not familiar with Linux security. If you don’t have the time to
become proficient, consider using a 3rd party commercial solution. A lot
of them have moved beyond static profiles like Apparmor and seccomp and
have begun using machine learning to block or alert on suspicious
activity. A handful of these solutions can be found below in the
link:#tools-and-resources[tools] section. Additional options can be
found on the https://aws.amazon.com/marketplace/features/containers[AWS
Marketplace for Containers].

=== Consider add/dropping Linux capabilities before writing seccomp policies

Capabilities involve various checks in kernel functions reachable by
syscalls. If the check fails, the syscall typically returns an error.
The check can be done either right at the beginning of a specific
syscall, or deeper in the kernel in areas that might be reachable
through multiple different syscalls (such as writing to a specific
privileged file). Seccomp, on the other hand, is a syscall filter which
is applied to all syscalls before they are run. A process can set up a
filter which allows them to revoke their right to run certain syscalls,
or specific arguments for certain syscalls.

Before using seccomp, consider whether adding/removing Linux
capabilities gives you the control you need. See
https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container[Setting
capabilities for- containers] for further information.

=== See whether you can accomplish your aims by using Pod Security Policies (PSPs)

Pod Security Policies offer a lot of different ways to improve your
security posture without introducing undue complexity. Explore the
options available in PSPs before venturing into building seccomp and
Apparmor profiles.

!!! warning As of Kubernetes 1.25, PSPs have been removed and replaced
with the
https://kubernetes.io/docs/concepts/security/pod-security-admission/[Pod
Security Admission] controller. Third-party alternatives which exist
include OPA/Gatekeeper and Kyverno. A collection of Gatekeeper
constraints and constraint templates for implementing policies commonly
found in PSPs can be pulled from the
https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/pod-security-policy[Gatekeeper
library] repository on GitHub. And many replacements for PSPs can be
found in the https://main.kyverno.io/policies/[Kyverno policy library]
including the full collection of
https://kubernetes.io/docs/concepts/security/pod-security-standards/[Pod
Security Standards].

== Tools and Resources

* https://itnext.io/seccomp-in-kubernetes-part-i-7-things-you-should-know-before-you-even-start-97502ad6b6d6[7
things you should know before you start]
* https://github.com/kubernetes/kubernetes/tree/master/test/images/apparmor-loader[AppArmor
Loader]
* https://kubernetes.io/docs/tutorials/clusters/apparmor/#setting-up-nodes-with-profiles[Setting
up nodes with profiles]
* https://github.com/kubernetes-sigs/security-profiles-operator[Security
Profiles Operator] is a Kubernetes enhancement which aims to make it
easier for users to use SELinux, seccomp and AppArmor in Kubernetes
clusters. It provides capabilities for both generating profiles from
running workloads and loading profiles onto Kubernetes nodes for use in
Pods.
* https://github.com/inspektor-gadget/inspektor-gadget[Inspektor Gadget]
allows inspecting, tracing, and profiling many aspects of runtime
behavior on Kubernetes, including assisting in the generation of seccomp
profiles.
* https://www.aquasec.com/products/aqua-cloud-native-security-platform/[Aqua]
* https://www.qualys.com/apps/container-security/[Qualys]
* https://www.stackrox.com/use-cases/threat-detection/[Stackrox]
* https://sysdig.com/products/kubernetes-security/[Sysdig Secure]
* https://docs.paloaltonetworks.com/cn-series[Prisma]
* https://www.suse.com/neuvector/[NeuVector by SUSE] open source,
zero-trust container security platform, provides process profile rules
and file access rules.

:leveloffset: 1
:leveloffset: +1

= Protecting the infrastructure (hosts)

Inasmuch as it’s important to secure your container images, it’s equally
important to safeguard the infrastructure that runs them. This section
explores different ways to mitigate risks from attacks launched directly
against the host. These guidelines should be used in conjunction with
those outlined in the link:runtime.md[Runtime Security] section.

== Recommendations

=== Use an OS optimized for running containers

Consider using Flatcar Linux, Project Atomic, RancherOS, and
https://github.com/bottlerocket-os/bottlerocket/[Bottlerocket], a
special purpose OS from AWS designed for running Linux containers. It
includes a reduced attack surface, a disk image that is verified on
boot, and enforced permission boundaries using SELinux.

Alternately, use the
https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-amis.html[EKS
optimized AMI] for your Kubernetes worker nodes. The EKS optimized AMI
is released regularly and contains a minimal set of OS packages and
binaries necessary to run your containerized workloads.

Please refer https://github.com/aws-samples/amazon-eks-ami-rhel[Amazon
EKS AMI RHEL Build Specification] for a sample configuration script
which can be used for building a custom Amazon EKS AMI running on Red
Hat Enterprise Linux using Hashicorp Packer. This script can be further
leveraged to build STIG compliant EKS custom AMIs.

=== Keep your worker node OS updated

Regardless of whether you use a container-optimized host OS like
Bottlerocket or a larger, but still minimalist, Amazon Machine Image
like the EKS optimized AMIs, it is best practice to keep these host OS
images up to date with the latest security patches.

For the EKS optimized AMIs, regularly check the
https://github.com/awslabs/amazon-eks-ami/blob/master/CHANGELOG.md[CHANGELOG]
and/or https://github.com/awslabs/amazon-eks-ami/releases[release notes
channel] and automate the rollout of updated worker node images into
your cluster.

=== Treat your infrastructure as immutable and automate the replacement of your worker nodes

Rather than performing in-place upgrades, replace your workers when a
new patch or update becomes available. This can be approached a couple
of ways. You can either add instances to an existing autoscaling group
using the latest AMI as you sequentially cordon and drain nodes until
all of the nodes in the group have been replaced with the latest AMI.
Alternatively, you can add instances to a new node group while you
sequentially cordon and drain nodes from the old node group until all of
the nodes have been replaced. EKS
https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html[managed
node groups] uses the first approach and will display a message in the
console to upgrade your workers when a new AMI becomes available.
`+eksctl+` also has a mechanism for creating node groups with the latest
AMI and for gracefully cordoning and draining pods from nodes groups
before the instances are terminated. If you decide to use a different
method for replacing your worker nodes, it is strongly recommended that
you automate the process to minimize human oversight as you will likely
need to replace workers regularly as new updates/patches are released
and when the control plane is upgraded.

With EKS Fargate, AWS will automatically update the underlying
infrastructure as updates become available. Oftentimes this can be done
seamlessly, but there may be times when an update will cause your pod to
be rescheduled. Hence, we recommend that you create deployments with
multiple replicas when running your application as a Fargate pod.

=== Periodically run kube-bench to verify compliance with https://www.cisecurity.org/benchmark/kubernetes/[CIS benchmarks for Kubernetes]

kube-bench is an open source project from Aqua that evaluates your
cluster against the CIS benchmarks for Kubernetes. The benchmark
describes the best practices for securing unmanaged Kubernetes clusters.
The CIS Kubernetes Benchmark encompasses the control plane and the data
plane. Since Amazon EKS provides a fully managed control plane, not all
of the recommendations from the CIS Kubernetes Benchmark are applicable.
To ensure this scope reflects how Amazon EKS is implemented, AWS created
the _CIS Amazon EKS Benchmark_. The EKS benchmark inherits from CIS
Kubernetes Benchmark with additional inputs from the community with
specific configuration considerations for EKS clusters.

When running https://github.com/aquasecurity/kube-bench[kube-bench]
against an EKS cluster, follow
https://github.com/aquasecurity/kube-bench/blob/main/docs/running.md#running-cis-benchmark-in-an-eks-cluster[these
instructions] from Aqua Security. For further information see
https://aws.amazon.com/blogs/containers/introducing-cis-amazon-eks-benchmark/[Introducing
The CIS Amazon EKS Benchmark].

=== Minimize access to worker nodes

Instead of enabling SSH access, use
https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html[SSM
Session Manager] when you need to remote into a host. Unlike SSH keys
which can be lost, copied, or shared, Session Manager allows you to
control access to EC2 instances using IAM. Moreover, it provides an
audit trail and log of the commands that were run on the instance.

As of August 19th, 2020 Managed Node Groups support custom AMIs and EC2
Launch Templates. This allows you to embed the SSM agent into the AMI or
install it as the worker node is being bootstrapped. If you rather not
modify the Optimized AMI or the ASG’s launch template, you can install
the SSM agent with a DaemonSet as in
https://github.com/aws-samples/ssm-agent-daemonset-installer[this
example].

==== Minimal IAM policy for SSM based SSH Access

The `+AmazonSSMManagedInstanceCore+` AWS managed policy contains a
number of permissions that are not required for SSM Session Manager /
SSM RunCommand if you’re just looking to avoid SSH access. Of concern
specifically is the `+*+` permissions for `+ssm:GetParameter(s)+` which
would allow for the role to access all parameters in Parameter Store
(including SecureStrings with the AWS managed KMS key configured).

The following IAM policy contains the minimal set of permissions to
enable node access via SSM Systems Manager.

[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "EnableAccessViaSSMSessionManager",
      "Effect": "Allow",
      "Action": [
        "ssmmessages:OpenDataChannel",
        "ssmmessages:OpenControlChannel",
        "ssmmessages:CreateDataChannel",
        "ssmmessages:CreateControlChannel",
        "ssm:UpdateInstanceInformation"
      ],
      "Resource": "*"
    },
    {
      "Sid": "EnableSSMRunCommand",
      "Effect": "Allow",
      "Action": [
        "ssm:UpdateInstanceInformation",
        "ec2messages:SendReply",
        "ec2messages:GetMessages",
        "ec2messages:GetEndpoint",
        "ec2messages:FailMessage",
        "ec2messages:DeleteMessage",
        "ec2messages:AcknowledgeMessage"
      ],
      "Resource": "*"
    }
  ]
}
----

With this policy in place and the
https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html[Session
Manager plugin] installed, you can then run

[source,bash]
----
aws ssm start-session --target [INSTANCE_ID_OF_EKS_NODE]
----

to access the node.

!!! note You may also want to consider adding permissions to
https://docs.aws.amazon.com/systems-manager/latest/userguide/getting-started-create-iam-instance-profile.html#create-iam-instance-profile-ssn-logging[enable
Session Manager logging].

=== Deploy workers onto private subnets

By deploying workers onto private subnets, you minimize their exposure
to the Internet where attacks often originate. Beginning April 22, 2020,
the assignment of public IP addresses to nodes in a managed node groups
will be controlled by the subnet they are deployed onto. Prior to this,
nodes in a Managed Node Group were automatically assigned a public IP.
If you choose to deploy your worker nodes on to public subnets,
implement restrictive AWS security group rules to limit their exposure.

=== Run Amazon Inspector to assess hosts for exposure, vulnerabilities, and deviations from best practices

You can use
https://docs.aws.amazon.com/inspector/latest/user/what-is-inspector.html[Amazon
Inspector] to check for unintended network access to your nodes and for
vulnerabilities on the underlying Amazon EC2 instances.

Amazon Inspector can provide common vulnerabilities and exposures (CVE)
data for your Amazon EC2 instances only if the Amazon EC2 Systems
Manager (SSM) agent is installed and enabled. This agent is preinstalled
on several
https://docs.aws.amazon.com/systems-manager/latest/userguide/ami-preinstalled-agent.html[Amazon
Machine Images (AMIs)] including
https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html[EKS
optimized Amazon Linux AMIs]. Regardless of SSM agent status, all of
your Amazon EC2 instances are scanned for network reachability issues.
For more information about configuring scans for Amazon EC2, see
https://docs.aws.amazon.com/inspector/latest/user/enable-disable-scanning-ec2.html[Scanning
Amazon EC2 instances].

!!! attention Inspector cannot be run on the infrastructure used to run
Fargate pods.

== Alternatives

=== Run SELinux

!!! info Available on Red Hat Enterprise Linux (RHEL), CentOS,
Bottlerocket, and Amazon Linux 2023

SELinux provides an additional layer of security to keep containers
isolated from each other and from the host. SELinux allows
administrators to enforce mandatory access controls (MAC) for every
user, application, process, and file. Think of it as a backstop that
restricts the operations that can be performed against to specific
resources based on a set of labels. On EKS, SELinux can be used to
prevent containers from accessing each other’s resources.

Container SELinux policies are defined in the
https://github.com/containers/container-selinux[container-selinux]
package. Docker CE requires this package (along with its dependencies)
so that the processes and files created by Docker (or other container
runtimes) run with limited system access. Containers leverage the
`+container_t+` label which is an alias to `+svirt_lxc_net_t+`. These
policies effectively prevent containers from accessing certain features
of the host.

When you configure SELinux for Docker, Docker automatically labels
workloads `+container_t+` as a type and gives each container a unique
MCS level. This will isolate containers from one another. If you need
looser restrictions, you can create your own profile in SElinux which
grants a container permissions to specific areas of the file system.
This is similar to PSPs in that you can create different profiles for
different containers/pods. For example, you can have a profile for
general workloads with a set of restrictive controls and another for
things that require privileged access.

SELinux for Containers has a set of options that can be configured to
modify the default restrictions. The following SELinux Booleans can be
enabled or disabled based on your needs:

[width="100%",cols="30%,^40%,30%",options="header",]
|===
|Boolean |Default |Description
|`+container_connect_any+` |`+off+` |Allow containers to access
privileged ports on the host. For example, if you have a container that
needs to map ports to 443 or 80 on the host.

|`+container_manage_cgroup+` |`+off+` |Allow containers to manage cgroup
configuration. For example, a container running systemd will need this
to be enabled.

|`+container_use_cephfs+` |`+off+` |Allow containers to use a ceph file
system.
|===

By default, containers are allowed to read/execute under `+/usr+` and
read most content from `+/etc+`. The files under `+/var/lib/docker+` and
`+/var/lib/containers+` have the label `+container_var_lib_t+`. To view
a full list of default, labels see the
https://github.com/containers/container-selinux/blob/master/container.fc[container.fc]
file.

[source,bash]
----
docker container run -it \
  -v /var/lib/docker/image/overlay2/repositories.json:/host/repositories.json \
  centos:7 cat /host/repositories.json
# cat: /host/repositories.json: Permission denied

docker container run -it \
  -v /etc/passwd:/host/etc/passwd \
  centos:7 cat /host/etc/passwd
# cat: /host/etc/passwd: Permission denied
----

Files labeled with `+container_file_t+` are the only files that are
writable by containers. If you want a volume mount to be writeable, you
will needed to specify `+:z+` or `+:Z+` at the end.

* `+:z+` will re-label the files so that the container can read/write
* `+:Z+` will re-label the files so that *only* the container can
read/write

[source,bash]
----
ls -Z /var/lib/misc
# -rw-r--r--. root root system_u:object_r:var_lib_t:s0   postfix.aliasesdb-stamp

docker container run -it \
  -v /var/lib/misc:/host/var/lib/misc:z \
  centos:7 echo "Relabeled!"

ls -Z /var/lib/misc
#-rw-r--r--. root root system_u:object_r:container_file_t:s0 postfix.aliasesdb-stamp
----

[source,bash]
----
docker container run -it \
  -v /var/log:/host/var/log:Z \
  fluentbit:latest
----

In Kubernetes, relabeling is slightly different. Rather than having
Docker automatically relabel the files, you can specify a custom MCS
label to run the pod. Volumes that support relabeling will automatically
be relabeled so that they are accessible. Pods with a matching MCS label
will be able to access the volume. If you need strict isolation, set a
different MCS label for each pod.

[source,yaml]
----
securityContext:
  seLinuxOptions:
    # Provide a unique MCS label per container
    # You can specify user, role, and type also
    # enforcement based on type and level (svert)
    level: s0:c144:c154
----

In this example `+s0:c144:c154+` corresponds to an MCS label assigned to
a file that the container is allowed to access.

On EKS you could create policies that allow for privileged containers to
run, like FluentD and create an SELinux policy to allow it to read from
/var/log on the host without needing to relabel the host directory. Pods
with the same label will be able to access the same host volumes.

We have implemented
https://github.com/aws-samples/amazon-eks-custom-amis[sample AMIs for
Amazon EKS] that have SELinux configured on CentOS 7 and RHEL 7. These
AMIs were developed to demonstrate sample implementations that meet
requirements of highly regulated customers, such as STIG, CJIS, and C2S.

!!! caution SELinux will ignore containers where the type is unconfined.

== Tools and resources

* https://platform9.com/blog/selinux-kubernetes-rbac-and-shipping-security-policies-for-on-prem-applications/[SELinux
Kubernetes RBAC and Shipping Security Policies for On-prem Applications]
* https://jayunit100.blogspot.com/2019/07/iterative-hardening-of-kubernetes-and.html[Iterative
Hardening of Kubernetes]
* https://linux.die.net/man/1/audit2allow[Audit2Allow]
* https://linux.die.net/man/8/sealert[SEAlert]
* https://www.redhat.com/en/blog/generate-selinux-policies-containers-with-udica[Generate
SELinux policies for containers with Udica] describes a tool that looks
at container spec files for Linux capabilities, ports, and mount points,
and generates a set of SELinux rules that allow the container to run
properly
* https://github.com/aws-samples/amazon-eks-custom-amis#hardening[AMI
Hardening] playbooks for hardening the OS to meet different regulatory
requirements
* https://github.com/keikoproj/upgrade-manager[Keiko Upgrade Manager] an
open source project from Intuit that orchestrates the rotation of worker
nodes.
* https://sysdig.com/products/kubernetes-security/[Sysdig Secure]
* https://eksctl.io/[eksctl]

:leveloffset: 1
:leveloffset: +1

= Compliance

Compliance is a shared responsibility between AWS and the consumers of
its services. Generally speaking, AWS is responsible for "`security of
the cloud`" whereas its users are responsible for "`security in the
cloud.`" The line that delineates what AWS and its users are responsible
for will vary depending on the service. For example, with Fargate, AWS
is responsible for managing the physical security of its data centers,
the hardware, the virtual infrastructure (Amazon EC2), and the container
runtime (Docker). Users of Fargate are responsible for securing the
container image and their application. Knowing who is responsible for
what is an important consideration when running workloads that must
adhere to compliance standards.

The following table shows the compliance programs with which the
different container services conform.

[width="99%",cols="30%,^17%,^17%,^19%,^17%",options="header",]
|===
|Compliance Program |Amazon ECS Orchestrator |Amazon EKS Orchestrator
|ECS Fargate |Amazon ECR
|PCI DSS Level 1 |1 |1 |1 |1

|HIPAA Eligible |1 |1 |1 |1

|SOC I |1 |1 |1 |1

|SOC II |1 |1 |1 |1

|SOC III |1 |1 |1 |1

|ISO 27001:2013 |1 |1 |1 |1

|ISO 9001:2015 |1 |1 |1 |1

|ISO 27017:2015 |1 |1 |1 |1

|ISO 27018:2019 |1 |1 |1 |1

|IRAP |1 |1 |1 |1

|FedRAMP Moderate (East/West) |1 |1 |0 |1

|FedRAMP High (GovCloud) |1 |1 |0 |1

|DOD CC SRG |1 |DISA Review (IL5) |0 |1

|HIPAA BAA |1 |1 |1 |1

|MTCS |1 |1 |0 |1

|C5 |1 |1 |0 |1

|K-ISMS |1 |1 |0 |1

|ENS High |1 |1 |0 |1

|OSPAR |1 |1 |0 |1

|HITRUST CSF |1 |1 |1 |1
|===

Compliance status changes over time. For the latest status, always refer
to https://aws.amazon.com/compliance/services-in-scope/.

For further information about cloud accreditation models and best
practices, see the AWS whitepaper,
https://d1.awsstatic.com/whitepapers/accreditation-models-for-secure-cloud-adoption.pdf[Accreditation
Models for Secure Cloud Adoption]

== Shifting Left

The concept of shifting left involves catching policy violations and
errors earlier in the software development lifecycle. From a security
perspective, this can be very beneficial. A developer, for example, can
fix issues with their configuration before their application is deployed
to the cluster. Catching mistakes like this earlier will help prevent
configurations that violate your policies from being deployed.

=== Policy as Code

Policy can be thought of as a set of rules for governing behaviors,
i.e. behaviors that are allowed or those that are prohibited. For
example, you may have a policy that says that all Dockerfiles should
include a USER directive that causes the container to run as a non-root
user. As a document, a policy like this can be hard to discover and
enforce. It may also become outdated as your requirements change. With
Policy as Code (PaC) solutions, you can automate security, compliance,
and privacy controls that detect, prevent, reduce, and counteract known
and persistent threats. Furthermore, they give you mechanism to codify
your policies and manage them as you do other code artifacts. The
benefit of this approach is that you can reuse your DevOps and GitOps
strategies to manage and consistently apply policies across fleets of
Kubernetes clusters. Please refer to
https://aws.github.io/aws-eks-best-practices/security/docs/pods/#pod-security[Pod
Security] for information about PaC options and the future of PSPs.

=== Use policy-as-code tools in pipelines to detect violations before deployment

* https://www.openpolicyagent.org/[OPA] is an open source policy engine
that’s part of the CNCF. It’s used for making policy decisions and can
be run a variety of different ways, e.g. as a language library or a
service. OPA policies are written in a Domain Specific Language (DSL)
called Rego. While it is often run as part of a Kubernetes Dynamic
Admission Controller as the
https://github.com/open-policy-agent/gatekeeper[Gatekeeper] project, OPA
can also be incorporated into your CI/CD pipeline. This allows
developers to get feedback about their configuration earlier in the
release cycle which can subsequently help them resolve issues before
they get to production. A collection of common OPA policies can be found
in the GitHub
https://github.com/aws/aws-eks-best-practices/tree/master/policies/opa[repository]
for this project.
* https://github.com/open-policy-agent/conftest[Conftest] is built on
top of OPA and it provides a developer focused experience for testing
Kubernetes configuration.
* https://kyverno.io/[Kyverno] is a policy engine designed for
Kubernetes. With Kyverno, policies are managed as Kubernetes resources
and no new language is required to write policies. This allows using
familiar tools such as kubectl, git, and kustomize to manage policies.
Kyverno policies can validate, mutate, and generate Kubernetes resources
plus ensure OCI image supply chain security. The
https://kyverno.io/docs/kyverno-cli/[Kyverno CLI] can be used to test
policies and validate resources as part of a CI/CD pipeline. All the
Kyverno community policies can be found on the
https://kyverno.io/policies/[Kyverno website], and for examples using
the Kyverno CLI to write tests in pipelines, see the
https://github.com/kyverno/policies[policies repository].

== Tools and resources

* https://catalog.workshops.aws/eks-security-immersionday/en-US/10-regulatory-compliance[Amazon
EKS Security Immersion Workshop - Regulatory Compliance]
* https://github.com/aquasecurity/kube-bench[kube-bench]
* https://github.com/docker/docker-bench-security[docker-bench-security]
* https://aws.amazon.com/inspector/[AWS Inspector]
* https://github.com/kubernetes/community/blob/master/sig-security/security-audit-2019/findings/Kubernetes%20Final%20Report.pdf[Kubernetes
Security Review] A 3rd party security assessment of Kubernetes 1.13.4
(2019)
* https://www.suse.com/neuvector/[NeuVector by SUSE] open source,
zero-trust container security platform, provides compliance reporting
and custom compliance checks

:leveloffset: 1
:leveloffset: +1

= Incident response and forensics

Your ability to react quickly to an incident can help minimize damage
caused from a breach. Having a reliable alerting system that can warn
you of suspicious behavior is the first step in a good incident response
plan. When an incident does arise, you have to quickly decide whether to
destroy and replace the effected container, or isolate and inspect the
container. If you choose to isolate the container as part of a forensic
investigation and root cause analysis, then the following set of
activities should be followed:

== Sample incident response plan

=== Identify the offending Pod and worker node

Your first course of action should be to isolate the damage. Start by
identifying where the breach occurred and isolate that Pod and its node
from the rest of the infrastructure.

=== Identify the offending Pods and worker nodes using workload name

If you know the name and namespace of the offending pod, you can
identify the the worker node running the pod as follows:

[source,bash]
----
kubectl get pods &lt;name&gt; --namespace &lt;namespace&gt; -o=jsonpath='{.spec.nodeName}{"\n"}'
----

If a https://kubernetes.io/docs/concepts/workloads/controllers/[Workload
Resource] such as a Deployment has been compromised, it is likely that
all the pods that are part of the workload resource are compromised. Use
the following command to list all the pods of the Workload Resource and
the nodes they are running on:

[source,bash]
----
selector=$(kubectl get deployments &lt;name&gt; \
 --namespace &lt;namespace&gt; -o json | jq -j \
'.spec.selector.matchLabels | to_entries | .[] | "\(.key)=\(.value)"')

kubectl get pods --namespace &lt;namespace&gt; --selector=$selector \
-o json | jq -r '.items[] | "\(.metadata.name) \(.spec.nodeName)"'
----

The above command is for deployments. You can run the same command for
other workload resources such as replicasets,, statefulsets, etc.

=== Identify the offending Pods and worker nodes using service account name

In some cases, you may identify that a service account is compromised.
It is likely that pods using the identified service account are
compromised. You can identify all the pods using the service account and
nodes they are running on with the following command:

[source,bash]
----
kubectl get pods -o json --namespace &lt;namespace&gt; | \
    jq -r '.items[] |
    select(.spec.serviceAccount == "&lt;service account name&gt;") |
    "\(.metadata.name) \(.spec.nodeName)"'
----

=== Identify Pods with vulnerable or compromised images and worker nodes

In some cases, you may discover that a container image being used in
pods on your cluster is malicious or compromised. A container image is
malicious or compromised, if it was found to contain malware, is a known
bad image or has a CVE that has been exploited. You should consider all
the pods using the container image compromised. You can identify the
pods using the image and nodes they are running on with the following
command:

[source,bash]
----
IMAGE=&lt;Name of the malicious/compromised image&gt;

kubectl get pods -o json --all-namespaces | \
    jq -r --arg image "$IMAGE" '.items[] |
    select(.spec.containers[] | .image == $image) |
    "\(.metadata.name) \(.metadata.namespace) \(.spec.nodeName)"'
----

=== Isolate the Pod by creating a Network Policy that denies all ingress and egress traffic to the pod

A deny all traffic rule may help stop an attack that is already underway
by severing all connections to the pod. The following Network Policy
will apply to a pod with the label `+app=web+`.

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
  - Ingress
  - Egress
----

!!! attention A Network Policy may prove ineffective if an attacker has
gained access to underlying host. If you suspect that has happened, you
can use
https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html[AWS
Security Groups] to isolate a compromised host from other hosts. When
changing a host’s security group, be aware that it will impact all
containers running on that host.

=== Revoke temporary security credentials assigned to the pod or worker node if necessary

If the worker node has been assigned an IAM role that allows Pods to
gain access to other AWS resources, remove those roles from the instance
to prevent further damage from the attack. Similarly, if the Pod has
been assigned an IAM role, evaluate whether you can safely remove the
IAM policies from the role without impacting other workloads.

=== Cordon the worker node

By cordoning the impacted worker node, you’re informing the scheduler to
avoid scheduling pods onto the affected node. This will allow you to
remove the node for forensic study without disrupting other workloads.

!!! info This guidance is not applicable to Fargate where each Fargate
pod run in its own sandboxed environment. Instead of cordoning,
sequester the affected Fargate pods by applying a network policy that
denies all ingress and egress traffic.

=== Enable termination protection on impacted worker node

An attacker may attempt to erase their misdeeds by terminating an
affected node. Enabling
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#Using_ChangingDisableAPITermination[termination
protection] can prevent this from happening.
https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#instance-protection[Instance
scale-in protection] will protect the node from a scale-in event.

!!! warning You cannot enable termination protection on a Spot instance.

=== Label the offending Pod/Node with a label indicating that it is part of an active investigation

This will serve as a warning to cluster administrators not to tamper
with the affected Pods/Nodes until the investigation is complete.

=== Capture volatile artifacts on the worker node

* *Capture the operating system memory*. This will capture the Docker
daemon (or other container runtime) and its subprocesses per container.
This can be accomplished using tools like
https://github.com/504ensicsLabs/LiME[LiME] and
https://www.volatilityfoundation.org/[Volatility], or through
higher-level tools such as
https://aws.amazon.com/solutions/implementations/automated-forensics-orchestrator-for-amazon-ec2/[Automated
Forensics Orchestrator for Amazon EC2] that build on top of them.
* *Perform a netstat tree dump of the processes running and the open
ports*. This will capture the docker daemon and its subprocess per
container.
* *Run commands to save container-level state before evidence is
altered*. You can use capabilities of the container runtime to capture
information about currently running containers. For example, with
Docker, you could do the following:
** `+docker top CONTAINER+` for processes running.
** `+docker logs CONTAINER+` for daemon level held logs.
** `+docker inspect CONTAINER+` for various information about the
container.
+
The same could be achieved with containerd using the
https://github.com/containerd/nerdctl[nerdctl] CLI, in place of
`+docker+` (e.g. `+nerdctl inspect+`). Some additional commands are
available depending on the container runtime. For example, Docker has
`+docker diff+` to see changes to the container filesystem or
`+docker checkpoint+` to save all container state including volatile
memory (RAM). See
https://kubernetes.io/blog/2022/12/05/forensic-container-checkpointing-alpha/[this
Kubernetes blog post] for discussion of similar capabilities with
containerd or CRI-O runtimes.
* *Pause the container for forensic capture*.
* *Snapshot the instance’s EBS volumes*.

=== Redeploy compromised Pod or Workload Resource

Once you have gathered data for forensic analysis, you can redeploy the
compromised pod or workload resource.

First roll out the fix for the vulnerability that was compromised and
start new replacement pods. Then delete the vulnerable pods.

If the vulnerable pods are managed by a higher-level Kubernetes workload
resource (for example, a Deployment or DaemonSet), deleting them will
schedule new ones. So vulnerable pods will be launched again. In that
case you should deploy a new replacement workload resource after fixing
the vulnerability. Then you should delete the vulnerable workload.

== Recommendations

=== Review the AWS Security Incident Response Whitepaper

While this section gives a brief overview along with a few
recommendations for handling suspected security breaches, the topic is
exhaustively covered in the white paper,
https://docs.aws.amazon.com/whitepapers/latest/aws-security-incident-response-guide/welcome.html[AWS
Security Incident Response].

=== Practice security game days

Divide your security practitioners into 2 teams: red and blue. The red
team will be focused on probing different systems for vulnerabilities
while the blue team will be responsible for defending against them. If
you don’t have enough security practitioners to create separate teams,
consider hiring an outside entity that has knowledge of Kubernetes
exploits.

https://github.com/cyberark/kubesploit[Kubesploit] is a penetration
testing framework from CyberArk that you can use to conduct game days.
Unlike other tools which scan your cluster for vulnerabilities,
kubesploit simulates a real-world attack. This gives your blue team an
opportunity to practice its response to an attack and gauge its
effectiveness.

=== Run penetration tests against your cluster

Periodically attacking your own cluster can help you discover
vulnerabilities and misconfigurations. Before getting started, follow
the https://aws.amazon.com/security/penetration-testing/[penetration
test guidelines] before conducting a test against your cluster.

== Tools and resources

* https://github.com/aquasecurity/kube-hunter[kube-hunter], a
penetration testing tool for Kubernetes.
* https://www.gremlin.com/product/#kubernetes[Gremlin], a chaos
engineering toolkit that you can use to simulate attacks against your
applications and infrastructure.
* https://github.com/kubernetes/sig-security/blob/main/sig-security-external-audit/security-audit-2019/findings/AtredisPartners_Attacking_Kubernetes-v1.0.pdf[Attacking
and Defending Kubernetes Installations]
* https://www.cyberark.com/resources/threat-research-blog/kubesploit-a-new-offensive-tool-for-testing-containerized-environments[kubesploit]
* https://www.suse.com/neuvector/[NeuVector by SUSE] open source,
zero-trust container security platform, provides vulnerability- and risk
reporting as well as security event notification
* https://www.youtube.com/watch?v=CH7S5rE3j8w[Advanced Persistent
Threats]
* https://www.youtube.com/watch?v=LtCx3zZpOfs[Kubernetes Practical
Attack and Defense]
* https://www.youtube.com/watch?v=1LMo0CftVC4[Compromising Kubernetes
Cluster by Exploiting RBAC Permissions]

:leveloffset: 1
:leveloffset: +1

= Image security

You should consider the container image as your first line of defense
against an attack. An insecure, poorly constructed image can allow an
attacker to escape the bounds of the container and gain access to the
host. Once on the host, an attacker can gain access to sensitive
information or move laterally within the cluster or with your AWS
account. The following best practices will help mitigate risk of this
happening.

== Recommendations

=== Create minimal images

Start by removing all extraneous binaries from the container image. If
you’re using an unfamiliar image from Dockerhub, inspect the image using
an application like https://github.com/wagoodman/dive[Dive] which can
show you the contents of each of the container’s layers. Remove all
binaries with the SETUID and SETGID bits as they can be used to escalate
privilege and consider removing all shells and utilities like nc and
curl that can be used for nefarious purposes. You can find the files
with SETUID and SETGID bits with the following command:

[source,bash]
----
find / -perm /6000 -type f -exec ls -ld {} \;
----

To remove the special permissions from these files, add the following
directive to your container image:

[source,docker]
----
RUN find / -xdev -perm /6000 -type f -exec chmod a-s {} \; || true
----

Colloquially, this is known as de-fanging your image.

=== Use multi-stage builds

Using multi-stage builds is a way to create minimal images. Oftentimes,
multi-stage builds are used to automate parts of the Continuous
Integration cycle. For example, multi-stage builds can be used to lint
your source code or perform static code analysis. This affords
developers an opportunity to get near immediate feedback instead of
waiting for a pipeline to execute. Multi-stage builds are attractive
from a security standpoint because they allow you to minimize the size
of the final image pushed to your container registry. Container images
devoid of build tools and other extraneous binaries improves your
security posture by reducing the attack surface of the image. For
additional information about multi-stage builds, see
https://docs.docker.com/develop/develop-images/multistage-build/[Docker’s
multi-stage builds documentation].

=== Create Software Bill of Materials (SBOMs) for your container image

A "`software bill of materials`" (SBOM) is a nested inventory of the
software artifacts that make up your container image. SBOM is a key
building block in software security and software supply chain risk
management. https://anchore.com/sbom/[Generating&amp;#44; storing SBOMS in a
central repository and scanning SBOMs for vulnerabilities] helps address
the following concerns:

* *Visibility*: understand what components make up your container image.
Storing in a central repository allows SBOMs to be audited and scanned
anytime, even post deployment to detect and respond to new
vulnerabilities such as zero day vulnerabilities.
* *Provenance Verification*: assurance that existing assumptions of
where and how an artifact originates from are true and that the artifact
or its accompanying metadata have not been tampered with during the
build or delivery processes.
* *Trustworthiness*: assurance that a given artifact and its contents
can be trusted to do what it is purported to do, i.e. is suitable for a
purpose. This involves judgement on whether the code is safe to execute
and making informed decisions about the risks associated with executing
the code. Trustworthiness is assured by creating an attested pipeline
execution report along with attested SBOM and attested CVE scan report
to assure the consumers of the image that this image is in-fact created
through secure means (pipeline) with secure components.
* *Dependency Trust Verification*: recursive checking of an artifact’s
dependency tree for trustworthiness and provenance of the artifacts it
uses. Drift in SBOMs can help detect malicious activity including
unauthorized, untrusted dependencies, infiltration attempts.

The following tools can be used to generate SBOM:

* https://docs.aws.amazon.com/inspector[Amazon Inspector] can be used to
https://docs.aws.amazon.com/inspector/latest/user/sbom-export.html[create
and export SBOMs].
* https://github.com/anchore/syft[Syft from Anchore] can also be used
for SBOM generation. For quicker vulnerability scans, the SBOM generated
for a container image can be used as an input to scan. The SBOM and scan
report are then
https://github.com/sigstore/cosign/blob/main/doc/cosign_attach_attestation.md[attested
and attached] to the image before pushing the image to a central OCI
repository such as Amazon ECR for review and audit purposes.

Learn more about securing your software supply chain by reviewing
https://project.linuxfoundation.org/hubfs/CNCF_SSCP_v1.pdf[CNCF Software
Supply Chain Best Practices guide].

=== Scan images for vulnerabilities regularly

Like their virtual machine counterparts, container images can contain
binaries and application libraries with vulnerabilities or develop
vulnerabilities over time. The best way to safeguard against exploits is
by regularly scanning your images with an image scanner. Images that are
stored in Amazon ECR can be scanned on push or on-demand (once during a
24 hour period). ECR currently supports
https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html[two
types of scanning - Basic and Enhanced]. Basic scanning leverages
https://github.com/quay/clair[Clair] an open source image scanning
solution for no cost.
https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html[Enhanced
scanning] uses Amazon Inspector to provide automatic continuous scans
for https://aws.amazon.com/inspector/pricing/[additional cost]. After an
image is scanned, the results are logged to the event stream for ECR in
EventBridge. You can also see the results of a scan from within the ECR
console. Images with a HIGH or CRITICAL vulnerability should be deleted
or rebuilt. If an image that has been deployed develops a vulnerability,
it should be replaced as soon as possible.

Knowing where images with vulnerabilities have been deployed is
essential to keeping your environment secure. While you could
conceivably build an image tracking solution yourself, there are already
several commercial offerings that provide this and other advanced
capabilities out of the box, including:

* https://github.com/anchore/grype[Grype]
* https://docs.paloaltonetworks.com/prisma/prisma-cloud/prisma-cloud-admin-compute/tools/twistcli_scan_images[Palo
Alto - Prisma Cloud (twistcli)]
* https://www.aquasec.com/[Aqua]
* https://github.com/Portshift/kubei[Kubei]
* https://github.com/aquasecurity/trivy[Trivy]
* https://support.snyk.io/hc/en-us/articles/360003946917-Test-images-with-the-Snyk-Container-CLI[Snyk]

A Kubernetes validation webhook could also be used to validate that
images are free of critical vulnerabilities. Validation webhooks are
invoked prior to the Kubernetes API. They are typically used to reject
requests that don’t comply with the validation criteria defined in the
webhook.
https://aws.amazon.com/blogs/containers/building-serverless-admission-webhooks-for-kubernetes-with-aws-sam/[This]
is an example of a serverless webhook that calls the ECR
describeImageScanFindings API to determine whether a pod is pulling an
image with critical vulnerabilities. If vulnerabilities are found, the
pod is rejected and a message with list of CVEs is returned as an Event.

=== Use attestations to validate artifact integrity

An attestation is a cryptographically signed "`statement`" that claims
something - a "`predicate`" e.g. a pipeline run or the SBOM or the
vulnerability scan report is true about another thing - a "`subject`"
i.e. the container image.

Attestations help users to validate that an artifact comes from a
trusted source in the software supply chain. As an example, we may use a
container image without knowing all the software components or
dependencies that are included in that image. However, if we trust
whatever the producer of the container image says about what software is
present, we can use the producer’s attestation to rely on that artifact.
This means that we can proceed to use the artifact safely in our
workflow in place of having done the analysis ourself.

* Attestations can be created using
https://docs.aws.amazon.com/signer/latest/developerguide/Welcome.html[AWS
Signer] or
https://github.com/sigstore/cosign/blob/main/doc/cosign_attest.md[Sigstore
cosign].
* Kubernetes admission controllers such as https://kyverno.io/[Kyverno]
can be used to
https://kyverno.io/docs/writing-policies/verify-images/sigstore/[verify
attestations].
* Refer to this
https://catalog.us-east-1.prod.workshops.aws/workshops/49343bb7-2cc5-4001-9d3b-f6a33b3c4442/en-US/0-introduction[workshop]
to learn more about software supply chain management best practices on
AWS using open source tools with topics including creating and attaching
attestations to a container image.

=== Create IAM policies for ECR repositories

Nowadays, it is not uncommon for an organization to have multiple
development teams operating independently within a shared AWS account.
If these teams don’t need to share assets, you may want to create a set
of IAM policies that restrict access to the repositories each team can
interact with. A good way to implement this is by using ECR
https://docs.aws.amazon.com/AmazonECR/latest/userguide/Repositories.html#repository-concepts[namespaces].
Namespaces are a way to group similar repositories together. For
example, all of the registries for team A can be prefaced with the
team-a/ while those for team B can use the team-b/ prefix. The policy to
restrict access might look like the following:

[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowPushPull",
      "Effect": "Allow",
      "Action": [
        "ecr:GetDownloadUrlForLayer",
        "ecr:BatchGetImage",
        "ecr:BatchCheckLayerAvailability",
        "ecr:PutImage",
        "ecr:InitiateLayerUpload",
        "ecr:UploadLayerPart",
        "ecr:CompleteLayerUpload"
      ],
      "Resource": [
        "arn:aws:ecr:&lt;region&gt;:&lt;account_id&gt;:repository/team-a/*"
      ]
    }
  ]
}
----

=== Consider using ECR private endpoints

The ECR API has a public endpoint. Consequently, ECR registries can be
accessed from the Internet so long as the request has been authenticated
and authorized by IAM. For those who need to operate in a sandboxed
environment where the cluster VPC lacks an Internet Gateway (IGW), you
can configure a private endpoint for ECR. Creating a private endpoint
enables you to privately access the ECR API through a private IP address
instead of routing traffic across the Internet. For additional
information on this topic, see
https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html[Amazon
ECR interface VPC endpoints].

=== Implement endpoint policies for ECR

The default endpoint policy for allows access to all ECR repositories
within a region. This might allow an attacker/insider to exfiltrate data
by packaging it as a container image and pushing it to a registry in
another AWS account. Mitigating this risk involves creating an endpoint
policy that limits API access to ECR repositories. For example, the
following policy allows all AWS principles in your account to perform
all actions against your and only your ECR repositories:

[source,json]
----
{
  "Statement": [
    {
      "Sid": "LimitECRAccess",
      "Principal": "*",
      "Action": "*",
      "Effect": "Allow",
      "Resource": "arn:aws:ecr:&lt;region&gt;:&lt;account_id&gt;:repository/*"
    }
  ]
}
----

You can enhance this further by setting a condition that uses the new
`+PrincipalOrgID+` attribute which will prevent pushing/pulling of
images by an IAM principle that is not part of your AWS Organization.
See,
https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-principalorgid[aws:PrincipalOrgID]
for additional details. We recommended applying the same policy to both
the `+com.amazonaws.&lt;region&gt;.ecr.dkr+` and the
`+com.amazonaws.&lt;region&gt;.ecr.api+` endpoints. Since EKS pulls images for
kube-proxy, coredns, and aws-node from ECR, you will need to add the
account ID of the registry,
e.g. `+602401143452.dkr.ecr.us-west-2.amazonaws.com/*+` to the list of
resources in the endpoint policy or alter the policy to allow pulls from
“*” and restrict pushes to your account ID. The table below reveals the
mapping between the AWS accounts where EKS images are vended from and
cluster region.

[cols=",",options="header",]
|===
|Account Number |Region
|602401143452 |All commercial regions except for those listed below
|— |—
|800184023465 |ap-east-1 - Asia Pacific (Hong Kong)
|558608220178 |me-south-1 - Middle East (Bahrain)
|918309763551 |cn-north-1 - China (Beijing)
|961992271922 |cn-northwest-1 - China (Ningxia)
|===

For further information about using endpoint policies, see
https://aws.amazon.com/blogs/containers/using-vpc-endpoint-policies-to-control-amazon-ecr-access/[Using
VPC endpoint policies to control Amazon ECR access].

=== Implement lifecycle policies for ECR

The
https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf[NIST
Application Container Security Guide] warns about the risk of "`stale
images in registries`", noting that over time old images with
vulnerable, out-of-date software packages should be removed to prevent
accidental deployment and exposure. Each ECR repository can have a
lifecycle policy that sets rules for when images expire. The
https://docs.aws.amazon.com/AmazonECR/latest/userguide/LifecyclePolicies.html[AWS
official documentation] describes how to set up test rules, evaluate
them and then apply them. There are several
https://docs.aws.amazon.com/AmazonECR/latest/userguide/lifecycle_policy_examples.html[lifecycle
policy examples] in the official docs that show different ways of
filtering the images in a repository:

* Filtering by image age or count
* Filtering by tagged or untagged images
* Filtering by image tags, either in multiple rules or a single rule

???+ warning If the image for long running application is purged from
ECR, it can cause an image pull errors when the application is
redeployed or scaled horizontally. When using image lifecycle policies,
be sure you have good CI/CD practices in place to keep deployments and
the images that they reference up to date and always create [image]
expiry rules that account for how often you do releases/deployments.

=== Create a set of curated images

Rather than allowing developers to create their own images, consider
creating a set of vetted images for the different application stacks in
your organization. By doing so, developers can forego learning how to
compose Dockerfiles and concentrate on writing code. As changes are
merged into Master, a CI/CD pipeline can automatically compile the
asset, store it in an artifact repository and copy the artifact into the
appropriate image before pushing it to a Docker registry like ECR. At
the very least you should create a set of base images from which
developers to create their own Dockerfiles. Ideally, you want to avoid
pulling images from Dockerhub because 1/ you don’t always know what is
in the image and 2/ about
https://www.kennasecurity.com/blog/one-fifth-of-the-most-used-docker-containers-have-at-least-one-critical-vulnerability/[a
fifth] of the top 1000 images have vulnerabilities. A list of those
images and their vulnerabilities can be found
https://vulnerablecontainers.org/[here].

=== Add the USER directive to your Dockerfiles to run as a non-root user

As was mentioned in the pod security section, you should avoid running
container as root. While you can configure this as part of the podSpec,
it is a good habit to use the `+USER+` directive to your Dockerfiles.
The `+USER+` directive sets the UID to use when running `+RUN+`,
`+ENTRYPOINT+`, or `+CMD+` instruction that appears after the USER
directive.

=== Lint your Dockerfiles

Linting can be used to verify that your Dockerfiles are adhering to a
set of predefined guidelines, e.g. the inclusion of the `+USER+`
directive or the requirement that all images be tagged.
https://github.com/projectatomic/dockerfile_lint[dockerfile_lint] is an
open source project from RedHat that verifies common best practices and
includes a rule engine that you can use to build your own rules for
linting Dockerfiles. It can be incorporated into a CI pipeline, in that
builds with Dockerfiles that violate a rule will automatically fail.

=== Build images from Scratch

Reducing the attack surface of your container images should be primary
aim when building images. The ideal way to do this is by creating
minimal images that are devoid of binaries that can be used to exploit
vulnerabilities. Fortunately, Docker has a mechanism to create images
from
https://docs.docker.com/develop/develop-images/baseimages/#create-a-simple-parent-image-using-scratch[`+scratch+`].
With languages like Go, you can create a static linked binary and
reference it in your Dockerfile as in this example:

[source,docker]
----
############################
# STEP 1 build executable binary
############################
FROM golang:alpine AS builder# Install git.
# Git is required for fetching the dependencies.
RUN apk update &amp;&amp; apk add --no-cache gitWORKDIR $GOPATH/src/mypackage/myapp/COPY . . # Fetch dependencies.
# Using go get.
RUN go get -d -v# Build the binary.
RUN go build -o /go/bin/hello

############################
# STEP 2 build a small image
############################
FROM scratch# Copy our static executable.
COPY --from=builder /go/bin/hello /go/bin/hello# Run the hello binary.
ENTRYPOINT ["/go/bin/hello"]
----

This creates a container image that consists of your application and
nothing else, making it extremely secure.

=== Use immutable tags with ECR

https://aws.amazon.com/about-aws/whats-new/2019/07/amazon-ecr-now-supports-immutable-image-tags/[Immutable
tags] force you to update the image tag on each push to the image
repository. This can thwart an attacker from overwriting an image with a
malicious version without changing the image’s tags. Additionally, it
gives you a way to easily and uniquely identify an image.

=== Sign your images, SBOMs, pipeline runs and vulnerability reports

When Docker was first introduced, there was no cryptographic model for
verifying container images. With v2, Docker added digests to the image
manifest. This allowed an image’s configuration to be hashed and for the
hash to be used to generate an ID for the image. When image signing is
enabled, the Docker engine verifies the manifest’s signature, ensuring
that the content was produced from a trusted source and no tampering has
occurred. After each layer is downloaded, the engine verifies the digest
of the layer, ensuring that the content matches the content specified in
the manifest. Image signing effectively allows you to create a secure
supply chain, through the verification of digital signatures associated
with the image.

We can use
https://docs.aws.amazon.com/signer/latest/developerguide/Welcome.html[AWS
Signer] or https://github.com/sigstore/cosign[Sigstore Cosign], to sign
container images, create attestations for SBOMs, vulnerability scan
reports and pipeline run reports. These attestations assure the
trustworthiness and integrity of the image, that it is in fact created
by the trusted pipeline without any interference or tampering, and that
it contains only the software components that are documented (in the
SBOM) that is verified and trusted by the image publisher. These
attestations can be attached to the container image and pushed to the
repository.

In the next section we will see how to use the attested artifacts for
audits and admissions controller verification.

=== Image integrity verification using Kubernetes admission controller

We can verify image signatures, attested artifacts in an automated way
before deploying the image to target Kubernetes cluster using
https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/[dynamic
admission controller] and admit deployments only when the security
metadata of the artifacts comply with the admission controller policies.

For example we can write a policy that cryptographically verifies the
signature of an image, an attested SBOM, attested pipeline run report,
or attested CVE scan report. We can write conditions in the policy to
check data in the report, e.g. a CVE scan should not have any critical
CVEs. Deployment is allowed only for images that satisfy these
conditions and all other deployments will be rejected by the admissions
controller.

Examples of admission controller include:

* https://kyverno.io/[Kyverno]
* https://github.com/open-policy-agent/gatekeeper[OPA Gatekeeper]
* https://github.com/IBM/portieris[Portieris]
* https://github.com/deislabs/ratify[Ratify]
* https://github.com/grafeas/kritis[Kritis]
* https://github.com/kelseyhightower/grafeas-tutorial[Grafeas tutorial]
* https://github.com/Shopify/voucher[Voucher]

=== Update the packages in your container images

You should include RUN `+apt-get update &amp;&amp; apt-get upgrade+` in your
Dockerfiles to upgrade the packages in your images. Although upgrading
requires you to run as root, this occurs during image build phase. The
application doesn’t need to run as root. You can install the updates and
then switch to a different user with the USER directive. If your base
image runs as a non-root user, switch to root and back; don’t solely
rely on the maintainers of the base image to install the latest security
updates.

Run `+apt-get clean+` to delete the installer files from
`+/var/cache/apt/archives/+`. You can also run
`+rm -rf /var/lib/apt/lists/*+` after installing packages. This removes
the index files or the lists of packages that are available to install.
Be aware that these commands may be different for each package manager.
For example:

[source,docker]
----
RUN apt-get update &amp;&amp; apt-get install -y \
    curl \
    git \
    libsqlite3-dev \
    &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*
----

== Tools and resources

* https://catalog.workshops.aws/eks-security-immersionday/en-US/12-image-security[Amazon
EKS Security Immersion Workshop - Image Security]
* https://github.com/docker-slim/docker-slim[docker-slim] Build secure
minimal images
* https://github.com/goodwithtech/dockle[dockle] Verifies that your
Dockerfile aligns with best practices for creating secure images
* https://github.com/projectatomic/dockerfile_lint[dockerfile-lint] Rule
based linter for Dockerfiles
* https://github.com/hadolint/hadolint[hadolint] A smart dockerfile
linter
* https://github.com/open-policy-agent/gatekeeper[Gatekeeper and OPA] A
policy based admission controller
* https://kyverno.io/[Kyverno] A Kubernetes-native policy engine
* https://in-toto.io/[in-toto] Allows the user to verify if a step in
the supply chain was intended to be performed, and if the step was
performed by the right actor
* https://github.com/theupdateframework/notary[Notary] A project for
signing container images
* https://github.com/notaryproject/nv2[Notary v2]
* https://grafeas.io/[Grafeas] An open artifact metadata API to audit
and govern your software supply chain
* https://www.suse.com/neuvector/[NeuVector by SUSE] open source,
zero-trust container security platform, provides container, image and
registry scanning for vulnerabilities, secrets and compliance.

:leveloffset: 1
:leveloffset: +1

= Multi Account Strategy

AWS recommends using a
https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/organizing-your-aws-environment.html[multi
account strategy] and AWS organizations to help isolate and manage your
business applications and data. There are
https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/benefits-of-using-multiple-aws-accounts.html[many
benefits] to using a multi account strategy:

* Increased AWS API service quotas. Quotas are applied to AWS accounts,
and using multiple accounts for your workloads increases the overall
quota available to your workloads.
* Simpler Identity and Access Management (IAM) policies. Granting
workloads and the operators that support them access to only their own
AWS accounts means less time crafting fine-grained IAM policies to
achieve the principle of least privilege.
* Improved Isolation of AWS resources. By design, all resources
provisioned within an account are logically isolated from resources
provisioned in other accounts. This isolation boundary provides you with
a way to limit the risks of an application-related issue,
misconfiguration, or malicious actions. If an issue occurs within one
account, impacts to workloads contained in other accounts can be either
reduced or eliminated.
* More benefits, as described in the
https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/benefits-of-using-multiple-aws-accounts.html#group-workloads-based-on-business-purpose-and-ownership[AWS
Multi Account Strategy Whitepaper]

The following sections will explain how to implement a multi account
strategy for your EKS workloads using either a centralized, or
de-centralized EKS cluster approach.

== Planning for a Multi Workload Account Strategy for Multi Tenant Clusters

In a multi account AWS strategy, resources that belong to a given
workload such as S3 buckets, ElastiCache clusters and DynamoDB Tables
are all created in an AWS account that contains all the resources for
that workload. These are referred to as a workload account, and the EKS
cluster is deployed into an account referred to as the cluster account.
Cluster accounts will be explored in the next section. Deploying
resources into a dedicated workload account is similar to deploying
kubernetes resources into a dedicated namespace.

Workload accounts can then be further broken down by software
development lifecycle or other requirements if appropriate. For example
a given workload can have a production account, a development account,
or accounts for hosting instances of that workload in a specific region.
https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/organizing-workload-oriented-ous.html[More
information] is available in this AWS whitepaper.

You can adopt the following approaches when implementing EKS Multi
account strategy:

== Centralized EKS Cluster

In this approach, your EKS Cluster will be deployed in a single AWS
account called the `+Cluster Account+`. Using
https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[IAM
roles for Service Accounts (IRSA)] or
https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html[EKS
Pod Identities] to deliver temporary AWS credentials and
https://aws.amazon.com/ram/[AWS Resource Access Manager (RAM)] to
simplify network access, you can adopt a multi account strategy for your
multi tenant EKS cluster. The cluster account will contain the VPC,
subnets, EKS cluster, EC2/Fargate compute resources (worker nodes), and
any additional networking configurations needed to run your EKS cluster.

In a multi workload account strategy for multi tenant cluster, AWS
accounts typically align with
https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/[kubernetes
namespaces] as a mechanism for isolating groups of resources.
link:/security/docs/multitenancy/[Best practices for tenant isolation]
within an EKS cluster should still be followed when implementing a multi
account strategy for multi tenant EKS clusters.

It is possible to have multiple `+Cluster Accounts+` in your AWS
organization, and it is a best practice to have multiple
`+Cluster Accounts+` that align with your software development lifecycle
needs. For workloads operating at a very large scale, you may require
multiple `+Cluster Accounts+` to ensure that there are enough kubernetes
and AWS service quotas available to all your workloads.

[width="100%",cols="^100%",options="header",]
|===
|image:./images/multi-account-eks.jpg[multi-account-eks]
|In the above diagram, AWS RAM is used to share subnets from a cluster
account into a workload account. Then workloads running in EKS pods use
IRSA or EKS Pod Identities and role chaining to assume a role in their
workload account and access their AWS resources.
|===

=== Implementing a Multi Workload Account Strategy for Multi Tenant Cluster

==== Sharing Subnets With AWS Resource Access Manager

https://aws.amazon.com/ram/[AWS Resource Access Manager] (RAM) allows
you to share resources across AWS accounts.

If
https://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html#getting-started-sharing-orgs[RAM
is enabled for your AWS Organization], you can share the VPC Subnets
from the Cluster account to your workload accounts. This will allow AWS
resources owned by your workload accounts, such as
https://aws.amazon.com/elasticache/[Amazon ElastiCache] Clusters or
https://aws.amazon.com/rds/[Amazon Relational Database Service (RDS)]
Databases to be deployed into the same VPC as your EKS cluster, and be
consumable by the workloads running on your EKS cluster.

To share a resource via RAM, open up RAM in the AWS console of the
cluster account and select "`Resource Shares`" and "`Create Resource
Share`". Name your Resource Share and Select the subnets you want to
share. Select Next again and enter the 12 digit account IDs for the
workload accounts you wish to share the subnets with, select next again,
and click Create resource share to finish. After this step, the workload
account can deploy resources into those subnets.

RAM shares can also be created programmatically, or with infrastructure
as code.

==== Choosing Between EKS Pod Identities and IRSA

At re:Invent 2023, AWS launched EKS Pod Identities as a simpler way of
delivering temporary AWS credentials to your pods on EKS. Both IRSA and
EKS Pod Identities are valid methods for delivering temporary AWS
credentials to your EKS pods and will continue to be supported. You
should consider which method of delivering best meets your needs.

When working with a EKS cluster and multiple AWS accounts, IRSA can
directly assume roles in AWS accounts other than the account the EKS
cluster is hosted in directly, while EKS Pod identities require you to
configure role chaining. Refer
https://docs.aws.amazon.com/eks/latest/userguide/service-accounts.html#service-accounts-iam[EKS
documentation] for an in-depth comparison.

===== Accessing AWS API Resources with IAM Roles For Service Accounts

https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[IAM
Roles for Service Accounts (IRSA)] allows you to deliver temporary AWS
credentials to your workloads running on EKS. IRSA can be used to get
temporary credentials for IAM roles in the workload accounts from the
cluster account. This allows your workloads running on your EKS clusters
in the cluster account to consume AWS API resources, such as S3 buckets
hosted in the workload account seemlessly, and use IAM authentication
for resources like Amazon RDS Databases or Amazon EFS FileSystems.

AWS API resources and other Resources that use IAM authentication in a
workload account can only be accessed by credentials for IAM roles in
that same workload account, except where cross account access is capable
and has been explicity enabled.

====== Enabling IRSA for cross account access

To enable IRSA for workloads in your Cluster Account to access resources
in your Workload accounts, you first must create an IAM OIDC identity
provider in your workload account. This can be done with the same
procedure for setting up
https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html[IRSA],
except the Identity Provider will be created in the workload account.

Then when configuring IRSA for your workloads on EKS, you can
https://docs.aws.amazon.com/eks/latest/userguide/associate-service-account-role.html[follow
the same steps as the documentation], but use the
https://docs.aws.amazon.com/eks/latest/userguide/cross-account-access.html[12
digit account id of the workload account] as mentioned in the section
"`Example Create an identity provider from another account’s cluster`".

After this is configured, your application running in EKS will be able
to directly use its service account to assume a role in the workload
account, and use resources within it.

===== Accessing AWS API Resources with EKS Pod Identities

https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html[EKS
Pod Identities] is a new way of delivering AWS credentials to your
workloads running on EKS. EKS pod identities simplifies the
configuration of AWS resources as you no longer need to manage OIDC
configurations to deliver AWS credentials to your pods on EKS.

====== Enabling EKS Pod Identities for cross account access

Unlike IRSA, EKS Pod Identities can only be used to directly grant
access to a role in the same account as the EKS cluster. To access a
role in another AWS account, pods that use EKS Pod Identities must
perform
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#iam-term-role-chaining[Role
Chaining].

Role chaining can be configured in an applications profile with their
aws configuration file using the
https://docs.aws.amazon.com/sdkref/latest/guide/feature-process-credentials.html[Process
Credentials Provider] available in various AWS SDKs.
`+credential_process+` can be used as a credential source when
configuring a profile, such as:

[source,bash]
----
# Content of the AWS Config file
[profile account_b_role]
source_profile = account_a_role
role_arn = arn:aws:iam::444455556666:role/account-b-role

[profile account_a_role]
credential_process = /eks-credential-processrole.sh
----

The source of the script called by credential_process:

[source,bash]
----
#!/bin/bash
# Content of the eks-credential-processrole.sh
# This will retreive the credential from the pod identities agent,
# and return it to the AWS SDK when referenced in a profile
curl -H "Authorization: $(cat $AWS_CONTAINER_AUTHORIZATION_TOKEN_FILE)" $AWS_CONTAINER_CREDENTIALS_FULL_URI | jq -c '{AccessKeyId: .AccessKeyId, SecretAccessKey: .SecretAccessKey, SessionToken: .Token, Expiration: .Expiration, Version: 1}'
----

You can create an aws config file as shown above with both Account A and
B roles and specify the AWS_CONFIG_FILE and AWS_PROFILE env vars in your
pod spec. EKS Pod identity webhook does not override if the env vars
already exists in the pod spec.

[source,yaml]
----
# Snippet of the PodSpec
containers:
  - name: container-name
    image: container-image:version
    env:
    - name: AWS_CONFIG_FILE
      value: path-to-customer-provided-aws-config-file
    - name: AWS_PROFILE
      value: account_b_role
----

When configuring role trust policies for role chaining with EKS pod
identities, you can reference
https://docs.aws.amazon.com/eks/latest/userguide/pod-id-abac.html[EKS
specific attributes] as session tags and use attribute based access
control(ABAC) to limit access to your IAM roles to only specific EKS Pod
identity sessions, such as the Kubernetes Service Account a pod belongs
to.

Please note that some of these attributes may not be universally unique,
for example two EKS clusters may have identical namespaces, and one
cluster may have identically named service accounts across namespaces.
So when granting access via EKS Pod Identities and ABAC, it is a best
practice to always consider the cluster arn and namespace when granting
access to a service account.

====== ABAC and EKS Pod Identities for cross account access

When using EKS Pod Identities to assume roles (role chaining) in other
accounts as part of a multi account strategy, you have the option to
assign a unique IAM role for each service account that needs to access
another account, or use a common IAM role across multiple service
accounts and use ABAC to control what accounts it can access.

To use ABAC to control what service accounts can assume a role into
another account with role chaining, you create a role trust policy
statement that only allows a role to be assumed by a role session when
the expected values are present. The following role trust policy will
only let a role from the EKS cluster account (account ID 111122223333)
assume a role if the `+kubernetes-service-account+`, `+eks-cluster-arn+`
and `+kubernetes-namespace+` tags all have the expected value.

[source,json]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::111122223333:root"
            },
            "Action": "sts:AssumeRole",
            "Condition": {
                "StringEquals": {
                    "aws:PrincipalTag/kubernetes-service-account": "PayrollApplication",
                    "aws:PrincipalTag/eks-cluster-arn": "arn:aws:eks:us-east-1:111122223333:cluster/ProductionCluster",
                    "aws:PrincipalTag/kubernetes-namespace": "PayrollNamespace"
                }
            }
        }
    ]
}
----

When using this strategy it is a best practice to ensure that the common
IAM role only has `+sts:AssumeRole+` permissions and no other AWS
access.

It is important when using ABAC that you control who has the ability to
tag IAM roles and users to only those who have a strict need to do so.
Someone with the ability to tag an IAM role or user would be able to set
tags on roles/users identical to what would be set by EKS Pod Identities
and may be able to escalate their privileges. You can restrict who has
the access to set tags the `+kubernetes-+` and `+eks-+` tags on IAM role
and users using IAM policy, or Service Control Policy (SCP).

== De-centralized EKS Clusters

In this approach, EKS clusters are deployed to respective workload AWS
Accounts and live along side with other AWS resources like Amazon S3
buckets, VPCs, Amazon DynamoDB tables, etc., Each workload account is
independent, self-sufficient, and operated by respective Business
Unit/Application teams. This model allows the creation of reusuable
blueprints for various cluster capabilities (AI/ML cluster, Batch
processing, General purpose, etc.,) and vend the clusters based on the
application team requirements. Both application and platform teams
operate out of their respective
https://www.weave.works/technologies/gitops/[GitOps] repositories to
manage the deployments to the workload clusters.

[width="100%",cols="^100%",options="header",]
|===
|image:./images/multi-account-eks-decentralized.png[De-centralized EKS
Cluster Architecture]
|In the above diagram, Amazon EKS clusters and other AWS resources are
deployed to respective workload accounts. Then workloads running in EKS
pods use IRSA or EKS Pod Identities to access their AWS resources.
|===

GitOps is a way of managing application and infrastructure deployment so
that the whole system is described declaratively in a Git repository.
It’s an operational model that offers you the ability to manage the
state of multiple Kubernetes clusters using the best practices of
version control, immutable artifacts, and automation. In this multi
cluster model, each workload cluster is bootstrapped with multiple Git
repos, allowing each team (application, platform, security, etc.,) to
deploy their respective changes on the cluster.

You would utilize
https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[IAM
roles for Service Accounts (IRSA)] or
https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html[EKS
Pod Identities] in each account to allow your EKS workloads to get
temporary aws credentials to securely access other AWS resources. IAM
roles are created in respective workload AWS Accounts and map them to
k8s service accounts to provide temporary IAM access. So, no
cross-account access is required in this approach. Follow the
https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[IAM
roles for Service Accounts] documentation on how to setup in each
workload for IRSA, and
https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html[EKS
Pod Identities] documentation on how to setup EKS pod identities in each
account.

=== Centralized Networking

You can also utilize AWS RAM to share the VPC Subnets to workload
accounts and launch Amazon EKS clusters and other AWS resources in them.
This enables centralized network managment/administration, simplified
network connectivity, and de-centralized EKS clusters. Refer this
https://aws.amazon.com/blogs/containers/use-shared-vpcs-in-amazon-eks/[AWS
blog] for a detailed walkthrough and considerations of this approach.

[width="100%",cols="^100%",options="header",]
|===
|image:./images/multi-account-eks-shared-subnets.png[De-centralized EKS
Cluster Architecture using VPC Shared Subnets]
|In the above diagram, AWS RAM is used to share subnets from a central
networking account into a workload account. Then EKS cluster and other
AWS resources are launched in those subnets in respective workload
accounts. EKS pods use IRSA or EKS Pod Identities to access their AWS
resources.
|===

== Centralized vs De-centralized EKS clusters

The decision to run with a Centralized or De-centralized will depend on
your requirements. This table demonstrates the key differences with each
strategy.

[width="100%",cols="&lt;34%,&lt;33%,&lt;33%",options="header",]
|===
|# |Centralized EKS cluster |De-centralized EKS clusters
|Cluster Management: |Managing a single EKS cluster is easier than
administrating multiple clusters |An Efficient cluster management
automation is necessary to reduce the operational overhead of managing
multiple EKS clusters

|Cost Efficiency: |Allows reuse of EKS cluster and network resources,
which promotes cost efficiency |Requires networking and cluster setups
per workload, which requires additional resources

|Resilience: |Multiple workloads on the centralized cluster may be
impacted if a cluster becomes impaired |If a cluster becomes impaired,
the damage is limited to only the workloads that run on that cluster.
All other workloads are unaffected

|Isolation &amp; Security: |Isolation/Soft Multi-tenancy is achieved using
k8s native constructs like `+Namespaces+`. Workloads may share the
underlying resources like CPU, memory, etc. AWS resources are isolated
into their own workload accounts which by default are not accessible
from other AWS accounts. |Stronger isolation on compute resources as the
workloads run in individual clusters and nodes that don’t share any
resources. AWS resources are isolated into their own workload accounts
which by default are not accessible from other AWS accounts.

|Performance &amp; Scalabity: |As workloads grow to very large scales you
may encounter kubernetes and AWS service quotas in the cluster account.
You can deploy addtional cluster accounts to scale even further |As more
clusters and VPCs are present, each workload has more available k8s and
AWS service quota

|Networking: |Single VPC is used per cluster, allowing for simpler
connectivity for applications on that cluster |Routing must be
established between the de-centralized EKS cluster VPCs

|Kubernetes Access Management: |Need to maintain many different roles
and users in the cluster to provide access to all workload teams and
ensure kubernetes resources are properly segregated |Simplified access
management as each cluster is dedicated to a workload/team

|AWS Access Management: |AWS resources are deployed into to their own
account which can only be accessed by default with IAM roles in the
workload account. IAM roles in the workload accounts are assumed cross
account either with IRSA or EKS Pod Identities. |AWS resources are
deployed into to their own account which can only be accessed by default
with IAM roles in the workload account. IAM roles in the workload
accounts are delivered directly to pods with IRSA or EKS Pod Identities
|===

:leveloffset: 1</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2024-06-26 22:06:05 UTC
</div>
</div>
</body>
</html>